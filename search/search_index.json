{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TensorFlow Docs TensorFlow Docs\u306b\u3064\u3044\u3066 TensorFlow Docs\u306f\u3001TensorFlow\u306e\u5b66\u7fd2\u7528\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3059\u3002 \u4fee\u6b63\u4f9d\u983c\u7b49 Github\u306eRepo\u306b Issues \u3092\u3042\u3052\u308b\u3002","title":"TensorFlow Docs"},{"location":"#tensorflow-docs","text":"","title":"TensorFlow Docs"},{"location":"#tensorflow-docs_1","text":"TensorFlow Docs\u306f\u3001TensorFlow\u306e\u5b66\u7fd2\u7528\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3059\u3002","title":"TensorFlow Docs\u306b\u3064\u3044\u3066"},{"location":"#_1","text":"Github\u306eRepo\u306b Issues \u3092\u3042\u3052\u308b\u3002","title":"\u4fee\u6b63\u4f9d\u983c\u7b49"},{"location":"SUMMARY/","text":"\u306f\u3058\u3081\u306b \u958b\u767a\u74b0\u5883 Ubuntu AWS EC2 c4.large Docker (CPU) AWS EC2 p2.xlarge Docker (GPU) VM Docker (CPU) VM Docker Google Cloud Datalab (CPU) \u958b\u767a\u74b0\u5883 Jetson TX2 OpenCV-3.2/OpenMPI-2.1.1/TensorFlow-1.3.0\u3092\u30d3\u30eb\u30c9\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u65b9\u6cd5 OpenCV-3.2/OpenMPI-2.1.1/TensorFlow-1.3.0\u306e\u30d3\u30eb\u30c9\u6e08\u307f\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u5c0e\u5165\u3059\u308b\u65b9\u6cd5 \u958b\u767a\u74b0\u5883(CloudML) CloudML\u306e\u8a2d\u5b9a TensorFlow\u306eVersion up CloudML\u306eEmacs\u306e\u64cd\u4f5c \u958b\u767a\u74b0\u5883(Local) Local\u74b0\u5883\u306e\u6574\u5099 \u958b\u767a\u74b0\u5883(Datalab\u7de8) Datalab\u306eTF\u30921.0.0\u306bUpdate Datalab\u306ePython\u306eversion\u30922\u7cfb\u304b\u30893\u7cfb\u306b\u5909\u66f4\u3059\u308b \u958b\u767a\u74b0\u5883\u69cb\u7bc9(Android) Android(Ubuntu) Android(OS X) Android(OS X) Android(Hexagon) \u958b\u767a\u74b0\u5883\u69cb\u7bc9(RaspPI) RaspPI TensorFlow HelloWorld \u5b9a\u6570\u30fb\u5909\u6570\u30fb\u30d7\u30ec\u30fc\u30b9\u30d5\u30a9\u30eb\u30c0 \u578b\u30fb\u6b21\u5143\u6570\u30fb\u30e9\u30f3\u30af\u30fb\u30b5\u30a4\u30ba \u6f14\u7b97\u5b50 \u7dcf\u548c\u30fb\u7dcf\u7a4d \u6700\u5c0f\u5024\u30fb\u6700\u5927\u5024 \u30af\u30ea\u30c3\u30d4\u30f3\u30b0 \u6b63\u898f\u5206\u5e03\u30fb\u5207\u65ad\u6b63\u898f\u5206\u5e03 \u30d6\u30fc\u30ea\u30a2\u30f3 \u30ad\u30e3\u30b9\u30c8 \u6bd4\u8f03 \u96c6\u7d04 \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570 \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f L2\u6b63\u5247\u5316 \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u8fbc Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part1 Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part2 Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part3 \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part1 \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part2 \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part3 \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part4 \u7dda\u5f62\u56de\u5e30 \u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee \u7dda\u5f62\u56de\u5e30 ) \u7dda\u5f62\u56de\u5e30 TensorBoard1 ) \u7dda\u5f62\u56de\u5e30 TensorBoard2 ) \u7dda\u5f62\u56de\u5e30 \u89e3\u6790 ) \u7dda\u5f62\u56de\u5e30 \u8ab2\u984c ) \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 \u30a6\u30a3\u30eb\u30b9\u5206\u5e03 \u30c7\u30fc\u30bf\u4f5c\u6210 \u30a6\u30a3\u30eb\u30b9\u5206\u5e03 \u5b66\u7fd2 \u30a6\u30a3\u30eb\u30b9\u5206\u5e03 \u8a55\u4fa1 \u30a6\u30a3\u30eb\u30b9\u5206\u5e03 Android \u30ac\u30f3\u30af\u30e9\u30b9 \u30c7\u30fc\u30bf\u4f5c\u6210 \u30ac\u30f3\u30af\u30e9\u30b9 \u5b66\u7fd2 [\u30ac\u30f3\u30af\u30e9\u30b9 \u8a55\u4fa1] \u591a\u30af\u30e9\u30b9\u5206\u985e \u30ef\u30a4\u30f3\u7b49\u7d1a \u30c7\u30fc\u30bf\u4f5c\u6210 \u30ef\u30a4\u30f3\u7b49\u7d1a \u5b66\u7fd2 [\u30ef\u30a4\u30f3\u7b49\u7d1a \u8a55\u4fa1] IRIS \u30c7\u30fc\u30bf\u4f5c\u6210 IRIS \u5b66\u7fd2 IRIS Tensorboard\u7de8 [IRIS \u8a55\u4fa1] \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0 Adam \u4e71\u6570\u304b\u3089\u30af\u30e9\u30b9\u5206\u985e Android Version\u3092\u8868\u793a\u3059\u308b \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc \u5b66\u7fd2\u6e08\u307f\u7d50\u679c\u3092\u5b9f\u884c \u5b66\u7fd2\u6e08\u307f\u7d50\u679c\u3092\u5b9f\u884c\u305d\u306e2 \u5b66\u7fd2\u6e08\u307f\u7d50\u679c\u3092\u5b9f\u884c(TFversion1.0.1\u7528) MNIST MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d0\u30c3\u30c1\u8aad\u307f\u8fbc\u307f MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u60c5\u5831 MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u6e96\u5099\u7de8 MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u91cd\u307f\u6e1b\u8870 MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af TensorBoard\u7de8 MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u66f8\u8fbc\u307f MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30d0\u30c3\u30c1\u6b63\u898f\u5316 MNIST \u8a55\u4fa1 DeepDream \u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u753b\u50cf\u8a8d\u8b58 \u305d\u306e1 \u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u753b\u50cf\u8a8d\u8b58 \u305d\u306e2 \u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0 \u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0 RNN RNN\u306b\u3088\u308b\u30af\u30e9\u30b9\u5206\u985e Matplotlib Matplotlib\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u70b9\u3092\u3046\u3064 \u7dda\u3092\u5f15\u304f End to End Learning Emulator \u958b\u767a\u74b0\u5883 \u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8 \u5b66\u7fd2\u30e2\u30c7\u30eb \u5b9f\u884c Etc Link GAN MLP\u3067\u306eGAN\u306b\u3088\u308bMNIST\u753b\u50cf\u751f\u6210 DCGAN\u306b\u3088\u308bMNIST\u753b\u50cf\u751f\u6210","title":"SUMMARY"},{"location":"tensorflow_operator/","text":"Tensor(\u884c\u5217)\u306e\u548c tf.add tf.add(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat1 + mat2 = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#add Tensor(\u884c\u5217)\u306e\u5dee tf.subtract tf.subtract(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat2 - mat1 = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#sub Tensor(\u884c\u5217)\u306e\u7a4d tf.matmul tf.matmul(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat1 x mat2 = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#matmul Tensor(\u884c\u5217)\u306e\u8981\u7d20\u306e\u7a4d tf.mul tf.multiply(mat_a, mat_b, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat_a x mat_b = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#mul","title":"Tensor(\u884c\u5217)\u306e\u548c"},{"location":"tensorflow_operator/#tensor","text":"","title":"Tensor(\u884c\u5217)\u306e\u548c"},{"location":"tensorflow_operator/#tfadd","text":"tf.add(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.add"},{"location":"tensorflow_operator/#sample","text":"mat1 + mat2 = result_mat","title":"Sample"},{"location":"tensorflow_operator/#_1","text":"https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#add","title":"\u53c2\u8003"},{"location":"tensorflow_operator/#tensor_1","text":"","title":"Tensor(\u884c\u5217)\u306e\u5dee"},{"location":"tensorflow_operator/#tfsubtract","text":"tf.subtract(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.subtract"},{"location":"tensorflow_operator/#sample_1","text":"mat2 - mat1 = result_mat","title":"Sample"},{"location":"tensorflow_operator/#_2","text":"https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#sub","title":"\u53c2\u8003"},{"location":"tensorflow_operator/#tensor_2","text":"","title":"Tensor(\u884c\u5217)\u306e\u7a4d"},{"location":"tensorflow_operator/#tfmatmul","text":"tf.matmul(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.matmul"},{"location":"tensorflow_operator/#sample_2","text":"mat1 x mat2 = result_mat","title":"Sample"},{"location":"tensorflow_operator/#_3","text":"https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#matmul","title":"\u53c2\u8003"},{"location":"tensorflow_operator/#tensor_3","text":"","title":"Tensor(\u884c\u5217)\u306e\u8981\u7d20\u306e\u7a4d"},{"location":"tensorflow_operator/#tfmul","text":"tf.multiply(mat_a, mat_b, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.mul"},{"location":"tensorflow_operator/#sample_3","text":"mat_a x mat_b = result_mat","title":"Sample"},{"location":"tensorflow_operator/#_4","text":"https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#mul","title":"\u53c2\u8003"},{"location":"android/build/","text":"Android\u30a2\u30d7\u30ea\u958b\u767a\u74b0\u5883\u3067TensorFlow\u306e\u5b66\u7fd2\u6e08\u307f\u30b0\u30e9\u30d5(\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u5f62\u5f0f\u306e\u30e2\u30c7\u30eb)\u3092\u8aad\u307f\u8fbc\u307f\u5b9f\u884c\u3059\u308b\u307e\u3067 1 2 3 4 5 6 7 ######################################## # Java8 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## add-apt-repository \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main\" apt-get update add-apt-repository ppa:webupd8team/y-ppa-manager apt-get install oracle-java8-installer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ######################################## # Android SDK \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https://developer.android.com/studio/index.html # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u30c4\u30fc\u30eb(tools_r25.2.3-linux.zip) # Android Studio\u3068TensorFlow Android Interface\u306e\u30d3\u30eb\u30c9\u8a2d\u5b9a\u306b\u5408\u308f\u305b\u3066\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b export ANDROID_HOME= ${ HOME } /Android/Sdk export NDK_ROOT= ${ ANDROID_HOME } /ndk-bundle mkdir -p ${ ANDROID_HOME } unzip tools_r25.2.3-linux.zip mv tools ${ ANDROID_HOME } ${ ANDROID_HOME } /tools/bin/sdkmanager --update # TensorFlow Android Interface\u306e\u30d3\u30eb\u30c9\u8a2d\u5b9a\u306b\u5408\u308f\u305b\u3066build-tools\u3068ndk-bundle\u3068tools\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b ${ ANDROID_HOME } /tools/bin/sdkmanager --list ${ ANDROID_HOME } /tools/bin/sdkmanager \\ \"build-tools;24.0.2\" \\ ndk-bundle \\ tools \\ \"platforms;android-24\" \\ \"platforms;android-21\" \\ \"cmake;3.6.3155560\" \\ \"patcher;v4\" \\ \"extras;android;m2repository\" # \u30ef\u30fc\u30cb\u30f3\u30b0\u304c\u51fa\u308b\u6642\u306f\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u308b vi ~/.android/repositories.cfg ### User Sources for Android SDK Manager count=0 1 2 3 4 5 ######################################## # build package \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # TensorFlow Android Interface \u30e9\u30a4\u30d6\u30e9\u30ea\u4f5c\u6210\u7528 apt-get install git automake libtool zlib1g-dev Android Java Inference Interface for TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android \u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u6b32\u3057\u3044\u306e\u3067\u3059\u304c\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089build\u3057\u306a\u3044\u3068\u624b\u306b\u5165\u3089\u306a\u3044\u307f\u305f\u3044\u306a\u306e\u3067build\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306fbazel\u3067\u306f\u306a\u304f\u3001build_all_android.sh\u3092\u4f7f\u3063\u3066\u30d3\u30eb\u30c9\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002 1 2 3 4 5 ######################################## # TensorFlow r1.0 source code ######################################## mkdir ~/github; cd ~/github git clone -b r1.0 --recurse-submodules https://github.com/tensorflow/tensorflow.git 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ######################################## # TensorFlow Android Interface build with build_all_android.sh ######################################## # export NDK_ROOT= ${ ANDROID_HOME } /ndk-bundle # ${ ANDROID_HOME } /tools/bin/sdkmanager \"platforms;android-24\" \"platforms;android-21\" \"platforms;android-21\" \"build-tools;24.0.2\" # Makefile\u304b\u3089-march=native\u3092\u524a\u9664\u3059\u308b r0.12\u3067\u306f\u306a\u304b\u3063\u305f\u3051\u3069\u3001v1.0.0-rc2\u304b\u3089-march=native\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u3066\u30a8\u30e9\u30fc\u306b\u306a\u308b\u306e\u3067\u524a\u9664\u3059\u308b # https://github.com/tensorflow/tensorflow/commit/c4e3d4a74e86fce3a09badd20952f067ff340f32 # tensorflow/tensorflow/contrib/makefile/Makefile L:140 vi ${ HOME } /github/tensorflow/tensorflow/contrib/makefile/Makefile #OPTFLAGS := -O2 -march=native OPTFLAGS := -O2 ${ HOME } /github/tensorflow/tensorflow/contrib/makefile/build_all_android.sh \u30a8\u30e9\u30fc\u306b\u306a\u3063\u305f\u3089package\u4e0d\u8db3\u304bTensorFlow\u306e\u958b\u767a\u304c\u9032\u3093\u3067Makefile\u3084\u30b3\u30fc\u30c9\u304c\u5909\u308f\u3063\u3066\u3044\u308b\u305f\u3081\u3060\u3068\u601d\u3046\u306e\u3067\u3001\u30a8\u30e9\u30fc\u5185\u5bb9\u3092\u307f\u3066\u4fee\u6b63\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 Android \u30a2\u30d7\u30ea\u3092\u4f5c\u6210\u3059\u308b 1 2 3 4 5 6 ######################################## # Android Studio\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b ######################################## # https://developer.android.com/studio/index.html?hl=ja unzip android-studio-ide-145.3537739-linux.zip mv android-studio ~/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ######################################## # Android Studio - Hello Application JNI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b ######################################## # Gradle Scripts # settings . gradle ( Project Settings ) include ':app' , ':TensorFlow-Android-Inference' findProject ( \":TensorFlow-Android-Inference\" ). projectDir = new File ( \"/home/guppy/github/tensorflow/tensorflow/contrib/android/cmake\" ) # Gradle Scripts # build . gradle ( Module : app ) # tensorflow_inference\u3067\u306f\u306a\u304f \u3001 TensorFlow - Android - Inference\u3068\u3059\u308b \u3002 dependencies { ... debugCompile project ( path : ':TensorFlow-Android-Inference' , configuration : 'debug' ) releaseCompile project ( path : ':TensorFlow-Android-Inference' , configuration : 'release' ) } TensorFlow\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5 https://github.com/FaBoPlatform/TensorFlow/blob/master/android/run.md TensorFlow\u30e2\u30c7\u30eb https://github.com/FaBoPlatform/TensorFlow/blob/master/android/model.pb","title":"Build"},{"location":"android/build/#android-java-inference-interface-for-tensorflow","text":"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android \u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u6b32\u3057\u3044\u306e\u3067\u3059\u304c\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089build\u3057\u306a\u3044\u3068\u624b\u306b\u5165\u3089\u306a\u3044\u307f\u305f\u3044\u306a\u306e\u3067build\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u306fbazel\u3067\u306f\u306a\u304f\u3001build_all_android.sh\u3092\u4f7f\u3063\u3066\u30d3\u30eb\u30c9\u3059\u308b\u65b9\u6cd5\u3092\u8a18\u8f09\u3057\u307e\u3059\u3002 1 2 3 4 5 ######################################## # TensorFlow r1.0 source code ######################################## mkdir ~/github; cd ~/github git clone -b r1.0 --recurse-submodules https://github.com/tensorflow/tensorflow.git 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ######################################## # TensorFlow Android Interface build with build_all_android.sh ######################################## # export NDK_ROOT= ${ ANDROID_HOME } /ndk-bundle # ${ ANDROID_HOME } /tools/bin/sdkmanager \"platforms;android-24\" \"platforms;android-21\" \"platforms;android-21\" \"build-tools;24.0.2\" # Makefile\u304b\u3089-march=native\u3092\u524a\u9664\u3059\u308b r0.12\u3067\u306f\u306a\u304b\u3063\u305f\u3051\u3069\u3001v1.0.0-rc2\u304b\u3089-march=native\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u3066\u30a8\u30e9\u30fc\u306b\u306a\u308b\u306e\u3067\u524a\u9664\u3059\u308b # https://github.com/tensorflow/tensorflow/commit/c4e3d4a74e86fce3a09badd20952f067ff340f32 # tensorflow/tensorflow/contrib/makefile/Makefile L:140 vi ${ HOME } /github/tensorflow/tensorflow/contrib/makefile/Makefile #OPTFLAGS := -O2 -march=native OPTFLAGS := -O2 ${ HOME } /github/tensorflow/tensorflow/contrib/makefile/build_all_android.sh \u30a8\u30e9\u30fc\u306b\u306a\u3063\u305f\u3089package\u4e0d\u8db3\u304bTensorFlow\u306e\u958b\u767a\u304c\u9032\u3093\u3067Makefile\u3084\u30b3\u30fc\u30c9\u304c\u5909\u308f\u3063\u3066\u3044\u308b\u305f\u3081\u3060\u3068\u601d\u3046\u306e\u3067\u3001\u30a8\u30e9\u30fc\u5185\u5bb9\u3092\u307f\u3066\u4fee\u6b63\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002","title":"Android Java Inference Interface for TensorFlow"},{"location":"android/build/#android","text":"1 2 3 4 5 6 ######################################## # Android Studio\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b ######################################## # https://developer.android.com/studio/index.html?hl=ja unzip android-studio-ide-145.3537739-linux.zip mv android-studio ~/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ######################################## # Android Studio - Hello Application JNI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b ######################################## # Gradle Scripts # settings . gradle ( Project Settings ) include ':app' , ':TensorFlow-Android-Inference' findProject ( \":TensorFlow-Android-Inference\" ). projectDir = new File ( \"/home/guppy/github/tensorflow/tensorflow/contrib/android/cmake\" ) # Gradle Scripts # build . gradle ( Module : app ) # tensorflow_inference\u3067\u306f\u306a\u304f \u3001 TensorFlow - Android - Inference\u3068\u3059\u308b \u3002 dependencies { ... debugCompile project ( path : ':TensorFlow-Android-Inference' , configuration : 'debug' ) releaseCompile project ( path : ':TensorFlow-Android-Inference' , configuration : 'release' ) }","title":"Android \u30a2\u30d7\u30ea\u3092\u4f5c\u6210\u3059\u308b"},{"location":"android/build/#tensorflow","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/android/run.md","title":"TensorFlow\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5"},{"location":"android/build/#tensorflow_1","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/android/model.pb","title":"TensorFlow\u30e2\u30c7\u30eb"},{"location":"android/build_cmake/","text":"Android Java Inference Interface for TensorFlow\u3092Module\u3068\u3057\u3066\u3001Android Studio\u306b\u53d6\u308a\u8fbc\u3080\u3002Bazzel\u306eBuild\u7b49\u3082Android Studio\u5185\u304b\u3089\u5b9f\u884c\u3002 https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android/cmake setting.gradle 1 2 3 include ':app',':TensorFlow-Android-Inference' findProject(\":TensorFlow-Android-Inference\").projectDir = new File(\"/Users/sasakiakira/Documents/workspace_ai_android/android/bazel/tensorflow/tensorflow/contrib/android/cmake\") build.gradle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def bazel_location = '/usr/local/bin/bazel' def cpuType = 'armeabi-v7a' def nativeDir = 'libs/' + cpuType project . buildDir = 'gradleBuild' getProject (). setBuildDir ( 'gradleBuild' ) apply plugin : 'com.android.application' android { compileSdkVersion 24 buildToolsVersion \"25.0.2\" defaultConfig { applicationId \"io.fabo.virus\" minSdkVersion 21 targetSdkVersion 24 versionCode 1 versionName \"1.0\" testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\" } buildTypes { release { minifyEnabled false proguardFiles getDefaultProguardFile ( 'proguard-android.txt' ), 'proguard-rules.pro' } } lintOptions { abortOnError false } sourceSets { main { manifest . srcFile 'AndroidManifest.xml' java . srcDirs = [ 'src', '../../contrib/android/java' ] resources . srcDirs = [ 'src' ] aidl . srcDirs = [ 'src' ] renderscript . srcDirs = [ 'src' ] res . srcDirs = [ 'res' ] assets . srcDirs = [ 'assets' ] jniLibs . srcDirs = [ 'libs' ] } debug . setRoot ( 'build-types/debug' ) release . setRoot ( 'build-types/release' ) } } dependencies { compile fileTree ( dir : 'libs' , include : [ '*.jar' ] ) androidTestCompile ( 'com.android.support.test.espresso:espresso-core:2.2.2' , { exclude group : 'com.android.support' , module : 'support-annotations' } ) compile 'com.android.support:appcompat-v7:24.2.1' testCompile 'junit:junit:4.12' debugCompile project ( path : ':TensorFlow-Android-Inference' , configuration : 'debug' ) releaseCompile project ( path : ':TensorFlow-Android-Inference' , configuration : 'release' ) } task buildNative ( type : Exec ) { workingDir '/Users/sasakiakira/Documents/workspace_ai_android/android/bazel/tensorflow' commandLine bazel_location , 'build' , '-c' , 'opt' , \\ 'tensorflow/examples/android:tensorflow_native_libs' , \\ '--crosstool_top=//external:android/crosstool' , \\ '--cpu=' + cpuType , \\ '--host_crosstool_top=@bazel_tools//tools/cpp:toolchain' } task copyNativeLibs ( type : Copy ) { from ( '/Users/sasakiakira/Documents/workspace_ai_android/android/bazel/tensorflow/bazel-bin/tensorflow/contrib/android/' ) { include '**/*.so' } into nativeDir duplicatesStrategy = 'include' } copyNativeLibs . dependsOn buildNative assemble . dependsOn copyNativeLibs task findbugs ( type : FindBugs , dependsOn : 'assembleDebug' ) { copyNativeLibs } setting.gradle 1 2 3 include ':app',':TensorFlow-Android-Inference' findProject(\":TensorFlow-Android-Inference\").projectDir = new File(\"/Users/sasakiakira/Documents/workspace_ai_android/android/bazel/tensorflow/tensorflow/contrib/android/cmake\") 1 2 $ brew instal automake $ brew install libtool","title":"Build cmake"},{"location":"android/build_hexagon/","text":"Hexagon\u3092\u6709\u52b9\u306b\u3057\u305fBuild 1 $ ./build_all_android.sh -X Hexagon\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea \u30e9\u30a4\u30d6\u30e9\u30ea\u540d DL\u5148 libhexagon_controller.so https://storage.googleapis.com/download.tensorflow.org/deps/hexagon/libhexagon_controller.so libhexagon_nn_skel.so https://storage.googleapis.com/download.tensorflow.org/deps/hexagon/libhexagon_nn_skel.so 1 2 3 4 5 6 7 8 download_and_push() { URL=\"$1\" LOCAL_DEST=\"$2\" ANDROID_DEST=\"$3\" curl -Ls \" ${ URL } \" -o \" ${ LOCAL_DEST } \" adb shell mkdir -p \" ${ ANDROID_DEST } \" adb push \" ${ LOCAL_DEST } \" \" ${ ANDROID_DEST } \" } \u3067\u3001adb\u3067 libhexagon_controller.so \u3068 libhexagon_nn_skel.so \u3092adb push\u3059\u308b\u51e6\u7406\u304c\u5165\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u30c7\u30d0\u30a4\u30b9\u3092\u63a5\u7d9a\u3057\u3066\u304a\u304f\u3002 \u73fe\u5728\u306e\u30c7\u30d0\u30a4\u30b9\u3067\u306f\u3001libhexagon_nn_skel.so\u3092\u5b9f\u6a5f\u306b\u8ee2\u9001\u3059\u308b\u969b\u306b\u3001 1 $ adb shell mkdir /vendor/lib/rfsa/adsp \u3067 1 mkdir: '/vendor/lib/rfsa' : Read-only file system \u3068\u306a\u308a\u3001\u5148\u306b\u9032\u3081\u306a\u3044\u3002","title":"Build hexagon"},{"location":"android/build_hexagon/#hexagonbuild","text":"1 $ ./build_all_android.sh -X Hexagon\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea \u30e9\u30a4\u30d6\u30e9\u30ea\u540d DL\u5148 libhexagon_controller.so https://storage.googleapis.com/download.tensorflow.org/deps/hexagon/libhexagon_controller.so libhexagon_nn_skel.so https://storage.googleapis.com/download.tensorflow.org/deps/hexagon/libhexagon_nn_skel.so 1 2 3 4 5 6 7 8 download_and_push() { URL=\"$1\" LOCAL_DEST=\"$2\" ANDROID_DEST=\"$3\" curl -Ls \" ${ URL } \" -o \" ${ LOCAL_DEST } \" adb shell mkdir -p \" ${ ANDROID_DEST } \" adb push \" ${ LOCAL_DEST } \" \" ${ ANDROID_DEST } \" } \u3067\u3001adb\u3067 libhexagon_controller.so \u3068 libhexagon_nn_skel.so \u3092adb push\u3059\u308b\u51e6\u7406\u304c\u5165\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u30c7\u30d0\u30a4\u30b9\u3092\u63a5\u7d9a\u3057\u3066\u304a\u304f\u3002 \u73fe\u5728\u306e\u30c7\u30d0\u30a4\u30b9\u3067\u306f\u3001libhexagon_nn_skel.so\u3092\u5b9f\u6a5f\u306b\u8ee2\u9001\u3059\u308b\u969b\u306b\u3001 1 $ adb shell mkdir /vendor/lib/rfsa/adsp \u3067 1 mkdir: '/vendor/lib/rfsa' : Read-only file system \u3068\u306a\u308a\u3001\u5148\u306b\u9032\u3081\u306a\u3044\u3002","title":"Hexagon\u3092\u6709\u52b9\u306b\u3057\u305fBuild"},{"location":"android/build_osx/","text":"TensorFlow1.0.0\u3092Git\u304b\u3089Clone TensorFlow 1.0\u3092Git\u3067Clone\u3059\u308b\u3002 1 git clone -b r1.0 --recurse-submodules https://github.com/tensorflow/tensorflow.git Android SDK\u306e\u30d1\u30b9\u3092\u901a\u3059 \u30d1\u30b9 Default\u306e\u5834\u6240 ANDROID_HOME /Users/username/Library/Android/sdk/ NDK_ROOT ${ANDROID_HOME}/ndk-bundle 1 2 $ export ANDROID_HOME = /Users/username/Library/Android/sdk/ $ export NDK_ROOT = ${ ANDROID_HOME } /ndk-bundle \u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092BREW\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb 1 $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" \u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30b3\u30de\u30f3\u30c9 autoconf $ brew install autoconf alcocal $ brew install automake libtool $ brew install libtool 1 2 3 $ brew install autoconf $ brew install automake $ brew install libtool Makefile\u306b -march=native \u304c\u3042\u308b\u3068\u3001\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u306e\u3067\u3001 -march=native \u3092\u524a\u9664\u3057\u3066\u304a\u304f\u3002 /tensorflow/tensorflow/contrib/makefile/Makefile Shell 1 2 #OPTFLAGS := -O2 -march=native $ OPTFLAGS : = -O2 Shell 1 2 $ cd /tensorflow/tensorflow/contrib/makefile/ $ build_all_android.sh ` Android Studio - Hello Application JNI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b Shell 1 2 3 4 5 # Gradle Scripts # settings.gradle (Project Settings) include ':app' , ':TensorFlow-Android-Inference' findProject ( \":TensorFlow-Android-Inference\" ) .projectDir = new File ( \"/home/guppy/github/tensorflow/tensorflow/contrib/android/cmake\" ) Shell 1 2 3 4 5 6 7 8 # Gradle Scripts # build.gradle(Module:app) # tensorflow_inference\u3067\u306f\u306a\u304f\u3001TensorFlow-Android-Inference\u3068\u3059\u308b\u3002 dependencies { ... debugCompile project ( path: ':TensorFlow-Android-Inference' , configuration: 'debug' ) releaseCompile project ( path: ':TensorFlow-Android-Inference' , configuration: 'release' ) } TensorFlow\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5 https://github.com/FaBoPlatform/TensorFlow/blob/master/android/run.md TensorFlow\u30e2\u30c7\u30eb https://github.com/FaBoPlatform/TensorFlow/blob/master/android/model.pb","title":"Build osx"},{"location":"android/build_osx/#tensorflow100gitclone","text":"TensorFlow 1.0\u3092Git\u3067Clone\u3059\u308b\u3002 1 git clone -b r1.0 --recurse-submodules https://github.com/tensorflow/tensorflow.git","title":"TensorFlow1.0.0\u3092Git\u304b\u3089Clone"},{"location":"android/build_osx/#android-sdk","text":"\u30d1\u30b9 Default\u306e\u5834\u6240 ANDROID_HOME /Users/username/Library/Android/sdk/ NDK_ROOT ${ANDROID_HOME}/ndk-bundle 1 2 $ export ANDROID_HOME = /Users/username/Library/Android/sdk/ $ export NDK_ROOT = ${ ANDROID_HOME } /ndk-bundle","title":"Android SDK\u306e\u30d1\u30b9\u3092\u901a\u3059"},{"location":"android/build_osx/#brew","text":"1 $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" \u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u30b3\u30de\u30f3\u30c9 autoconf $ brew install autoconf alcocal $ brew install automake libtool $ brew install libtool 1 2 3 $ brew install autoconf $ brew install automake $ brew install libtool Makefile\u306b -march=native \u304c\u3042\u308b\u3068\u3001\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3059\u308b\u306e\u3067\u3001 -march=native \u3092\u524a\u9664\u3057\u3066\u304a\u304f\u3002 /tensorflow/tensorflow/contrib/makefile/Makefile Shell 1 2 #OPTFLAGS := -O2 -march=native $ OPTFLAGS : = -O2 Shell 1 2 $ cd /tensorflow/tensorflow/contrib/makefile/ $ build_all_android.sh `","title":"\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092BREW\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"android/build_osx/#android-studio-hello-application-jni","text":"Shell 1 2 3 4 5 # Gradle Scripts # settings.gradle (Project Settings) include ':app' , ':TensorFlow-Android-Inference' findProject ( \":TensorFlow-Android-Inference\" ) .projectDir = new File ( \"/home/guppy/github/tensorflow/tensorflow/contrib/android/cmake\" ) Shell 1 2 3 4 5 6 7 8 # Gradle Scripts # build.gradle(Module:app) # tensorflow_inference\u3067\u306f\u306a\u304f\u3001TensorFlow-Android-Inference\u3068\u3059\u308b\u3002 dependencies { ... debugCompile project ( path: ':TensorFlow-Android-Inference' , configuration: 'debug' ) releaseCompile project ( path: ':TensorFlow-Android-Inference' , configuration: 'release' ) }","title":"Android Studio - Hello Application JNI\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b"},{"location":"android/build_osx/#tensorflow","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/android/run.md","title":"TensorFlow\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5"},{"location":"android/build_osx/#tensorflow_1","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/android/model.pb","title":"TensorFlow\u30e2\u30c7\u30eb"},{"location":"android/build_osx_bazel/","text":"TensorFlow1.0.0\u3092Git\u304b\u3089Clone TensorFlow 1.0\u3092Git\u3067Clone\u3059\u308b\u3002 1 $ git clone -b r1.0 --recurse-submodules https://github.com/tensorflow/tensorflow.git BREW\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb 1 $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" Java 8.x\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb 1 $ brew cask install java Bazel\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb 1 2 $ brew install bazel $ brew update bazel WORKSPACE\u306e\u7de8\u96c6 \u30d1\u30b9 Default\u306e\u5834\u6240 ANDROID_HOME /Users/username/Library/Android/sdk/ NDK_ROOT ${ANDROID_HOME}/ndk-bundle tensorflow/WORKSPACE \u3092\u7de8\u96c6\u3059\u308b\u3002android_sdk_repository\u3068android_ndk_repository\u3092\u81ea\u5206\u306e\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u8a2d\u5b9a\u3059\u308b\u3002android sdk\u306e\u30d5\u30a9\u30eb\u30c0\u306e\u4e0b\u306bndk-bundle\u3068\u3044\u3046\u30d5\u30a9\u30eb\u30c0\u304c\u306a\u3044\u5834\u5408\u306f\u3001Android Studio\u306eManager for Android SDK and Tools\u306eAndroid Tools\u304b\u3089NDK\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u3002 tensorflow/WORKSPACE 1 2 3 4 5 6 7 8 9 10 11 12 13 # Uncomment and update the paths in these entries to build the Android demo. android_sdk_repository ( name = \"androidsdk\" , api_level = 23 , build_tools_version = \"23.0.1\" , # Replace with path to Android SDK on your system path = \"/Users/sasakiakira/Library/Android/sdk/\" , ) android_ndk_repository ( name = \"androidndk\" , path = \"/Users/sasakiakira/Library/Android/sdk/ndk-bundle/\" , api_level = 21 ) Bazel\u3067libtensorflow_inference.so\u3092Build 1 2 3 4 $ bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\ --crosstool_top = //external:android/crosstool \\ --host_crosstool_top = @bazel_tools//tools/cpp:toolchain \\ --cpu = armeabi-v7a libtensorflow_inference.so\u304c\u751f\u6210\u3055\u308c\u305f\u304b\u78ba\u8a8d\u3059\u308b\u3002 1 $ ls bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so \u751f\u6210\u3057\u305flibtensorflow_inference.so \u3092\u4efb\u610f\u306e\u5834\u6240\u306b\u30b3\u30d4\u30fc\u3057\u3066\u304a\u304f\u3002 Bazel\u3067libandroid_tensorflow_inference_java.jar\u3092Build 1 $ bazel build //tensorflow/contrib/android:android_tensorflow_inference_java libandroid_tensorflow_inference_java.jar\u304c\u751f\u6210\u3055\u308c\u305f\u304b\u78ba\u8a8d\u3059\u308b\u3002 1 $ bazel-bin/tensorflow/contrib/android/libandroid_tensorflow_inference_java.jar \u751f\u6210\u3057\u305flibtensorflow_inference.jar \u3092\u4efb\u610f\u306e\u5834\u6240\u306b\u30b3\u30d4\u30fc\u3057\u3066\u304a\u304f\u3002 Android Studio\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3078\u306e\u53d6\u308a\u8fbc\u307f libtensorflow_inference.so \u3068 libtensorflow_inference.jar \u3092\u305d\u308c\u305e\u308c\u3001 libs/armedabi-v7a \u3068 libs \u306b\u65b0\u898f\u30d5\u30a9\u30eb\u30c0\u3092\u4f5c\u6210\u3057\u3066\u30b3\u30d4\u30fc\u3059\u308b\u3002 \u307e\u305f\u3001Model\u30c7\u30fc\u30bf\u306f\u3001 aseets \u30d5\u30a9\u30a4\u30eb\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3002 \u4eca\u56de\u306f\u3001 \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u8fbc \u3067\u4f5c\u6210\u3057\u305f model.pb \u3092\u4f7f\u7528\u3059\u308b\u3002 \u307e\u305f\u3001build.gradle\u3092\u4e0b\u8a18\u306e\u3088\u3046\u306b\u66f8\u304d\u76f4\u3057\u3001TensorFlow\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a2\u30d7\u30ea\u5185\u3067\u4f7f\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Gradle Scripts # build.gradle(Module:app) android { ... sourceSets { main { jniLibs.srcDirs = [ 'libs' ] assets.srcDirs = [ 'assets' ] } } } ... dependencies { ... compile files ( 'libs/libandroid_tensorflow_interface_java.jar' ) } Android\u30a2\u30d7\u30ea 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package io.fabo.helloandroid ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/model.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"result:\" + result ); int [] a_value = new int [ 1 ] ; a_value [ 0 ] = 3 ; int [] b_value = new int [ 1 ] ; b_value [ 0 ] = 4 ; mTensorFlowIF . fillNodeInt ( \"input_a\" , new int [] { 1 }, a_value ); mTensorFlowIF . fillNodeInt ( \"input_b\" , new int [] { 1 }, b_value ); int [] result_value = new int [ 1 ] ; mTensorFlowIF . runInference ( new String [] { \"add_op\" }); mTensorFlowIF . readNodeInt ( \"add_op\" , result_value ); Log . i ( TAG , \"result_value:\" + result_value [ 0 ] ); } } \u53c2\u8003 TensorFlow\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5 https://github.com/FaBoPlatform/TensorFlow/blob/master/android/run.md TensorFlow\u30e2\u30c7\u30eb https://github.com/FaBoPlatform/TensorFlow/blob/master/android/model.pb","title":"Build osx bazel"},{"location":"android/build_osx_bazel/#tensorflow100gitclone","text":"TensorFlow 1.0\u3092Git\u3067Clone\u3059\u308b\u3002 1 $ git clone -b r1.0 --recurse-submodules https://github.com/tensorflow/tensorflow.git","title":"TensorFlow1.0.0\u3092Git\u304b\u3089Clone"},{"location":"android/build_osx_bazel/#brew","text":"1 $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \"","title":"BREW\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"android/build_osx_bazel/#java-8x","text":"1 $ brew cask install java","title":"Java 8.x\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"android/build_osx_bazel/#bazel","text":"1 2 $ brew install bazel $ brew update bazel","title":"Bazel\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"android/build_osx_bazel/#workspace","text":"\u30d1\u30b9 Default\u306e\u5834\u6240 ANDROID_HOME /Users/username/Library/Android/sdk/ NDK_ROOT ${ANDROID_HOME}/ndk-bundle tensorflow/WORKSPACE \u3092\u7de8\u96c6\u3059\u308b\u3002android_sdk_repository\u3068android_ndk_repository\u3092\u81ea\u5206\u306e\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u8a2d\u5b9a\u3059\u308b\u3002android sdk\u306e\u30d5\u30a9\u30eb\u30c0\u306e\u4e0b\u306bndk-bundle\u3068\u3044\u3046\u30d5\u30a9\u30eb\u30c0\u304c\u306a\u3044\u5834\u5408\u306f\u3001Android Studio\u306eManager for Android SDK and Tools\u306eAndroid Tools\u304b\u3089NDK\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u3002 tensorflow/WORKSPACE 1 2 3 4 5 6 7 8 9 10 11 12 13 # Uncomment and update the paths in these entries to build the Android demo. android_sdk_repository ( name = \"androidsdk\" , api_level = 23 , build_tools_version = \"23.0.1\" , # Replace with path to Android SDK on your system path = \"/Users/sasakiakira/Library/Android/sdk/\" , ) android_ndk_repository ( name = \"androidndk\" , path = \"/Users/sasakiakira/Library/Android/sdk/ndk-bundle/\" , api_level = 21 )","title":"WORKSPACE\u306e\u7de8\u96c6"},{"location":"android/build_osx_bazel/#bazellibtensorflow_inferencesobuild","text":"1 2 3 4 $ bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\ --crosstool_top = //external:android/crosstool \\ --host_crosstool_top = @bazel_tools//tools/cpp:toolchain \\ --cpu = armeabi-v7a libtensorflow_inference.so\u304c\u751f\u6210\u3055\u308c\u305f\u304b\u78ba\u8a8d\u3059\u308b\u3002 1 $ ls bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so \u751f\u6210\u3057\u305flibtensorflow_inference.so \u3092\u4efb\u610f\u306e\u5834\u6240\u306b\u30b3\u30d4\u30fc\u3057\u3066\u304a\u304f\u3002","title":"Bazel\u3067libtensorflow_inference.so\u3092Build"},{"location":"android/build_osx_bazel/#bazellibandroid_tensorflow_inference_javajarbuild","text":"1 $ bazel build //tensorflow/contrib/android:android_tensorflow_inference_java libandroid_tensorflow_inference_java.jar\u304c\u751f\u6210\u3055\u308c\u305f\u304b\u78ba\u8a8d\u3059\u308b\u3002 1 $ bazel-bin/tensorflow/contrib/android/libandroid_tensorflow_inference_java.jar \u751f\u6210\u3057\u305flibtensorflow_inference.jar \u3092\u4efb\u610f\u306e\u5834\u6240\u306b\u30b3\u30d4\u30fc\u3057\u3066\u304a\u304f\u3002","title":"Bazel\u3067libandroid_tensorflow_inference_java.jar\u3092Build"},{"location":"android/build_osx_bazel/#android-studio","text":"libtensorflow_inference.so \u3068 libtensorflow_inference.jar \u3092\u305d\u308c\u305e\u308c\u3001 libs/armedabi-v7a \u3068 libs \u306b\u65b0\u898f\u30d5\u30a9\u30eb\u30c0\u3092\u4f5c\u6210\u3057\u3066\u30b3\u30d4\u30fc\u3059\u308b\u3002 \u307e\u305f\u3001Model\u30c7\u30fc\u30bf\u306f\u3001 aseets \u30d5\u30a9\u30a4\u30eb\u306b\u30b3\u30d4\u30fc\u3059\u308b\u3002 \u4eca\u56de\u306f\u3001 \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u8fbc \u3067\u4f5c\u6210\u3057\u305f model.pb \u3092\u4f7f\u7528\u3059\u308b\u3002 \u307e\u305f\u3001build.gradle\u3092\u4e0b\u8a18\u306e\u3088\u3046\u306b\u66f8\u304d\u76f4\u3057\u3001TensorFlow\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a2\u30d7\u30ea\u5185\u3067\u4f7f\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Gradle Scripts # build.gradle(Module:app) android { ... sourceSets { main { jniLibs.srcDirs = [ 'libs' ] assets.srcDirs = [ 'assets' ] } } } ... dependencies { ... compile files ( 'libs/libandroid_tensorflow_interface_java.jar' ) }","title":"Android Studio\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3078\u306e\u53d6\u308a\u8fbc\u307f"},{"location":"android/build_osx_bazel/#android","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package io.fabo.helloandroid ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/model.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"result:\" + result ); int [] a_value = new int [ 1 ] ; a_value [ 0 ] = 3 ; int [] b_value = new int [ 1 ] ; b_value [ 0 ] = 4 ; mTensorFlowIF . fillNodeInt ( \"input_a\" , new int [] { 1 }, a_value ); mTensorFlowIF . fillNodeInt ( \"input_b\" , new int [] { 1 }, b_value ); int [] result_value = new int [ 1 ] ; mTensorFlowIF . runInference ( new String [] { \"add_op\" }); mTensorFlowIF . readNodeInt ( \"add_op\" , result_value ); Log . i ( TAG , \"result_value:\" + result_value [ 0 ] ); } }","title":"Android\u30a2\u30d7\u30ea"},{"location":"android/build_osx_bazel/#_1","text":"TensorFlow\u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u65b9\u6cd5 https://github.com/FaBoPlatform/TensorFlow/blob/master/android/run.md TensorFlow\u30e2\u30c7\u30eb https://github.com/FaBoPlatform/TensorFlow/blob/master/android/model.pb","title":"\u53c2\u8003"},{"location":"android/load_model/","text":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc Android\u30a2\u30d7\u30ea\u3067\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package io.fabo.helloandroid ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/model.pb\" ); Log . i ( TAG , \"result:\" + result ); } } build.gradle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 android { compileSdkVersion 24 buildToolsVersion \"25.0.2\" defaultConfig { applicationId \"io.fabo.helloandroid\" minSdkVersion 21 targetSdkVersion 24 versionCode 1 versionName \"1.0\" testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\" } buildTypes { release { minifyEnabled false proguardFiles getDefaultProguardFile ( 'proguard-android.txt' ), 'proguard-rules.pro' } } lintOptions { abortOnError false } sourceSets { main { manifest . srcFile 'src/main/AndroidManifest.xml' java . srcDirs = [ 'src/main/java/' , '../../contrib/android/java' ] // resources . srcDirs = [ 'src' ] // aidl . srcDirs = [ 'src' ] // renderscript . srcDirs = [ 'src' ] res . srcDirs = [ 'src/main/res' ] assets . srcDirs = [ 'asset' ] jniLibs . srcDirs = [ 'src/main/libs' ] } } }","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc"},{"location":"android/load_model/#_1","text":"Android\u30a2\u30d7\u30ea\u3067\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc"},{"location":"android/load_model/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package io.fabo.helloandroid ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/model.pb\" ); Log . i ( TAG , \"result:\" + result ); } } build.gradle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 android { compileSdkVersion 24 buildToolsVersion \"25.0.2\" defaultConfig { applicationId \"io.fabo.helloandroid\" minSdkVersion 21 targetSdkVersion 24 versionCode 1 versionName \"1.0\" testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\" } buildTypes { release { minifyEnabled false proguardFiles getDefaultProguardFile ( 'proguard-android.txt' ), 'proguard-rules.pro' } } lintOptions { abortOnError false } sourceSets { main { manifest . srcFile 'src/main/AndroidManifest.xml' java . srcDirs = [ 'src/main/java/' , '../../contrib/android/java' ] // resources . srcDirs = [ 'src' ] // aidl . srcDirs = [ 'src' ] // renderscript . srcDirs = [ 'src' ] res . srcDirs = [ 'src/main/res' ] assets . srcDirs = [ 'asset' ] jniLibs . srcDirs = [ 'src/main/libs' ] } } }","title":"Sample"},{"location":"android/load_model2/","text":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc \u305d\u306e2 Android\u30a2\u30d7\u30ea\u3067\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 \u4eca\u56de\u306f\u30a2\u30e4\u30e1\u306e\u5224\u5b9a\u3092\u884c\u3046\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3001\u305d\u306e\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u30c6\u30b9\u30c8\u3092\u884c\u3046 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u304b\u3089\u5165\u624b\u3059\u308b 1 $ curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data \u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u30d7\u30ed\u30b0\u30e9\u30e0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 # coding:utf-8 # tensorflow version1.0.0 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder name\u3092\u3064\u3051\u3066\u304a\u304f X = tf . placeholder ( tf . float32 , shape = ( None , 4 ), name = \"input\" ) # \u96a0\u308c\u5c64\u306e\u30ce\u30fc\u30c9\u6570 node_num = 1024 w_hidden = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b_hidden = tf . Variable ( tf . zeros ([ node_num ])) f_hidden = tf . matmul ( X , w_hidden ) + b_hidden hidden_layer = tf . nn . relu ( f_hidden ) # \u51fa\u529b\u5c64 w_output = tf . Variable ( tf . zeros ([ node_num , 3 ])) b_output = tf . Variable ( tf . zeros ([ 3 ])) f_output = tf . matmul ( hidden_layer , w_output ) + b_output #\u51fa\u529b\u7d50\u679c\u306e\u5024\u304c\u5165\u308b name\u3092\u3064\u3051\u3066\u304a\u304f p = tf . nn . softmax ( f_output , name = \"output\" ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.001 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) saver = tf . train . Saver () run_metadata = tf . RunMetadata () ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : #\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u4e8b\u524d\u306b\u4f5c\u6210\u3057\u3066\u304a\u304f ckpt = tf . train . get_checkpoint_state ( './ckpt-iris' ) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) else : print ( \"initialization\" ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \"Hidden_layer_wights\" , w_hidden ) tf . summary . histogram ( \"Hidden_layer_biases\" , b_hidden ) tf . summary . histogram ( \"Output_layer_wights\" , w_output ) tf . summary . histogram ( \"Output_layer_wights\" , b_output ) tf . summary . scalar ( \"Accuracy\" , accuracy ) tf . summary . scalar ( \"Loss\" , loss ) summary = tf . summary . merge_all () writer = tf . summary . FileWriter ( \"./iris_cassification_log\" , sess . graph ) #\u521d\u671f\u5316 sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 5000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_summary , train_loss , train_acc = sess . run ([ summary , loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) writer . add_summary ( train_summary , i ) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) saver . save ( sess , \"iris-model\" ) sess . close () \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf(pb\u30d5\u30a1\u30a4\u30eb)\u3092\u4f5c\u6210\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # coding:utf-8 # tensorflow version1.0.0 import tensorflow as tf from tensorflow.python.framework import graph_util #\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3059\u308b def freeze_graph ( model_folder ): checkpoint = tf . train . get_checkpoint_state ( model_folder ) input_checkpoint = checkpoint . model_checkpoint_path output_graph = model_folder + \"/frozen_model.pb\" print ( output_graph ) output_node_names = \"output,input\" #\u5b66\u7fd2\u6642\u306b\u8a08\u7b97\u306bcpu\u3084gpu\u306e\u6307\u5b9a\u3092\u884c\u306a\u3063\u3066\u3044\u305f\u6642\u3001\u8aad\u307f\u8fbc\u3080\u5074\u3067\u305d\u306e\u6307\u5b9a\u306b\u4f9d\u5b58\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u308b clear_devices = True #\u30b0\u30e9\u30d5\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b saver = tf . train . import_meta_graph ( input_checkpoint + '.meta' , clear_devices = clear_devices ) graph = tf . get_default_graph () input_graph_def = graph . as_graph_def () with tf . Session () as sess : #\u4fdd\u5b58\u3055\u308c\u3066\u3044\u308b\u91cd\u307f\u3084\u30d0\u30a4\u30a2\u30b9\u306e\u5909\u6570\u3092\u5fa9\u5143\u3059\u308b saver . restore ( sess , input_checkpoint ) output_graph_def = graph_util . convert_variables_to_constants ( sess , input_graph_def , output_node_names . split ( \",\" ) ) #\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u66f8\u304d\u51fa\u3059 with tf . gfile . GFile ( output_graph , \"wb\" ) as f : f . write ( output_graph_def . SerializeToString ()) print ( \" %d ops in the final graph.\" % len ( output_graph_def . node )) \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092Android\u3067\u8aad\u307f\u8fbc\u307f\u30c6\u30b9\u30c8\u3059\u308b pb\u30d5\u30a1\u30a4\u30eb\u306e\u7f6e\u304d\u5834\u6240\u306b\u3064\u3044\u3066\u306f\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc\u306e\u8a18\u4e8b\u3092\u53c2\u7167 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 import android.content.DialogInterface ; import android.content.res.AssetManager ; import android.support.v7.app.AlertDialog ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.view.View ; import android.widget.Button ; import android.widget.EditText ; import android.widget.TextView ; //\u5c0e\u5165\u306b\u3064\u3044\u3066\u306f\u958b\u767a\u74b0\u5883\u69cb\u7bc9(Android)\u3092\u53c2\u7167 import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { //\u5224\u5b9a\u7d50\u679c\u3092\u8868\u793a\u3059\u308b\u30c6\u30ad\u30b9\u30c8\u30d3\u30e5\u30fc private TextView ansView ; //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u30c6\u30ad\u30b9\u30c8\u30d5\u30a9\u30fc\u30e0 private EditText editIrisFeature1 ; private EditText editIrisFeature2 ; private EditText editIrisFeature3 ; private EditText editIrisFeature4 ; //\u5224\u5b9a\u3092\u958b\u59cb\u3059\u308b\u30dc\u30bf\u30f3 private Button detectButton ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); ansView = ( TextView ) findViewById ( R . id . answer_text ); editIrisFeature1 = ( EditText ) findViewById ( R . id . edit_Iris_feature1 ); editIrisFeature2 = ( EditText ) findViewById ( R . id . edit_Iris_feature2 ); editIrisFeature3 = ( EditText ) findViewById ( R . id . edit_Iris_feature3 ); editIrisFeature4 = ( EditText ) findViewById ( R . id . edit_Iris_feature4 ); detectButton = ( Button ) findViewById ( R . id . detect_Button ); //\u30dc\u30bf\u30f3\u3092\u62bc\u3057\u305f\u6642\u306e\u30a4\u30d9\u30f3\u30c8 detectButton . setOnClickListener ( new View . OnClickListener (){ @Override public void onClick ( View v ) { \u3000 //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3067\u5224\u5b9a\u306b\u4f7f\u3046 float [] features = new float [ 4 ] ; //\u5165\u529b\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a9\u30fc\u30e0\u304b\u3089\u5f97\u308b\u5024\u3092\u53d6\u5f97\u3059\u308b String value1 = null ; String value2 = null ; String value3 = null ; String value4 = null ; Boolean flag = false ; try { value1 = editIrisFeature1 . getText (). toString (); value2 = editIrisFeature2 . getText (). toString (); value3 = editIrisFeature3 . getText (). toString (); value4 = editIrisFeature4 . getText (). toString (); features [ 0 ] = Float . valueOf ( value1 ); features [ 1 ] = Float . valueOf ( value2 ); features [ 2 ] = Float . valueOf ( value3 ); features [ 3 ] = Float . valueOf ( value4 ); } catch ( java . lang . NumberFormatException e ){ //\u672a\u5165\u529b\u306e\u307e\u307e\u958b\u59cb\u3057\u305f\u6642\u30a8\u30e9\u30fc\u30c0\u30a4\u30a2\u30ed\u30b0\u3092\u8868\u793a\u3059\u308b\u3088\u3046\u306b\u3059\u308b flag = true ; ansView . setText ( \"\" ); AlertDialog . Builder alertDialog = new AlertDialog . Builder ( MainActivity . this ); alertDialog . setTitle ( \"input error\" ); alertDialog . setMessage ( \"Please input values\" ); alertDialog . setPositiveButton ( \"OK\" , new DialogInterface . OnClickListener () { public void onClick ( DialogInterface dialog , int which ) { } }); alertDialog . create (); alertDialog . show (); } if ( ! flag ){ //\u5224\u5b9a\u3092\u958b\u59cb\u3059\u308b onDetectClicked ( features ); } } }); } private void onDetectClicked ( float [] f ) { TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); //\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/frozen_model.pb\" ); ansView . setText ( \"\" ); //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b //\u7b2c\u4e00\u5f15\u6570\u306b\u306f\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306b\u6307\u5b9a\u3057\u305f\u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder\u306ename\u3092\u6307\u5b9a\u3059\u308b //\u7b2c\u4e8c\u5f15\u6570\u306b\u306fshape\u3092\u6307\u5b9a\u3059\u308b\u3000\u4eca\u56de\u306f\u4e00\u3064\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u3066\u30c6\u30b9\u30c8\u3059\u308b\u306e\u3067(1,4) //\u7b2c\u4e09\u5f15\u6570\u306f\u5165\u529b\u30c7\u30fc\u30bf mTensorFlowIF . fillNodeFloat ( \"input:0\" , new int [] { 1 , 4 }, f ); //\u5224\u5b9a\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217 float [] result_value = new float [ 3 ] ; //\u5224\u5b9a\u3092\u884c\u3046\u3053\u306e\u6642\u306b\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306e\u51fa\u529b\u306ename\u3092\u6307\u5b9a\u3059\u308b mTensorFlowIF . runInference ( new String [] { \"output:0\" }); //result_value\u306b\u7d50\u679c\u3092\u4ee3\u5165\u3059\u308b mTensorFlowIF . readNodeFloat ( \"output:0\" , result_value ); //result_value\u306b\u306f[a,b,c]\u3068\u5024\u304c\u5165\u3063\u3066\u304a\u308a\u3001\u4e00\u756a\u5927\u304d\u306a\u5024\u304c\u5165\u3063\u3066\u308b\u914d\u5217\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u5165\u529b\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u3068\u306a\u308b int ansIndex = getAnswer ( result_value ); switch ( ansIndex ){ case 0 : ansView . setText ( \"Detected : Iris-Setosa\" ); break ; case 1 : ansView . setText ( \"Detected : Iris-versicolor\" ); break ; case 2 : ansView . setText ( \"Detected : Iris-virginica\" ); break ; } } //\u4e00\u756a\u5927\u304d\u306a\u5024\u304c\u5165\u3063\u3066\u3044\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059 private int getAnswer ( float [] f ){ int argmax = 0 ; float max = f [ 0 ] ; for ( int i = 0 ; i < f . length ; i ++ ){ if ( max < f [ i ] ){ max = f [ i ] ; argmax = i ; } } return argmax ; } } activity_main.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 <?xml version=\"1.0\" encoding=\"utf-8\"?> <RelativeLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" xmlns:tools= \"http://schemas.android.com/tools\" android:id= \"@+id/activity_main\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingBottom= \"@dimen/activity_vertical_margin\" android:paddingLeft= \"@dimen/activity_horizontal_margin\" android:paddingRight= \"@dimen/activity_horizontal_margin\" android:paddingTop= \"@dimen/activity_vertical_margin\" tools:context= \"\u5404\u81ea\u306eproject\u540d\" > <LinearLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" android:orientation= \"horizontal\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingTop= \"16dp\" > <EditText android:id= \"@+id/edit_Iris_feature1\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature2\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature3\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature4\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> </LinearLayout> <Button android:id= \"@+id/detect_Button\" android:text= \"Button\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_centerHorizontal= \"true\" android:layout_marginTop= \"70dp\" /> <TextView android:id= \"@+id/answer_text\" android:layout_width= \"match_parent\" android:layout_height= \"wrap_content\" android:layout_below= \"@+id/detect_Button\" android:textSize= \"30sp\" android:layout_margin= \"30dp\" android:gravity= \"center\" /> </RelativeLayout> build.gradle android\u306e\u6240\u306b\u4e0b\u8a18\u3092\u8ffd\u52a0\u3057\u3066\u304a\u304f 1 2 3 4 5 6 sourceSets { main { jniLibs.srcDirs = ['libs'] assets.srcDirs = ['assets'] } } \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u30b5\u30f3\u30d7\u30eb\u3092\u9078\u3073\u3001\u6b63\u89e3\u30e9\u30d9\u30eb\u304c\u8fd4\u3063\u3066\u304d\u305f\u3089\u6210\u529f","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc \u305d\u306e2"},{"location":"android/load_model2/#2","text":"Android\u30a2\u30d7\u30ea\u3067\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 \u4eca\u56de\u306f\u30a2\u30e4\u30e1\u306e\u5224\u5b9a\u3092\u884c\u3046\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3001\u305d\u306e\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u30c6\u30b9\u30c8\u3092\u884c\u3046 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u3064\u3044\u3066\u306f\u4ee5\u4e0b\u304b\u3089\u5165\u624b\u3059\u308b 1 $ curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc \u305d\u306e2"},{"location":"android/load_model2/#_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 # coding:utf-8 # tensorflow version1.0.0 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder name\u3092\u3064\u3051\u3066\u304a\u304f X = tf . placeholder ( tf . float32 , shape = ( None , 4 ), name = \"input\" ) # \u96a0\u308c\u5c64\u306e\u30ce\u30fc\u30c9\u6570 node_num = 1024 w_hidden = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b_hidden = tf . Variable ( tf . zeros ([ node_num ])) f_hidden = tf . matmul ( X , w_hidden ) + b_hidden hidden_layer = tf . nn . relu ( f_hidden ) # \u51fa\u529b\u5c64 w_output = tf . Variable ( tf . zeros ([ node_num , 3 ])) b_output = tf . Variable ( tf . zeros ([ 3 ])) f_output = tf . matmul ( hidden_layer , w_output ) + b_output #\u51fa\u529b\u7d50\u679c\u306e\u5024\u304c\u5165\u308b name\u3092\u3064\u3051\u3066\u304a\u304f p = tf . nn . softmax ( f_output , name = \"output\" ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.001 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) saver = tf . train . Saver () run_metadata = tf . RunMetadata () ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : #\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306f\u4e8b\u524d\u306b\u4f5c\u6210\u3057\u3066\u304a\u304f ckpt = tf . train . get_checkpoint_state ( './ckpt-iris' ) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) else : print ( \"initialization\" ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \"Hidden_layer_wights\" , w_hidden ) tf . summary . histogram ( \"Hidden_layer_biases\" , b_hidden ) tf . summary . histogram ( \"Output_layer_wights\" , w_output ) tf . summary . histogram ( \"Output_layer_wights\" , b_output ) tf . summary . scalar ( \"Accuracy\" , accuracy ) tf . summary . scalar ( \"Loss\" , loss ) summary = tf . summary . merge_all () writer = tf . summary . FileWriter ( \"./iris_cassification_log\" , sess . graph ) #\u521d\u671f\u5316 sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 5000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_summary , train_loss , train_acc = sess . run ([ summary , loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) writer . add_summary ( train_summary , i ) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) saver . save ( sess , \"iris-model\" ) sess . close ()","title":"\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u30d7\u30ed\u30b0\u30e9\u30e0"},{"location":"android/load_model2/#pb","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # coding:utf-8 # tensorflow version1.0.0 import tensorflow as tf from tensorflow.python.framework import graph_util #\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3059\u308b def freeze_graph ( model_folder ): checkpoint = tf . train . get_checkpoint_state ( model_folder ) input_checkpoint = checkpoint . model_checkpoint_path output_graph = model_folder + \"/frozen_model.pb\" print ( output_graph ) output_node_names = \"output,input\" #\u5b66\u7fd2\u6642\u306b\u8a08\u7b97\u306bcpu\u3084gpu\u306e\u6307\u5b9a\u3092\u884c\u306a\u3063\u3066\u3044\u305f\u6642\u3001\u8aad\u307f\u8fbc\u3080\u5074\u3067\u305d\u306e\u6307\u5b9a\u306b\u4f9d\u5b58\u3057\u306a\u3044\u3088\u3046\u306b\u3059\u308b clear_devices = True #\u30b0\u30e9\u30d5\u3092\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b saver = tf . train . import_meta_graph ( input_checkpoint + '.meta' , clear_devices = clear_devices ) graph = tf . get_default_graph () input_graph_def = graph . as_graph_def () with tf . Session () as sess : #\u4fdd\u5b58\u3055\u308c\u3066\u3044\u308b\u91cd\u307f\u3084\u30d0\u30a4\u30a2\u30b9\u306e\u5909\u6570\u3092\u5fa9\u5143\u3059\u308b saver . restore ( sess , input_checkpoint ) output_graph_def = graph_util . convert_variables_to_constants ( sess , input_graph_def , output_node_names . split ( \",\" ) ) #\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u66f8\u304d\u51fa\u3059 with tf . gfile . GFile ( output_graph , \"wb\" ) as f : f . write ( output_graph_def . SerializeToString ()) print ( \" %d ops in the final graph.\" % len ( output_graph_def . node ))","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf(pb\u30d5\u30a1\u30a4\u30eb)\u3092\u4f5c\u6210\u3059\u308b"},{"location":"android/load_model2/#android","text":"pb\u30d5\u30a1\u30a4\u30eb\u306e\u7f6e\u304d\u5834\u6240\u306b\u3064\u3044\u3066\u306f\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc\u306e\u8a18\u4e8b\u3092\u53c2\u7167 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 import android.content.DialogInterface ; import android.content.res.AssetManager ; import android.support.v7.app.AlertDialog ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.view.View ; import android.widget.Button ; import android.widget.EditText ; import android.widget.TextView ; //\u5c0e\u5165\u306b\u3064\u3044\u3066\u306f\u958b\u767a\u74b0\u5883\u69cb\u7bc9(Android)\u3092\u53c2\u7167 import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { //\u5224\u5b9a\u7d50\u679c\u3092\u8868\u793a\u3059\u308b\u30c6\u30ad\u30b9\u30c8\u30d3\u30e5\u30fc private TextView ansView ; //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u30c6\u30ad\u30b9\u30c8\u30d5\u30a9\u30fc\u30e0 private EditText editIrisFeature1 ; private EditText editIrisFeature2 ; private EditText editIrisFeature3 ; private EditText editIrisFeature4 ; //\u5224\u5b9a\u3092\u958b\u59cb\u3059\u308b\u30dc\u30bf\u30f3 private Button detectButton ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); ansView = ( TextView ) findViewById ( R . id . answer_text ); editIrisFeature1 = ( EditText ) findViewById ( R . id . edit_Iris_feature1 ); editIrisFeature2 = ( EditText ) findViewById ( R . id . edit_Iris_feature2 ); editIrisFeature3 = ( EditText ) findViewById ( R . id . edit_Iris_feature3 ); editIrisFeature4 = ( EditText ) findViewById ( R . id . edit_Iris_feature4 ); detectButton = ( Button ) findViewById ( R . id . detect_Button ); //\u30dc\u30bf\u30f3\u3092\u62bc\u3057\u305f\u6642\u306e\u30a4\u30d9\u30f3\u30c8 detectButton . setOnClickListener ( new View . OnClickListener (){ @Override public void onClick ( View v ) { \u3000 //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3067\u5224\u5b9a\u306b\u4f7f\u3046 float [] features = new float [ 4 ] ; //\u5165\u529b\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a9\u30fc\u30e0\u304b\u3089\u5f97\u308b\u5024\u3092\u53d6\u5f97\u3059\u308b String value1 = null ; String value2 = null ; String value3 = null ; String value4 = null ; Boolean flag = false ; try { value1 = editIrisFeature1 . getText (). toString (); value2 = editIrisFeature2 . getText (). toString (); value3 = editIrisFeature3 . getText (). toString (); value4 = editIrisFeature4 . getText (). toString (); features [ 0 ] = Float . valueOf ( value1 ); features [ 1 ] = Float . valueOf ( value2 ); features [ 2 ] = Float . valueOf ( value3 ); features [ 3 ] = Float . valueOf ( value4 ); } catch ( java . lang . NumberFormatException e ){ //\u672a\u5165\u529b\u306e\u307e\u307e\u958b\u59cb\u3057\u305f\u6642\u30a8\u30e9\u30fc\u30c0\u30a4\u30a2\u30ed\u30b0\u3092\u8868\u793a\u3059\u308b\u3088\u3046\u306b\u3059\u308b flag = true ; ansView . setText ( \"\" ); AlertDialog . Builder alertDialog = new AlertDialog . Builder ( MainActivity . this ); alertDialog . setTitle ( \"input error\" ); alertDialog . setMessage ( \"Please input values\" ); alertDialog . setPositiveButton ( \"OK\" , new DialogInterface . OnClickListener () { public void onClick ( DialogInterface dialog , int which ) { } }); alertDialog . create (); alertDialog . show (); } if ( ! flag ){ //\u5224\u5b9a\u3092\u958b\u59cb\u3059\u308b onDetectClicked ( features ); } } }); } private void onDetectClicked ( float [] f ) { TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); //\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/frozen_model.pb\" ); ansView . setText ( \"\" ); //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b //\u7b2c\u4e00\u5f15\u6570\u306b\u306f\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306b\u6307\u5b9a\u3057\u305f\u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder\u306ename\u3092\u6307\u5b9a\u3059\u308b //\u7b2c\u4e8c\u5f15\u6570\u306b\u306fshape\u3092\u6307\u5b9a\u3059\u308b\u3000\u4eca\u56de\u306f\u4e00\u3064\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u3066\u30c6\u30b9\u30c8\u3059\u308b\u306e\u3067(1,4) //\u7b2c\u4e09\u5f15\u6570\u306f\u5165\u529b\u30c7\u30fc\u30bf mTensorFlowIF . fillNodeFloat ( \"input:0\" , new int [] { 1 , 4 }, f ); //\u5224\u5b9a\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217 float [] result_value = new float [ 3 ] ; //\u5224\u5b9a\u3092\u884c\u3046\u3053\u306e\u6642\u306b\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306e\u51fa\u529b\u306ename\u3092\u6307\u5b9a\u3059\u308b mTensorFlowIF . runInference ( new String [] { \"output:0\" }); //result_value\u306b\u7d50\u679c\u3092\u4ee3\u5165\u3059\u308b mTensorFlowIF . readNodeFloat ( \"output:0\" , result_value ); //result_value\u306b\u306f[a,b,c]\u3068\u5024\u304c\u5165\u3063\u3066\u304a\u308a\u3001\u4e00\u756a\u5927\u304d\u306a\u5024\u304c\u5165\u3063\u3066\u308b\u914d\u5217\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u5165\u529b\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u3068\u306a\u308b int ansIndex = getAnswer ( result_value ); switch ( ansIndex ){ case 0 : ansView . setText ( \"Detected : Iris-Setosa\" ); break ; case 1 : ansView . setText ( \"Detected : Iris-versicolor\" ); break ; case 2 : ansView . setText ( \"Detected : Iris-virginica\" ); break ; } } //\u4e00\u756a\u5927\u304d\u306a\u5024\u304c\u5165\u3063\u3066\u3044\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059 private int getAnswer ( float [] f ){ int argmax = 0 ; float max = f [ 0 ] ; for ( int i = 0 ; i < f . length ; i ++ ){ if ( max < f [ i ] ){ max = f [ i ] ; argmax = i ; } } return argmax ; } }","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092Android\u3067\u8aad\u307f\u8fbc\u307f\u30c6\u30b9\u30c8\u3059\u308b"},{"location":"android/load_model2/#activity_mainxml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 <?xml version=\"1.0\" encoding=\"utf-8\"?> <RelativeLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" xmlns:tools= \"http://schemas.android.com/tools\" android:id= \"@+id/activity_main\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingBottom= \"@dimen/activity_vertical_margin\" android:paddingLeft= \"@dimen/activity_horizontal_margin\" android:paddingRight= \"@dimen/activity_horizontal_margin\" android:paddingTop= \"@dimen/activity_vertical_margin\" tools:context= \"\u5404\u81ea\u306eproject\u540d\" > <LinearLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" android:orientation= \"horizontal\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingTop= \"16dp\" > <EditText android:id= \"@+id/edit_Iris_feature1\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature2\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature3\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature4\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> </LinearLayout> <Button android:id= \"@+id/detect_Button\" android:text= \"Button\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_centerHorizontal= \"true\" android:layout_marginTop= \"70dp\" /> <TextView android:id= \"@+id/answer_text\" android:layout_width= \"match_parent\" android:layout_height= \"wrap_content\" android:layout_below= \"@+id/detect_Button\" android:textSize= \"30sp\" android:layout_margin= \"30dp\" android:gravity= \"center\" /> </RelativeLayout> build.gradle android\u306e\u6240\u306b\u4e0b\u8a18\u3092\u8ffd\u52a0\u3057\u3066\u304a\u304f 1 2 3 4 5 6 sourceSets { main { jniLibs.srcDirs = ['libs'] assets.srcDirs = ['assets'] } } \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u30b5\u30f3\u30d7\u30eb\u3092\u9078\u3073\u3001\u6b63\u89e3\u30e9\u30d9\u30eb\u304c\u8fd4\u3063\u3066\u304d\u305f\u3089\u6210\u529f","title":"activity_main.xml"},{"location":"android/load_model3/","text":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc tensorflow1.0.1 Android\u30a2\u30d7\u30ea\u3067\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 Tensorflow\u306eversion\u304c1.0.1\u3060\u3068API\u306e\u4ed5\u69d8\u304c\u5909\u66f4\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u305d\u306e2\u306e\u8a18\u4e8b\u306e\u65b9\u306f\u52d5\u304b\u306a\u3044 \u4ee5\u4e0b\u304cversion1.0.1\u4ed5\u69d8\u306eMainActivity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 package com.example.yamikachan.irisdetector ; import android.content.DialogInterface ; import android.content.res.AssetManager ; import android.support.v7.app.AlertDialog ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.view.View ; import android.widget.Button ; import android.widget.EditText ; import android.widget.TextView ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private TextView ansView ; private EditText editIrisFeature1 ; private EditText editIrisFeature2 ; private EditText editIrisFeature3 ; private EditText editIrisFeature4 ; private Button detectButton ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); ansView = ( TextView ) findViewById ( R . id . answer_text ); editIrisFeature1 = ( EditText ) findViewById ( R . id . edit_Iris_feature1 ); editIrisFeature2 = ( EditText ) findViewById ( R . id . edit_Iris_feature2 ); editIrisFeature3 = ( EditText ) findViewById ( R . id . edit_Iris_feature3 ); editIrisFeature4 = ( EditText ) findViewById ( R . id . edit_Iris_feature4 ); detectButton = ( Button ) findViewById ( R . id . detect_Button ); detectButton . setOnClickListener ( new View . OnClickListener (){ @Override public void onClick ( View v ) { float [] features = new float [ 4 ] ; String value1 = null ; String value2 = null ; String value3 = null ; String value4 = null ; Boolean flag = false ; try { value1 = editIrisFeature1 . getText (). toString (); value2 = editIrisFeature2 . getText (). toString (); value3 = editIrisFeature3 . getText (). toString (); value4 = editIrisFeature4 . getText (). toString (); features [ 0 ] = Float . valueOf ( value1 ); features [ 1 ] = Float . valueOf ( value2 ); features [ 2 ] = Float . valueOf ( value3 ); features [ 3 ] = Float . valueOf ( value4 ); } catch ( java . lang . NumberFormatException e ){ flag = true ; ansView . setText ( \"\" ); AlertDialog . Builder alertDialog = new AlertDialog . Builder ( MainActivity . this ); alertDialog . setTitle ( \"input error\" ); alertDialog . setMessage ( \"Please input values\" ); alertDialog . setPositiveButton ( \"OK\" , new DialogInterface . OnClickListener () { public void onClick ( DialogInterface dialog , int which ) { } }); alertDialog . create (); alertDialog . show (); } if ( ! flag ){ onDetectClicked ( features ); } } }); } private void onDetectClicked ( float [] f ) { AssetManager mAssetManager = getAssets (); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface ( mAssetManager , \"file:///android_asset/frozen_model.pb\" ); ansView . setText ( \"\" ); //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b //\u7b2c\u4e00\u5f15\u6570\u306b\u306f\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306b\u6307\u5b9a\u3057\u305f\u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder\u306ename\u3092\u6307\u5b9a\u3059\u308b //\u7b2c\u4e8c\u5f15\u6570\u306b\u306f\u5165\u529b\u30c7\u30fc\u30bf //\u7b2c\u4e09\u5f15\u6570\u306b\u306fshape\u3092\u6307\u5b9a\u3059\u308b\u3000\u4eca\u56de\u306f\u4e00\u3064\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u3066\u30c6\u30b9\u30c8\u3059\u308b\u306e\u3067(1,4) mTensorFlowIF . feed ( \"input:0\" , f , new long [] { 1 , 4 }); //\u5224\u5b9a\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217 float [] result_value = new float [ 3 ] ; //\u5224\u5b9a\u3092\u884c\u3046\u3053\u306e\u6642\u306b\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306e\u51fa\u529b\u306ename\u3092\u6307\u5b9a\u3059\u308b mTensorFlowIF . run ( new String [] { \"output:0\" }); //result_value\u306b\u7d50\u679c\u3092\u4ee3\u5165\u3059\u308b mTensorFlowIF . fetch ( \"output:0\" , result_value ); int ansIndex = getAnswer ( result_value ); switch ( ansIndex ){ case 0 : ansView . setText ( \"Detected : Iris-Setosa\" ); break ; case 1 : ansView . setText ( \"Detected : Iris-versicolor\" ); break ; case 2 : ansView . setText ( \"Detected : Iris-virginica\" ); break ; } } private int getAnswer ( float [] f ){ int argmax = 0 ; float max = f [ 0 ] ; for ( int i = 0 ; i < f . length ; i ++ ){ if ( max < f [ i ] ){ max = f [ i ] ; argmax = i ; } } return argmax ; } } activity_main.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 <?xml version=\"1.0\" encoding=\"utf-8\"?> <RelativeLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" xmlns:tools= \"http://schemas.android.com/tools\" android:id= \"@+id/activity_main\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingBottom= \"@dimen/activity_vertical_margin\" android:paddingLeft= \"@dimen/activity_horizontal_margin\" android:paddingRight= \"@dimen/activity_horizontal_margin\" android:paddingTop= \"@dimen/activity_vertical_margin\" tools:context= \"\u5404\u81ea\u306eproject\u540d\" > <LinearLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" android:orientation= \"horizontal\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingTop= \"16dp\" > <EditText android:id= \"@+id/edit_Iris_feature1\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature2\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature3\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature4\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> </LinearLayout> <Button android:id= \"@+id/detect_Button\" android:text= \"Button\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_centerHorizontal= \"true\" android:layout_marginTop= \"70dp\" /> <TextView android:id= \"@+id/answer_text\" android:layout_width= \"match_parent\" android:layout_height= \"wrap_content\" android:layout_below= \"@+id/detect_Button\" android:textSize= \"30sp\" android:layout_margin= \"30dp\" android:gravity= \"center\" /> </RelativeLayout> build.gradle android\u306e\u6240\u306b\u4e0b\u8a18\u3092\u8ffd\u52a0\u3057\u3066\u304a\u304f 1 2 3 4 5 6 sourceSets { main { jniLibs.srcDirs = ['libs'] assets.srcDirs = ['assets'] } } \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u30b5\u30f3\u30d7\u30eb\u3092\u9078\u3073\u3001\u6b63\u89e3\u30e9\u30d9\u30eb\u304c\u8fd4\u3063\u3066\u304d\u305f\u3089\u6210\u529f","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc tensorflow1.0.1"},{"location":"android/load_model3/#tensorflow101","text":"Android\u30a2\u30d7\u30ea\u3067\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 Tensorflow\u306eversion\u304c1.0.1\u3060\u3068API\u306e\u4ed5\u69d8\u304c\u5909\u66f4\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u305d\u306e2\u306e\u8a18\u4e8b\u306e\u65b9\u306f\u52d5\u304b\u306a\u3044 \u4ee5\u4e0b\u304cversion1.0.1\u4ed5\u69d8\u306eMainActivity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 package com.example.yamikachan.irisdetector ; import android.content.DialogInterface ; import android.content.res.AssetManager ; import android.support.v7.app.AlertDialog ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.view.View ; import android.widget.Button ; import android.widget.EditText ; import android.widget.TextView ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private TextView ansView ; private EditText editIrisFeature1 ; private EditText editIrisFeature2 ; private EditText editIrisFeature3 ; private EditText editIrisFeature4 ; private Button detectButton ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); ansView = ( TextView ) findViewById ( R . id . answer_text ); editIrisFeature1 = ( EditText ) findViewById ( R . id . edit_Iris_feature1 ); editIrisFeature2 = ( EditText ) findViewById ( R . id . edit_Iris_feature2 ); editIrisFeature3 = ( EditText ) findViewById ( R . id . edit_Iris_feature3 ); editIrisFeature4 = ( EditText ) findViewById ( R . id . edit_Iris_feature4 ); detectButton = ( Button ) findViewById ( R . id . detect_Button ); detectButton . setOnClickListener ( new View . OnClickListener (){ @Override public void onClick ( View v ) { float [] features = new float [ 4 ] ; String value1 = null ; String value2 = null ; String value3 = null ; String value4 = null ; Boolean flag = false ; try { value1 = editIrisFeature1 . getText (). toString (); value2 = editIrisFeature2 . getText (). toString (); value3 = editIrisFeature3 . getText (). toString (); value4 = editIrisFeature4 . getText (). toString (); features [ 0 ] = Float . valueOf ( value1 ); features [ 1 ] = Float . valueOf ( value2 ); features [ 2 ] = Float . valueOf ( value3 ); features [ 3 ] = Float . valueOf ( value4 ); } catch ( java . lang . NumberFormatException e ){ flag = true ; ansView . setText ( \"\" ); AlertDialog . Builder alertDialog = new AlertDialog . Builder ( MainActivity . this ); alertDialog . setTitle ( \"input error\" ); alertDialog . setMessage ( \"Please input values\" ); alertDialog . setPositiveButton ( \"OK\" , new DialogInterface . OnClickListener () { public void onClick ( DialogInterface dialog , int which ) { } }); alertDialog . create (); alertDialog . show (); } if ( ! flag ){ onDetectClicked ( features ); } } }); } private void onDetectClicked ( float [] f ) { AssetManager mAssetManager = getAssets (); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface ( mAssetManager , \"file:///android_asset/frozen_model.pb\" ); ansView . setText ( \"\" ); //\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b //\u7b2c\u4e00\u5f15\u6570\u306b\u306f\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306b\u6307\u5b9a\u3057\u305f\u5165\u529b\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder\u306ename\u3092\u6307\u5b9a\u3059\u308b //\u7b2c\u4e8c\u5f15\u6570\u306b\u306f\u5165\u529b\u30c7\u30fc\u30bf //\u7b2c\u4e09\u5f15\u6570\u306b\u306fshape\u3092\u6307\u5b9a\u3059\u308b\u3000\u4eca\u56de\u306f\u4e00\u3064\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u3066\u30c6\u30b9\u30c8\u3059\u308b\u306e\u3067(1,4) mTensorFlowIF . feed ( \"input:0\" , f , new long [] { 1 , 4 }); //\u5224\u5b9a\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217 float [] result_value = new float [ 3 ] ; //\u5224\u5b9a\u3092\u884c\u3046\u3053\u306e\u6642\u306b\u30e2\u30c7\u30eb\u4f5c\u6210\u6642\u306e\u51fa\u529b\u306ename\u3092\u6307\u5b9a\u3059\u308b mTensorFlowIF . run ( new String [] { \"output:0\" }); //result_value\u306b\u7d50\u679c\u3092\u4ee3\u5165\u3059\u308b mTensorFlowIF . fetch ( \"output:0\" , result_value ); int ansIndex = getAnswer ( result_value ); switch ( ansIndex ){ case 0 : ansView . setText ( \"Detected : Iris-Setosa\" ); break ; case 1 : ansView . setText ( \"Detected : Iris-versicolor\" ); break ; case 2 : ansView . setText ( \"Detected : Iris-virginica\" ); break ; } } private int getAnswer ( float [] f ){ int argmax = 0 ; float max = f [ 0 ] ; for ( int i = 0 ; i < f . length ; i ++ ){ if ( max < f [ i ] ){ max = f [ i ] ; argmax = i ; } } return argmax ; } }","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u8aad\u8fbc tensorflow1.0.1"},{"location":"android/load_model3/#activity_mainxml","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 <?xml version=\"1.0\" encoding=\"utf-8\"?> <RelativeLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" xmlns:tools= \"http://schemas.android.com/tools\" android:id= \"@+id/activity_main\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingBottom= \"@dimen/activity_vertical_margin\" android:paddingLeft= \"@dimen/activity_horizontal_margin\" android:paddingRight= \"@dimen/activity_horizontal_margin\" android:paddingTop= \"@dimen/activity_vertical_margin\" tools:context= \"\u5404\u81ea\u306eproject\u540d\" > <LinearLayout xmlns:android= \"http://schemas.android.com/apk/res/android\" android:orientation= \"horizontal\" android:layout_width= \"match_parent\" android:layout_height= \"match_parent\" android:paddingTop= \"16dp\" > <EditText android:id= \"@+id/edit_Iris_feature1\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature2\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature3\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> <EditText android:id= \"@+id/edit_Iris_feature4\" android:inputType= \"numberDecimal\" android:layout_width= \"80dp\" android:layout_height= \"wrap_content\" android:background= \"#ffffff\" android:layout_marginLeft= \"5dp\" android:layout_marginRight= \"5dp\" /> </LinearLayout> <Button android:id= \"@+id/detect_Button\" android:text= \"Button\" android:layout_width= \"wrap_content\" android:layout_height= \"wrap_content\" android:layout_centerHorizontal= \"true\" android:layout_marginTop= \"70dp\" /> <TextView android:id= \"@+id/answer_text\" android:layout_width= \"match_parent\" android:layout_height= \"wrap_content\" android:layout_below= \"@+id/detect_Button\" android:textSize= \"30sp\" android:layout_margin= \"30dp\" android:gravity= \"center\" /> </RelativeLayout> build.gradle android\u306e\u6240\u306b\u4e0b\u8a18\u3092\u8ffd\u52a0\u3057\u3066\u304a\u304f 1 2 3 4 5 6 sourceSets { main { jniLibs.srcDirs = ['libs'] assets.srcDirs = ['assets'] } } \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u30b5\u30f3\u30d7\u30eb\u3092\u9078\u3073\u3001\u6b63\u89e3\u30e9\u30d9\u30eb\u304c\u8fd4\u3063\u3066\u304d\u305f\u3089\u6210\u529f","title":"activity_main.xml"},{"location":"android/load_trained/","text":"","title":"Load trained"},{"location":"android/load_trained/#_1","text":"","title":""},{"location":"android/run/","text":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u5b9f\u884c\u3059\u308b \u305d\u308c\u3067\u306f\u3001 \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f \u3067\u4f5c\u6210\u3057\u305f\u5b66\u7fd2\u6e08\u307f\u30c7\u30fc\u30bf\u3092\u5b9f\u969b\u306bAndroid\u30a2\u30d7\u30ea\u5185\u304b\u3089\u5b9f\u884c\u3057\u307e\u3059\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package io.fabo.helloandroid ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/model.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"result:\" + result ); int [] a_value = new int [ 1 ] ; a_value [ 0 ] = 3 ; int [] b_value = new int [ 1 ] ; b_value [ 0 ] = 4 ; mTensorFlowIF . fillNodeInt ( \"input_a\" , new int [] { 1 }, a_value ); mTensorFlowIF . fillNodeInt ( \"input_b\" , new int [] { 1 }, b_value ); int [] result_value = new int [ 1 ] ; mTensorFlowIF . runInference ( new String [] { \"add_op\" }); mTensorFlowIF . readNodeInt ( \"add_op\" , result_value ); Log . i ( TAG , \"result_value:\" + result_value [ 0 ] ); } }","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u5b9f\u884c\u3059\u308b"},{"location":"android/run/#_1","text":"\u305d\u308c\u3067\u306f\u3001 \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f \u3067\u4f5c\u6210\u3057\u305f\u5b66\u7fd2\u6e08\u307f\u30c7\u30fc\u30bf\u3092\u5b9f\u969b\u306bAndroid\u30a2\u30d7\u30ea\u5185\u304b\u3089\u5b9f\u884c\u3057\u307e\u3059\u3002","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u5b9f\u884c\u3059\u308b"},{"location":"android/run/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package io.fabo.helloandroid ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/model.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"result:\" + result ); int [] a_value = new int [ 1 ] ; a_value [ 0 ] = 3 ; int [] b_value = new int [ 1 ] ; b_value [ 0 ] = 4 ; mTensorFlowIF . fillNodeInt ( \"input_a\" , new int [] { 1 }, a_value ); mTensorFlowIF . fillNodeInt ( \"input_b\" , new int [] { 1 }, b_value ); int [] result_value = new int [ 1 ] ; mTensorFlowIF . runInference ( new String [] { \"add_op\" }); mTensorFlowIF . readNodeInt ( \"add_op\" , result_value ); Log . i ( TAG , \"result_value:\" + result_value [ 0 ] ); } }","title":"Sample"},{"location":"android/version/","text":"Version\u3092\u8868\u793a\u3059\u308b Android\u30a2\u30d7\u30ea\u3067JNI\u3067Bridge\u3055\u308c\u3066\u3044\u308bTensorFlow\u306eVersion\u3092\u8868\u793a\u3059\u308b Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package io.fabo.helloandroid ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.TensorFlow ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); String version = TensorFlow . version (); Log . i ( TAG , \"version\" + version ); } }","title":"Version\u3092\u8868\u793a\u3059\u308b"},{"location":"android/version/#version","text":"Android\u30a2\u30d7\u30ea\u3067JNI\u3067Bridge\u3055\u308c\u3066\u3044\u308bTensorFlow\u306eVersion\u3092\u8868\u793a\u3059\u308b","title":"Version\u3092\u8868\u793a\u3059\u308b"},{"location":"android/version/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package io.fabo.helloandroid ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.TensorFlow ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ) { super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); String version = TensorFlow . version (); Log . i ( TAG , \"version\" + version ); } }","title":"Sample"},{"location":"building_graph/tensorflow_graph_part1/","text":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part1 TensorFlow\u306b\u304a\u3051\u308b\u30b0\u30e9\u30d5(Graph)\u306f tf.Operation \u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u96c6\u5408\u3067\u3042\u308b\u3002\u307e\u305f\u3001tf.Operation\u306fTensor\u3092\u5165\u529b\u304a\u3088\u3073\u51fa\u529b\u306b\u6301\u3064\u30ce\u30fc\u30c9\u3068\u306a\u3063\u3066\u3044\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 1 \u30b0\u30e9\u30d5\u304c\u69cb\u7bc9\u3055\u308c\u308b\u69d8\u5b50\u3092\u78ba\u8a8d\u3059\u308b\u3002 tf.get_default_graph() \u306b\u3088\u308a\u3001TensorFlow\u3067\u4f7f\u308f\u308c\u308b\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30e9\u30d5\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b \u30b0\u30e9\u30d5\u306eOperation\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u306b\u306f\u3001 graph.get_operations() \u3092\u4f7f\u7528\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/usr/bin/env python # coding:utf-8 from __future__ import division from __future__ import print_function from __future__ import unicode_literals import numpy as np import tensorflow as tf # \u30b0\u30e9\u30d5\u306e\u53d6\u5f97 graph = tf . get_default_graph () # \u30ce\u30fc\u30c9\u4e00\u89a7 print ( \"*** STEP1 ***\" , graph . get_operations ()) x = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_x\" ) # 2x2 Tensor print ( \"*** STEP2 ***\" , graph . get_operations ()) y = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_y\" ) # 2x2 Tensor print ( \"*** STEP3 ***\" , graph . get_operations ()) matmul = tf . matmul ( x , y , name = \"matrix_mul\" ) # x\u3068y\u306e\u4e57\u7b97 print ( \"*** STEP4 ***\" , graph . get_operations ()) \u5b9f\u884c\u7d50\u679c : Operation\u30ce\u30fc\u30c9\u304c\u30b0\u30e9\u30d5\u306b\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 1 2 3 4 *** STEP1 *** [] *** STEP2 *** [<tensorflow.python.framework.ops.Operation object at 0x10d325850>] *** STEP3 *** [<tensorflow.python.framework.ops.Operation object at 0x10d325850>, <tensorflow.python.framework.ops.Operation object at 0x10d325bd0>] *** STEP4 *** [<tensorflow.python.framework.ops.Operation object at 0x10d325850>, <tensorflow.python.framework.ops.Operation object at 0x10d325bd0>, <tensorflow.python.framework.ops.Operation object at 0x10d325c50>] \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 2 \u30b0\u30e9\u30d5\u5185\u306e\u30ce\u30fc\u30c9\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u3002 op.name : \u30ce\u30fc\u30c9\u306e\u540d\u524d op.type : \u30ce\u30fc\u30c9\u306e\u578b op.op_def : \u30ce\u30fc\u30c9\u306eProtocol buffer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/usr/bin/env python # coding:utf-8 from __future__ import division from __future__ import print_function from __future__ import unicode_literals import numpy as np import tensorflow as tf # \u30b0\u30e9\u30d5\u306e\u53d6\u5f97 graph = tf . get_default_graph () # \u30ce\u30fc\u30c9\u4e00\u89a7 x = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_x\" ) # 2x2 Tensor y = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_y\" ) # 2x2 Tensor matmul = tf . matmul ( x , y , name = \"matrix_mul\" ) # x\u3068y\u306e\u4e57\u7b97 for op in graph . get_operations (): # \u30ce\u30fc\u30c9\u306e\u540d\u524d print ( \"***name***\" , op . name ) # \u30ce\u30fc\u30c9\u306e\u578b print ( \"***type***\" , op . type ) # \u30ce\u30fc\u30c9\u306e\u60c5\u5831\u3092\u8868\u3059protocol buffer print ( \"***op_def***\" , op . op_def ) \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 *** name *** matrix_x *** type *** Placeholder *** op_def *** name : \"Placeholder\" output_arg { name : \"output\" type_attr : \"dtype\" } attr { name : \"dtype\" type : \"type\" } attr { name : \"shape\" type : \"shape\" default_value { shape { } } } *** name *** matrix_y *** type *** Placeholder *** op_def *** name : \"Placeholder\" output_arg { name : \"output\" type_attr : \"dtype\" } attr { name : \"dtype\" type : \"type\" } attr { name : \"shape\" type : \"shape\" default_value { shape { } } } *** name *** matrix_mul *** type *** MatMul *** op_def *** name : \"MatMul\" input_arg { name : \"a\" type_attr : \"T\" } input_arg { name : \"b\" type_attr : \"T\" } output_arg { name : \"product\" type_attr : \"T\" } attr { name : \"transpose_a\" type : \"bool\" default_value { b : false } } attr { name : \"transpose_b\" type : \"bool\" default_value { b : false } } attr { name : \"T\" type : \"type\" allowed_values { list { type : DT_HALF type : DT_FLOAT type : DT_DOUBLE type : DT_INT32 type : DT_COMPLEX64 type : DT_COMPLEX128 } } }","title":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part1"},{"location":"building_graph/tensorflow_graph_part1/#tensorflow-part1","text":"TensorFlow\u306b\u304a\u3051\u308b\u30b0\u30e9\u30d5(Graph)\u306f tf.Operation \u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u96c6\u5408\u3067\u3042\u308b\u3002\u307e\u305f\u3001tf.Operation\u306fTensor\u3092\u5165\u529b\u304a\u3088\u3073\u51fa\u529b\u306b\u6301\u3064\u30ce\u30fc\u30c9\u3068\u306a\u3063\u3066\u3044\u308b\u3002","title":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part1"},{"location":"building_graph/tensorflow_graph_part1/#1","text":"\u30b0\u30e9\u30d5\u304c\u69cb\u7bc9\u3055\u308c\u308b\u69d8\u5b50\u3092\u78ba\u8a8d\u3059\u308b\u3002 tf.get_default_graph() \u306b\u3088\u308a\u3001TensorFlow\u3067\u4f7f\u308f\u308c\u308b\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b0\u30e9\u30d5\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b \u30b0\u30e9\u30d5\u306eOperation\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u306b\u306f\u3001 graph.get_operations() \u3092\u4f7f\u7528\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/usr/bin/env python # coding:utf-8 from __future__ import division from __future__ import print_function from __future__ import unicode_literals import numpy as np import tensorflow as tf # \u30b0\u30e9\u30d5\u306e\u53d6\u5f97 graph = tf . get_default_graph () # \u30ce\u30fc\u30c9\u4e00\u89a7 print ( \"*** STEP1 ***\" , graph . get_operations ()) x = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_x\" ) # 2x2 Tensor print ( \"*** STEP2 ***\" , graph . get_operations ()) y = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_y\" ) # 2x2 Tensor print ( \"*** STEP3 ***\" , graph . get_operations ()) matmul = tf . matmul ( x , y , name = \"matrix_mul\" ) # x\u3068y\u306e\u4e57\u7b97 print ( \"*** STEP4 ***\" , graph . get_operations ()) \u5b9f\u884c\u7d50\u679c : Operation\u30ce\u30fc\u30c9\u304c\u30b0\u30e9\u30d5\u306b\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 1 2 3 4 *** STEP1 *** [] *** STEP2 *** [<tensorflow.python.framework.ops.Operation object at 0x10d325850>] *** STEP3 *** [<tensorflow.python.framework.ops.Operation object at 0x10d325850>, <tensorflow.python.framework.ops.Operation object at 0x10d325bd0>] *** STEP4 *** [<tensorflow.python.framework.ops.Operation object at 0x10d325850>, <tensorflow.python.framework.ops.Operation object at 0x10d325bd0>, <tensorflow.python.framework.ops.Operation object at 0x10d325c50>]","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 1"},{"location":"building_graph/tensorflow_graph_part1/#2","text":"\u30b0\u30e9\u30d5\u5185\u306e\u30ce\u30fc\u30c9\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\u3002 op.name : \u30ce\u30fc\u30c9\u306e\u540d\u524d op.type : \u30ce\u30fc\u30c9\u306e\u578b op.op_def : \u30ce\u30fc\u30c9\u306eProtocol buffer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/usr/bin/env python # coding:utf-8 from __future__ import division from __future__ import print_function from __future__ import unicode_literals import numpy as np import tensorflow as tf # \u30b0\u30e9\u30d5\u306e\u53d6\u5f97 graph = tf . get_default_graph () # \u30ce\u30fc\u30c9\u4e00\u89a7 x = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_x\" ) # 2x2 Tensor y = tf . placeholder ( tf . float32 , shape = [ 2 , 2 ], name = \"matrix_y\" ) # 2x2 Tensor matmul = tf . matmul ( x , y , name = \"matrix_mul\" ) # x\u3068y\u306e\u4e57\u7b97 for op in graph . get_operations (): # \u30ce\u30fc\u30c9\u306e\u540d\u524d print ( \"***name***\" , op . name ) # \u30ce\u30fc\u30c9\u306e\u578b print ( \"***type***\" , op . type ) # \u30ce\u30fc\u30c9\u306e\u60c5\u5831\u3092\u8868\u3059protocol buffer print ( \"***op_def***\" , op . op_def ) \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 *** name *** matrix_x *** type *** Placeholder *** op_def *** name : \"Placeholder\" output_arg { name : \"output\" type_attr : \"dtype\" } attr { name : \"dtype\" type : \"type\" } attr { name : \"shape\" type : \"shape\" default_value { shape { } } } *** name *** matrix_y *** type *** Placeholder *** op_def *** name : \"Placeholder\" output_arg { name : \"output\" type_attr : \"dtype\" } attr { name : \"dtype\" type : \"type\" } attr { name : \"shape\" type : \"shape\" default_value { shape { } } } *** name *** matrix_mul *** type *** MatMul *** op_def *** name : \"MatMul\" input_arg { name : \"a\" type_attr : \"T\" } input_arg { name : \"b\" type_attr : \"T\" } output_arg { name : \"product\" type_attr : \"T\" } attr { name : \"transpose_a\" type : \"bool\" default_value { b : false } } attr { name : \"transpose_b\" type : \"bool\" default_value { b : false } } attr { name : \"T\" type : \"type\" allowed_values { list { type : DT_HALF type : DT_FLOAT type : DT_DOUBLE type : DT_INT32 type : DT_COMPLEX64 type : DT_COMPLEX128 } } }","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 2"},{"location":"building_graph/tensorflow_graph_part2/","text":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part2 \u30b0\u30e9\u30d5\u3092\u53d6\u5f97\u3059\u308b \u69cb\u7bc9\u3055\u308c\u305f\u30b0\u30e9\u30d5\u3092\u53d6\u5f97\u3059\u308b\u65b9\u6cd5\uff1a tf.get_default_graph() sess.graph 1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import tensorflow as tf # \u8db3\u3057\u7b97\u3092\u884c\u3046\u30b0\u30e9\u30d5\u3092\u69cb\u7bc9 a = tf . constant ( 1 , name = \"a\" ) b = tf . constant ( 2 , name = \"b\" ) add_op = tf . add ( a , b , name = \"add_op\" ) # \u65b9\u6cd5(1) g1 = tf . get_default_graph () print ( g1 ) # \u65b9\u6cd5(2) sess = tf . Session () g2 = sess . graph print ( g2 ) \u5b9f\u884c\u7d50\u679c \u540c\u4e00\u306e graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304c\u53d6\u5f97\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 1 2 <tensorflow.python.framework.ops.Graph object at 0x102541350> <tensorflow.python.framework.ops.Graph object at 0x102541350> \u8907\u6570\u306e\u30b0\u30e9\u30d5\u3092\u540c\u3058\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u6271\u3046 \u8db3\u3057\u7b97\u3092\u884c\u3046\u30b0\u30e9\u30d5\u3068\u5f15\u304d\u7b97\u3092\u884c\u3046\u30b0\u30e9\u30d5\u306e2\u3064\u3092\u540c\u3058\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u6271\u3046\u3002 \u8907\u6570\u306e\u30b0\u30e9\u30d5\u3092\u6271\u3046\u305f\u3081\u306e\u624b\u9806\uff1a tf.Graph() \u3067 graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u751f\u6210 tasizan_graph.as_default() \u3067\u5bfe\u8c61\u3068\u306a\u308b\u30b0\u30e9\u30d5\u3092\u6307\u5b9a \u30b0\u30e9\u30d5\u306e\u69cb\u7bc9 tf.Session(graph=...) \u3067\u5b9f\u884c\u3059\u308b\u30b0\u30e9\u30d5\u3092\u6307\u5b9a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # coding:utf-8 import tensorflow as tf # \u8db3\u3057\u7b97\u7528\u30b0\u30e9\u30d5 tasizan_graph = tf . Graph () # \u5f15\u304d\u7b97\u7528\u30b0\u30e9\u30d5 hikizan_graph = tf . Graph () # \u5f15\u304d\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9 with tasizan_graph . as_default (): a = tf . placeholder ( tf . int32 , shape = [], name = \"a\" ) b = tf . placeholder ( tf . int32 , shape = [], name = \"b\" ) add_op = tf . add ( a , b , name = \"add_op\" ) # \u8db3\u3057\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9 with hikizan_graph . as_default (): x = tf . placeholder ( tf . int32 , shape = [], name = \"x\" ) y = tf . placeholder ( tf . int32 , shape = [], name = \"y\" ) sub_op = tf . sub ( x , y , name = \"sub_op\" ) # \u8db3\u3057\u7b97\u306e\u5b9f\u884c with tf . Session ( graph = tasizan_graph ) as sess : ret = sess . run ( add_op , feed_dict = { a : 1 , b : 1 }) print ret # \u5f15\u304d\u7b97\u306e\u5b9f\u884c with tf . Session ( graph = hikizan_graph ) as sess : ret = sess . run ( sub_op , feed_dict = { x : 1 , y : 1 }) print ret \u5b9f\u884c\u7d50\u679c 1 2 2 0 \u53c2\u8003","title":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part2"},{"location":"building_graph/tensorflow_graph_part2/#tensorflow-part2","text":"","title":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part2"},{"location":"building_graph/tensorflow_graph_part2/#_1","text":"\u69cb\u7bc9\u3055\u308c\u305f\u30b0\u30e9\u30d5\u3092\u53d6\u5f97\u3059\u308b\u65b9\u6cd5\uff1a tf.get_default_graph() sess.graph 1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import tensorflow as tf # \u8db3\u3057\u7b97\u3092\u884c\u3046\u30b0\u30e9\u30d5\u3092\u69cb\u7bc9 a = tf . constant ( 1 , name = \"a\" ) b = tf . constant ( 2 , name = \"b\" ) add_op = tf . add ( a , b , name = \"add_op\" ) # \u65b9\u6cd5(1) g1 = tf . get_default_graph () print ( g1 ) # \u65b9\u6cd5(2) sess = tf . Session () g2 = sess . graph print ( g2 )","title":"\u30b0\u30e9\u30d5\u3092\u53d6\u5f97\u3059\u308b"},{"location":"building_graph/tensorflow_graph_part2/#_2","text":"\u540c\u4e00\u306e graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304c\u53d6\u5f97\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 1 2 <tensorflow.python.framework.ops.Graph object at 0x102541350> <tensorflow.python.framework.ops.Graph object at 0x102541350>","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"building_graph/tensorflow_graph_part2/#_3","text":"\u8db3\u3057\u7b97\u3092\u884c\u3046\u30b0\u30e9\u30d5\u3068\u5f15\u304d\u7b97\u3092\u884c\u3046\u30b0\u30e9\u30d5\u306e2\u3064\u3092\u540c\u3058\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u6271\u3046\u3002 \u8907\u6570\u306e\u30b0\u30e9\u30d5\u3092\u6271\u3046\u305f\u3081\u306e\u624b\u9806\uff1a tf.Graph() \u3067 graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u751f\u6210 tasizan_graph.as_default() \u3067\u5bfe\u8c61\u3068\u306a\u308b\u30b0\u30e9\u30d5\u3092\u6307\u5b9a \u30b0\u30e9\u30d5\u306e\u69cb\u7bc9 tf.Session(graph=...) \u3067\u5b9f\u884c\u3059\u308b\u30b0\u30e9\u30d5\u3092\u6307\u5b9a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # coding:utf-8 import tensorflow as tf # \u8db3\u3057\u7b97\u7528\u30b0\u30e9\u30d5 tasizan_graph = tf . Graph () # \u5f15\u304d\u7b97\u7528\u30b0\u30e9\u30d5 hikizan_graph = tf . Graph () # \u5f15\u304d\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9 with tasizan_graph . as_default (): a = tf . placeholder ( tf . int32 , shape = [], name = \"a\" ) b = tf . placeholder ( tf . int32 , shape = [], name = \"b\" ) add_op = tf . add ( a , b , name = \"add_op\" ) # \u8db3\u3057\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9 with hikizan_graph . as_default (): x = tf . placeholder ( tf . int32 , shape = [], name = \"x\" ) y = tf . placeholder ( tf . int32 , shape = [], name = \"y\" ) sub_op = tf . sub ( x , y , name = \"sub_op\" ) # \u8db3\u3057\u7b97\u306e\u5b9f\u884c with tf . Session ( graph = tasizan_graph ) as sess : ret = sess . run ( add_op , feed_dict = { a : 1 , b : 1 }) print ret # \u5f15\u304d\u7b97\u306e\u5b9f\u884c with tf . Session ( graph = hikizan_graph ) as sess : ret = sess . run ( sub_op , feed_dict = { x : 1 , y : 1 }) print ret","title":"\u8907\u6570\u306e\u30b0\u30e9\u30d5\u3092\u540c\u3058\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u6271\u3046"},{"location":"building_graph/tensorflow_graph_part2/#_4","text":"1 2 2 0","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"building_graph/tensorflow_graph_part2/#_5","text":"","title":"\u53c2\u8003"},{"location":"building_graph/tensorflow_graph_part3/","text":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part3 \u540d\u524d\u7a7a\u9593 TensorFlow\u3067\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u540d\u524d\u7a7a\u9593(name scope)\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u30ce\u30fc\u30c9\u306e\u7ba1\u7406\u3092\u4fbf\u5229\u306b\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308b\u3002 \u540d\u524d\u7a7a\u9593\u306f\u4f8b\u3048\u3070\u30d5\u30a9\u30eb\u30c0\u6a5f\u80fd\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u300c\u7b2c\u4e00\u5c64\u76ee first_layer \u300d\u306e\u91cd\u307f\u30ce\u30fc\u30c9 w \u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u300c\u7b2c\u4e8c\u5c64\u76ee second_layer \u300d\u306e\u91cd\u307f\u30ce\u30fc\u30c9 w \u3068\u3044\u3063\u305f\u3088\u3046\u306b\u547d\u540d\u898f\u5247\u306b\u4e00\u8cab\u6027\u3092\u6301\u305f\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # coding:utf-8 import tensorflow as tf # \u540d\u524d\u7a7a\u9593\u306e\u6307\u5b9a with tf . name_scope ( \"input_layer\" ): x = tf . placeholder ( shape = [ 64 , 64 ], dtype = tf . float32 , name = \"x\" ) with tf . name_scope ( \"first_layer\" ): w1 = tf . Variable ( x , name = \"weights\" ) with tf . name_scope ( \"second_layer\" ): # \u30cd\u30b9\u30c8\u3092\u6df1\u304f\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd with tf . name_scope ( \"sub_scope\" ): a = tf . Variable ([ 1.0 ], name = \"a\" ) b = tf . Variable ([ 2.0 ], name = \"b\" ) w2 = tf . Variable ( w1 + a + b , name = \"weights\" ) with tf . name_scope ( \"output_layer\" ): y = tf . Variable ( w2 , name = \"y\" ) g = tf . get_default_graph () # \u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b for op in g . get_operations (): print op . name # \u53ef\u8996\u5316 tf . summary . FileWriter ( 'graph_log' , graph = g ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 input_layer/x first_layer/weights first_layer/weights/Assign first_layer/weights/read second_layer/constant/a second_layer/constant/b second_layer/add second_layer/add_1 second_layer/weights second_layer/weights/Assign second_layer/weights/read output_layer/y output_layer/y/Assign output_layer/y/read TensorBoard\u306b\u3088\u308b\u53ef\u8996\u5316 tensorboard --logdir=./graph_log Tensor\u3092\u6307\u5b9a\u3057\u3066\u53d6\u5f97\u3059\u308b graph.get_tensor_by_name(...) \u3092\u7528\u3044\u3066Tensor\u3092\u53d6\u5f97\u3059\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding:utf-8 import tensorflow as tf with tf . name_scope ( \"input_layer\" ): x = tf . placeholder ( shape = [ 64 , 64 ], dtype = tf . float32 , name = \"x\" ) with tf . name_scope ( \"first_layer\" ): w1 = tf . Variable ( x , name = \"weights\" ) with tf . name_scope ( \"second_layer\" ): with tf . name_scope ( \"sub_scope\" ): a = tf . Variable ([ 1.0 ], name = \"a\" ) b = tf . Variable ([ 2.0 ], name = \"b\" ) w2 = tf . Variable ( w1 + a + b , name = \"weights\" ) with tf . name_scope ( \"output_layer\" ): y = tf . Variable ( w2 , name = \"y\" ) g = tf . get_default_graph () w = g . get_tensor_by_name ( \"first_layer/weights:0\" ) print ( w ) a = g . get_tensor_by_name ( \"second_layer/sub_scope/a:0\" ) print ( a ) \u5b9f\u884c\u7d50\u679c 1 2 Tensor(\"first_layer/weights:0\", shape=(64, 64), dtype=float32_ref) Tensor(\"second_layer/sub_scope/a:0\", shape=(1,), dtype=float32_ref)","title":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part3"},{"location":"building_graph/tensorflow_graph_part3/#tensorflow-part3","text":"","title":"Tensorflow\u306e\u30b0\u30e9\u30d5\u64cd\u4f5c Part3"},{"location":"building_graph/tensorflow_graph_part3/#_1","text":"TensorFlow\u3067\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u540d\u524d\u7a7a\u9593(name scope)\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u3001\u30ce\u30fc\u30c9\u306e\u7ba1\u7406\u3092\u4fbf\u5229\u306b\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308b\u3002 \u540d\u524d\u7a7a\u9593\u306f\u4f8b\u3048\u3070\u30d5\u30a9\u30eb\u30c0\u6a5f\u80fd\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u300c\u7b2c\u4e00\u5c64\u76ee first_layer \u300d\u306e\u91cd\u307f\u30ce\u30fc\u30c9 w \u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u300c\u7b2c\u4e8c\u5c64\u76ee second_layer \u300d\u306e\u91cd\u307f\u30ce\u30fc\u30c9 w \u3068\u3044\u3063\u305f\u3088\u3046\u306b\u547d\u540d\u898f\u5247\u306b\u4e00\u8cab\u6027\u3092\u6301\u305f\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002","title":"\u540d\u524d\u7a7a\u9593"},{"location":"building_graph/tensorflow_graph_part3/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # coding:utf-8 import tensorflow as tf # \u540d\u524d\u7a7a\u9593\u306e\u6307\u5b9a with tf . name_scope ( \"input_layer\" ): x = tf . placeholder ( shape = [ 64 , 64 ], dtype = tf . float32 , name = \"x\" ) with tf . name_scope ( \"first_layer\" ): w1 = tf . Variable ( x , name = \"weights\" ) with tf . name_scope ( \"second_layer\" ): # \u30cd\u30b9\u30c8\u3092\u6df1\u304f\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd with tf . name_scope ( \"sub_scope\" ): a = tf . Variable ([ 1.0 ], name = \"a\" ) b = tf . Variable ([ 2.0 ], name = \"b\" ) w2 = tf . Variable ( w1 + a + b , name = \"weights\" ) with tf . name_scope ( \"output_layer\" ): y = tf . Variable ( w2 , name = \"y\" ) g = tf . get_default_graph () # \u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b for op in g . get_operations (): print op . name # \u53ef\u8996\u5316 tf . summary . FileWriter ( 'graph_log' , graph = g )","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"building_graph/tensorflow_graph_part3/#_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 input_layer/x first_layer/weights first_layer/weights/Assign first_layer/weights/read second_layer/constant/a second_layer/constant/b second_layer/add second_layer/add_1 second_layer/weights second_layer/weights/Assign second_layer/weights/read output_layer/y output_layer/y/Assign output_layer/y/read","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"building_graph/tensorflow_graph_part3/#tensorboard","text":"tensorboard --logdir=./graph_log","title":"TensorBoard\u306b\u3088\u308b\u53ef\u8996\u5316"},{"location":"building_graph/tensorflow_graph_part3/#tensor","text":"graph.get_tensor_by_name(...) \u3092\u7528\u3044\u3066Tensor\u3092\u53d6\u5f97\u3059\u308b\u3002","title":"Tensor\u3092\u6307\u5b9a\u3057\u3066\u53d6\u5f97\u3059\u308b"},{"location":"building_graph/tensorflow_graph_part3/#_4","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding:utf-8 import tensorflow as tf with tf . name_scope ( \"input_layer\" ): x = tf . placeholder ( shape = [ 64 , 64 ], dtype = tf . float32 , name = \"x\" ) with tf . name_scope ( \"first_layer\" ): w1 = tf . Variable ( x , name = \"weights\" ) with tf . name_scope ( \"second_layer\" ): with tf . name_scope ( \"sub_scope\" ): a = tf . Variable ([ 1.0 ], name = \"a\" ) b = tf . Variable ([ 2.0 ], name = \"b\" ) w2 = tf . Variable ( w1 + a + b , name = \"weights\" ) with tf . name_scope ( \"output_layer\" ): y = tf . Variable ( w2 , name = \"y\" ) g = tf . get_default_graph () w = g . get_tensor_by_name ( \"first_layer/weights:0\" ) print ( w ) a = g . get_tensor_by_name ( \"second_layer/sub_scope/a:0\" ) print ( a )","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"building_graph/tensorflow_graph_part3/#_5","text":"1 2 Tensor(\"first_layer/weights:0\", shape=(64, 64), dtype=float32_ref) Tensor(\"second_layer/sub_scope/a:0\", shape=(1,), dtype=float32_ref)","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"building_graph/tensorflow_protocol_buffers_part1/","text":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part1 \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1(protocol buffer\u3001protobuf)\u306f\u3001Google\u793e\u306b\u3088\u3063\u3066\u958b\u767a\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u4ea4\u63db\u3084\u4fdd\u5b58\u306b\u7528\u3044\u3089\u308c\u308b\u30b7\u30ea\u30a2\u30e9\u30a4\u30ba\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u3042\u308b\u3002\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u5f62\u5f0f\u3067\u8a18\u8ff0\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\u3001XML\u3084JSON\u3068\u540c\u3058\u3088\u3046\u306b\u8907\u6570\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3067\u5171\u6709\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3001\u62e1\u5f35\u5b50 .proto \u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3055\u308c\u308b\u3002 \u516c\u5f0f\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 \u306b\u3088\u308c\u3070\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306fXML\u3068\u6bd4\u8f03\u3057\u3066\u4ee5\u4e0b\u306e\u70b9\u3067\u512a\u308c\u3066\u3044\u308b\u3068\u3057\u3066\u3044\u308b\u3002 \u3088\u308a\u30b7\u30f3\u30d7\u30eb 3\u500d\u304b\u308910\u500d\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u3044 20\u500d\u304b\u3089100\u500d\u901f\u3044 \u66d6\u6627\u6027\u304c\u3088\u308a\u4f4e\u3044 \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306e\u7c21\u5358\u306a\u4f8b \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u30fbXML\u30fbJSON\u306e\u6bd4\u8f03\u3092\u4ee5\u4e0b\u306b\u8f09\u305b\u308b\u3002 \u578b \u30e6\u30fc\u30b6\u5b9a\u7fa9\u578b required\u3001optional\u4fee\u98fe\u5b50 Protocol buffer\uff1a 1 2 3 4 5 person { required name : strings \"Taro Tanaka\" optional age : int32 18 required email : strings \"taro@email.com\" } XML\uff1a 1 2 3 4 5 <person> <name> Taro Tanaka </name> <age> 18 </age> <email> taro@email.com </email> </person> JSON\uff1a 1 2 3 4 5 6 7 { \"person\" : { \"name\" : \"Taro Tanaka\" , \"age\" : 18 , \"email\" : \"taro@email.com\" } } \u53c2\u8003 Protocol Buffers - Google Developers google/protobuf 5 Reasons to Use Protocol Buffers Instead of JSON For Your Next Service Protocol Buffers - Wikipedia Protocol Buffers \u5165\u9580","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part1"},{"location":"building_graph/tensorflow_protocol_buffers_part1/#part1","text":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1(protocol buffer\u3001protobuf)\u306f\u3001Google\u793e\u306b\u3088\u3063\u3066\u958b\u767a\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u4ea4\u63db\u3084\u4fdd\u5b58\u306b\u7528\u3044\u3089\u308c\u308b\u30b7\u30ea\u30a2\u30e9\u30a4\u30ba\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u3042\u308b\u3002\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u5f62\u5f0f\u3067\u8a18\u8ff0\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\u3001XML\u3084JSON\u3068\u540c\u3058\u3088\u3046\u306b\u8907\u6570\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3067\u5171\u6709\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3001\u62e1\u5f35\u5b50 .proto \u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3055\u308c\u308b\u3002 \u516c\u5f0f\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 \u306b\u3088\u308c\u3070\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306fXML\u3068\u6bd4\u8f03\u3057\u3066\u4ee5\u4e0b\u306e\u70b9\u3067\u512a\u308c\u3066\u3044\u308b\u3068\u3057\u3066\u3044\u308b\u3002 \u3088\u308a\u30b7\u30f3\u30d7\u30eb 3\u500d\u304b\u308910\u500d\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u3044 20\u500d\u304b\u3089100\u500d\u901f\u3044 \u66d6\u6627\u6027\u304c\u3088\u308a\u4f4e\u3044","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part1"},{"location":"building_graph/tensorflow_protocol_buffers_part1/#_1","text":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u30fbXML\u30fbJSON\u306e\u6bd4\u8f03\u3092\u4ee5\u4e0b\u306b\u8f09\u305b\u308b\u3002 \u578b \u30e6\u30fc\u30b6\u5b9a\u7fa9\u578b required\u3001optional\u4fee\u98fe\u5b50 Protocol buffer\uff1a 1 2 3 4 5 person { required name : strings \"Taro Tanaka\" optional age : int32 18 required email : strings \"taro@email.com\" } XML\uff1a 1 2 3 4 5 <person> <name> Taro Tanaka </name> <age> 18 </age> <email> taro@email.com </email> </person> JSON\uff1a 1 2 3 4 5 6 7 { \"person\" : { \"name\" : \"Taro Tanaka\" , \"age\" : 18 , \"email\" : \"taro@email.com\" } }","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306e\u7c21\u5358\u306a\u4f8b"},{"location":"building_graph/tensorflow_protocol_buffers_part1/#_2","text":"Protocol Buffers - Google Developers google/protobuf 5 Reasons to Use Protocol Buffers Instead of JSON For Your Next Service Protocol Buffers - Wikipedia Protocol Buffers \u5165\u9580","title":"\u53c2\u8003"},{"location":"building_graph/tensorflow_protocol_buffers_part2/","text":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part2 TensorFlow\u306e\u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\u306f\u3059\u3079\u3066\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u304c\u30d9\u30fc\u30b9\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u305d\u306e\u305f\u3081TensorFlow\u3067\u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306fC\u3084Python\u3001\u305d\u306e\u4ed6\u306e\u8a00\u8a9e\u3067\u5bb9\u6613\u306b\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3002 TensorFlow\u306b\u304a\u3051\u308b\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Graph \u304b\u3089 GraphDef \u306e\u6d41\u308c Graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u751f\u6210 as_graph_def() \u306b\u3088\u308b GraphDef \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u751f\u6210 Graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306fTensor\u3068\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u60c5\u5831\u3092\u6301\u3064\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u3042\u308a\u3001 GraphDef \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306f\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u7528\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u3042\u308b\u3002 \u4fdd\u5b58\u5f62\u5f0f \u30c6\u30ad\u30b9\u30c8 \u62e1\u5f35\u5b50 .pbtxt \u53ef\u8aad\u6027\u3042\u308a \u7de8\u96c6\u53ef\u80fd \u30d0\u30a4\u30ca\u30ea \u62e1\u5f35\u5b50 .pb \u30c6\u30ad\u30b9\u30c8\u5f62\u5f0f\u3088\u308a\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u3044 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 1+2\u3092\u884c\u3046\u30b0\u30e9\u30d5\u3092\u69cb\u7bc9\u3057\u3001\u305d\u306e\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u3092\u78ba\u8a8d\u3059\u308b\u3002 1 2 3 4 5 6 7 8 # coding:utf-8 import tensorflow as tf a = tf . constant ( 1 ) b = tf . constant ( 2 ) c = tf . add ( a , b ) graph = tf . get_default_graph () graph_def = graph . as_graph_def () print graph_def \u5b9f\u884c\u7d50\u679c \u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306b\u4f3c\u305f\u6587\u5b57\u5217\u306b\u51fa\u529b\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 node { name: \"Const\" op: \"Const\" attr { key: \"dtype\" value { type: DT_INT32 } } attr { key: \"value\" value { tensor { dtype: DT_INT32 tensor_shape { } int_val: 1 } } } } node { name: \"Const_1\" op: \"Const\" attr { key: \"dtype\" value { type: DT_INT32 } } attr { key: \"value\" value { tensor { dtype: DT_INT32 tensor_shape { } int_val: 2 } } } } node { name: \"Add\" op: \"Add\" input: \"Const\" input: \"Const_1\" attr { key: \"T\" value { type: DT_INT32 } } } versions { producer: 17 } \u88dc\u8db3: NodeDef \u30b0\u30e9\u30d5\u306b\u304a\u3051\u308b\u30ce\u30fc\u30c9\u306e\u60c5\u5831\u3092\u6271\u3046 NodeDef \u306e\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 \u30e1\u30f3\u30d0 \u8aac\u660e name \u30b0\u30e9\u30d5\u4e0a\u306e\u4e00\u610f\u3068\u306a\u308b\u30ce\u30fc\u30c9\u540d op \u5b9f\u884c\u3059\u308b\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3 input \u5165\u529b\u3055\u308c\u308b\u30ce\u30fc\u30c9\u306e\u30ea\u30b9\u30c8 device \u5b9f\u884c\u74b0\u5883\u306e\u60c5\u5831 attr \u30ce\u30fc\u30c9\u306ekey/value\u30c7\u30fc\u30bf \u53c2\u8003 A Tool Developer's Guide to TensorFlow Model Files","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part2"},{"location":"building_graph/tensorflow_protocol_buffers_part2/#part2","text":"TensorFlow\u306e\u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\u306f\u3059\u3079\u3066\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u304c\u30d9\u30fc\u30b9\u3068\u306a\u3063\u3066\u3044\u308b\u3002\u305d\u306e\u305f\u3081TensorFlow\u3067\u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306fC\u3084Python\u3001\u305d\u306e\u4ed6\u306e\u8a00\u8a9e\u3067\u5bb9\u6613\u306b\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3002","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part2"},{"location":"building_graph/tensorflow_protocol_buffers_part2/#tensorflow","text":"Graph \u304b\u3089 GraphDef \u306e\u6d41\u308c Graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u751f\u6210 as_graph_def() \u306b\u3088\u308b GraphDef \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u751f\u6210 Graph \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306fTensor\u3068\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u60c5\u5831\u3092\u6301\u3064\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u3042\u308a\u3001 GraphDef \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306f\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u7528\u30e9\u30a4\u30d6\u30e9\u30ea\u306b\u3088\u3063\u3066\u751f\u6210\u3055\u308c\u308b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u3042\u308b\u3002","title":"TensorFlow\u306b\u304a\u3051\u308b\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1"},{"location":"building_graph/tensorflow_protocol_buffers_part2/#_1","text":"\u30c6\u30ad\u30b9\u30c8 \u62e1\u5f35\u5b50 .pbtxt \u53ef\u8aad\u6027\u3042\u308a \u7de8\u96c6\u53ef\u80fd \u30d0\u30a4\u30ca\u30ea \u62e1\u5f35\u5b50 .pb \u30c6\u30ad\u30b9\u30c8\u5f62\u5f0f\u3088\u308a\u30b5\u30a4\u30ba\u304c\u5c0f\u3055\u3044","title":"\u4fdd\u5b58\u5f62\u5f0f"},{"location":"building_graph/tensorflow_protocol_buffers_part2/#_2","text":"1+2\u3092\u884c\u3046\u30b0\u30e9\u30d5\u3092\u69cb\u7bc9\u3057\u3001\u305d\u306e\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u3092\u78ba\u8a8d\u3059\u308b\u3002 1 2 3 4 5 6 7 8 # coding:utf-8 import tensorflow as tf a = tf . constant ( 1 ) b = tf . constant ( 2 ) c = tf . add ( a , b ) graph = tf . get_default_graph () graph_def = graph . as_graph_def () print graph_def","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"building_graph/tensorflow_protocol_buffers_part2/#_3","text":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306b\u4f3c\u305f\u6587\u5b57\u5217\u306b\u51fa\u529b\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 node { name: \"Const\" op: \"Const\" attr { key: \"dtype\" value { type: DT_INT32 } } attr { key: \"value\" value { tensor { dtype: DT_INT32 tensor_shape { } int_val: 1 } } } } node { name: \"Const_1\" op: \"Const\" attr { key: \"dtype\" value { type: DT_INT32 } } attr { key: \"value\" value { tensor { dtype: DT_INT32 tensor_shape { } int_val: 2 } } } } node { name: \"Add\" op: \"Add\" input: \"Const\" input: \"Const_1\" attr { key: \"T\" value { type: DT_INT32 } } } versions { producer: 17 }","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"building_graph/tensorflow_protocol_buffers_part2/#nodedef","text":"\u30b0\u30e9\u30d5\u306b\u304a\u3051\u308b\u30ce\u30fc\u30c9\u306e\u60c5\u5831\u3092\u6271\u3046 NodeDef \u306e\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 \u30e1\u30f3\u30d0 \u8aac\u660e name \u30b0\u30e9\u30d5\u4e0a\u306e\u4e00\u610f\u3068\u306a\u308b\u30ce\u30fc\u30c9\u540d op \u5b9f\u884c\u3059\u308b\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3 input \u5165\u529b\u3055\u308c\u308b\u30ce\u30fc\u30c9\u306e\u30ea\u30b9\u30c8 device \u5b9f\u884c\u74b0\u5883\u306e\u60c5\u5831 attr \u30ce\u30fc\u30c9\u306ekey/value\u30c7\u30fc\u30bf","title":"\u88dc\u8db3:NodeDef"},{"location":"building_graph/tensorflow_protocol_buffers_part2/#_4","text":"A Tool Developer's Guide to TensorFlow Model Files","title":"\u53c2\u8003"},{"location":"building_graph/tensorflow_protocol_buffers_part3/","text":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part3 \u5b66\u7fd2\u6e08\u307fmodel\u3092protocol buffer\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u30dd\u30a4\u30f3\u30c8 TensorFlow\u306e\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306b\u306f\u30dd\u30a4\u30f3\u30c8\u304c3\u3064\u3042\u308b\u3002 * \u30c7\u30fc\u30bf\u5165\u529b\u30ce\u30fc\u30c9\u3068\u306a\u308bplaceholder\u3084dequeue_op\u3001\u51fa\u529b\u30ce\u30fc\u30c9\u3068\u306a\u308bprediction\u3084accuracy\u306b\u306f\u540d\u524d\u3092\u4ed8\u3051\u3066\u304a\u304f\u3053\u3068 * checkpoint\u5f62\u5f0f\u3067\u4fdd\u5b58\u3057\u3066\u304a\u304f\u3053\u3068 * tf.Variable\u306fConst\u306b\u5909\u63db\u3057\u3066\u304b\u3089\u4fdd\u5b58\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3053\u3068 \u30dd\u30a4\u30f3\u30c81 \u5165\u529b\u30ce\u30fc\u30c9\u3001\u51fa\u529b\u30ce\u30fc\u30c9\u306f\u5f8c\u3067 input_x= graph.get_tensor_by_name('input_x:0') output_y= graph.get_tensor_by_name('output_y:0') \u306e\u3088\u3046\u306b\u540d\u524d\u3067\u30ce\u30fc\u30c9\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u308f\u304b\u308a\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u540d\u524d\u3092\u4ed8\u3051\u3066\u304a\u304f\u3002 \u30dd\u30a4\u30f3\u30c82 checkpoint\u3067\u306e\u4fdd\u5b58\u306f2\u884c\u3067\u51fa\u6765\u308b\u3002 1 2 3 saver = tf . train . Saver () ... \u5b66\u7fd2 ... saver . save ( sess , MODEL_DIR + '/model.ckpt' ) checkpoint\u3067\u306f\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u60c5\u5831\uff08graph\u306e\u30e1\u30bf\u60c5\u5831\uff09\u3068\u5b66\u7fd2\u5024\uff08tf.Variable\uff09\u304c\u5225\u3005\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3055\u308c\u308b\u3002\u30e1\u30bf\u60c5\u5831\u3092\u8aad\u307f\u8fbc\u3093\u3067graph\u3092\u5fa9\u5143\u3059\u308b\u969b\u306b\u3001\u5b66\u7fd2\u7528\u306b\u8a2d\u5b9a\u3057\u305fCPU/GPU\u306e\u30c7\u30d0\u30a4\u30b9\u4f9d\u5b58\u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\u3053\u308c\u306b\u3088\u3063\u3066\u30dd\u30fc\u30bf\u30d3\u30ea\u30c6\u30a3\u304c\u5411\u4e0a\u3059\u308b\u3002 \uff08\u305f\u3060\u3057\u3001tf.Variable\u3067\u5909\u6570\u3092\u7528\u610f\u3057\u3066\u3044\u306a\u3044\u5b66\u7fd2\u3057\u306a\u3044\u30e2\u30c7\u30eb\u306e\u5834\u5408\u306fcheckpoint\u3067\u306e\u4fdd\u5b58\u306f\u3067\u304d\u306a\u3044\u3002\u305d\u306e\u5834\u5408\u3001pb\u3078\u306e\u66f8\u304d\u51fa\u3057\u3001\u5229\u7528\u306b\u30dd\u30a4\u30f3\u30c83\u306f\u4e0d\u8981\u306b\u306a\u308b\u3002\uff09 \u30dd\u30a4\u30f3\u30c83 \u901a\u5e38\u306e\u30e2\u30c7\u30eb\u306f\u5b66\u7fd2\u6e08\u307f\u306eWeight\u3084Bias\u3092\u4fdd\u6301\u3059\u308b\u305f\u3081\u306etf.Variable\u306e\u5909\u6570\u3092\u6301\u3064\u3002\u3053\u306e\u5024\u3092pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u3053\u3068\u306f\u51fa\u6765\u306a\u3044\u305f\u3081\u3001 graph_util.convert_variables_to_constants() \u3092\u4f7f\u3063\u3066Const\u306b\u5909\u63db\u3057\u305fgraph\u3092pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u3002","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part3"},{"location":"building_graph/tensorflow_protocol_buffers_part3/#part3","text":"","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part3"},{"location":"building_graph/tensorflow_protocol_buffers_part3/#modelprotocol-buffer","text":"TensorFlow\u306e\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1\u306b\u306f\u30dd\u30a4\u30f3\u30c8\u304c3\u3064\u3042\u308b\u3002 * \u30c7\u30fc\u30bf\u5165\u529b\u30ce\u30fc\u30c9\u3068\u306a\u308bplaceholder\u3084dequeue_op\u3001\u51fa\u529b\u30ce\u30fc\u30c9\u3068\u306a\u308bprediction\u3084accuracy\u306b\u306f\u540d\u524d\u3092\u4ed8\u3051\u3066\u304a\u304f\u3053\u3068 * checkpoint\u5f62\u5f0f\u3067\u4fdd\u5b58\u3057\u3066\u304a\u304f\u3053\u3068 * tf.Variable\u306fConst\u306b\u5909\u63db\u3057\u3066\u304b\u3089\u4fdd\u5b58\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3053\u3068","title":"\u5b66\u7fd2\u6e08\u307fmodel\u3092protocol buffer\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u30dd\u30a4\u30f3\u30c8"},{"location":"building_graph/tensorflow_protocol_buffers_part3/#1","text":"\u5165\u529b\u30ce\u30fc\u30c9\u3001\u51fa\u529b\u30ce\u30fc\u30c9\u306f\u5f8c\u3067 input_x= graph.get_tensor_by_name('input_x:0') output_y= graph.get_tensor_by_name('output_y:0') \u306e\u3088\u3046\u306b\u540d\u524d\u3067\u30ce\u30fc\u30c9\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u308f\u304b\u308a\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u540d\u524d\u3092\u4ed8\u3051\u3066\u304a\u304f\u3002","title":"\u30dd\u30a4\u30f3\u30c81"},{"location":"building_graph/tensorflow_protocol_buffers_part3/#2","text":"checkpoint\u3067\u306e\u4fdd\u5b58\u306f2\u884c\u3067\u51fa\u6765\u308b\u3002 1 2 3 saver = tf . train . Saver () ... \u5b66\u7fd2 ... saver . save ( sess , MODEL_DIR + '/model.ckpt' ) checkpoint\u3067\u306f\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u60c5\u5831\uff08graph\u306e\u30e1\u30bf\u60c5\u5831\uff09\u3068\u5b66\u7fd2\u5024\uff08tf.Variable\uff09\u304c\u5225\u3005\u306e\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3055\u308c\u308b\u3002\u30e1\u30bf\u60c5\u5831\u3092\u8aad\u307f\u8fbc\u3093\u3067graph\u3092\u5fa9\u5143\u3059\u308b\u969b\u306b\u3001\u5b66\u7fd2\u7528\u306b\u8a2d\u5b9a\u3057\u305fCPU/GPU\u306e\u30c7\u30d0\u30a4\u30b9\u4f9d\u5b58\u3092\u524a\u9664\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\u3053\u308c\u306b\u3088\u3063\u3066\u30dd\u30fc\u30bf\u30d3\u30ea\u30c6\u30a3\u304c\u5411\u4e0a\u3059\u308b\u3002 \uff08\u305f\u3060\u3057\u3001tf.Variable\u3067\u5909\u6570\u3092\u7528\u610f\u3057\u3066\u3044\u306a\u3044\u5b66\u7fd2\u3057\u306a\u3044\u30e2\u30c7\u30eb\u306e\u5834\u5408\u306fcheckpoint\u3067\u306e\u4fdd\u5b58\u306f\u3067\u304d\u306a\u3044\u3002\u305d\u306e\u5834\u5408\u3001pb\u3078\u306e\u66f8\u304d\u51fa\u3057\u3001\u5229\u7528\u306b\u30dd\u30a4\u30f3\u30c83\u306f\u4e0d\u8981\u306b\u306a\u308b\u3002\uff09","title":"\u30dd\u30a4\u30f3\u30c82"},{"location":"building_graph/tensorflow_protocol_buffers_part3/#3","text":"\u901a\u5e38\u306e\u30e2\u30c7\u30eb\u306f\u5b66\u7fd2\u6e08\u307f\u306eWeight\u3084Bias\u3092\u4fdd\u6301\u3059\u308b\u305f\u3081\u306etf.Variable\u306e\u5909\u6570\u3092\u6301\u3064\u3002\u3053\u306e\u5024\u3092pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u3053\u3068\u306f\u51fa\u6765\u306a\u3044\u305f\u3081\u3001 graph_util.convert_variables_to_constants() \u3092\u4f7f\u3063\u3066Const\u306b\u5909\u63db\u3057\u305fgraph\u3092pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u3002","title":"\u30dd\u30a4\u30f3\u30c83"},{"location":"building_graph/tensorflow_protocol_buffers_part4/","text":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part4 tf.Variable\u3067\u5b9a\u7fa9\u3057\u305fv1\u3067\u3001v1=v1+1\u3092\u5b9f\u884c\u3059\u308b\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u300110\u56de\u5b9f\u884c\u5f8ccheckpoint\u306b\u4fdd\u5b58\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # -*- coding: utf-8 -*- # sample_pb1.py # v1(tf.Variable)\u306e\u5024\u3092\u66f4\u65b0\u3057\u3066checkpoint\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b import tensorflow as tf import os # jupyter\u5b9f\u884c\u7528\u306bGraph\u3092\u521d\u671f\u5316\u3059\u308b tf . reset_default_graph () # checkpoint\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u6e96\u5099 MODEL_DIR = \"./model_data\" if not os . path . exists ( MODEL_DIR ): os . makedirs ( MODEL_DIR ) # graph\u5b9a\u7fa9 # \u30dd\u30a4\u30f3\u30c81:\u91cd\u8981\u306a\u8981\u7d20\u306b\u306f\u5224\u308a\u3084\u3059\u3044\u540d\u524d\u3092\u4ed8\u3051\u3066\u304a\u304f v1 = tf . Variable ( initial_value = 0.0 , name = \"this_is_my_v1\" ) v1_add = v1 . assign_add ( 1 ) # v1=v1+1\u3092v1_add\u3068\u3057\u3066\u7528\u610f\u3059\u308b saver = tf . train . Saver () with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) for i in range ( 10 ): _v1 = sess . run ( v1_add ) # v1=v1+1\u3092\u5b9f\u884c\u3059\u308b print ( \"i: %d v1= %d \" % ( i , _v1 )) # \u30dd\u30a4\u30f3\u30c82:graph\u3068\u5b66\u7fd2\u6e08\u307fv1\u3092checkpoint\u306b\u4fdd\u5b58\u3059\u308b saver . save ( sess , MODEL_DIR + '/model.ckpt' ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 i : 0 v1 = 1 i : 1 v1 = 2 i : 2 v1 = 3 i : 3 v1 = 4 i : 4 v1 = 5 i : 5 v1 = 6 i : 6 v1 = 7 i : 7 v1 = 8 i : 8 v1 = 9 i : 9 v1 = 10 checkpoint\u3092\u8aad\u307f\u8fbc\u3093\u3067pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 # -*- coding: utf-8 -*- # sample_pb2.py # checkpoint\u304b\u3089meta\u3092\u8aad\u307f\u8fbc\u307fgraph\u3092\u5fa9\u5143 # operation\u3092\u8868\u793a # restore\u3067v1\u306e\u5024\u3092\u5fa9\u5143 # tf.Variable\u3092Const\u306b\u5909\u63db\u3057\u305fgraph\u3092pb\u306b\u51fa\u529b\u3059\u308b import tensorflow as tf from tensorflow.python.framework import graph_util # jupyter\u5b9f\u884c\u7528\u306bGraph\u3092\u521d\u671f\u5316\u3059\u308b tf . reset_default_graph () # checkpoint\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea MODEL_DIR = \"./model_data\" # \u4fdd\u5b58\u5148\u306epb\u30d5\u30a1\u30a4\u30eb\u540d FROZEN_MODEL_NAME = \"frozen_model.pb\" # \u30c7\u30d0\u30a4\u30b9\u60c5\u5831\u3092\u524a\u9664\u3059\u308b CLEAR_DEVICES = True # pb\u306b\u66f8\u304d\u51fa\u3059operation\u540d OUTPUT_NODE_NAMES = \"this_is_my_v1\" # graph\u306eoperation\u3092\u8868\u793a\u3059\u308b def print_graph_operations ( graph ): # print operations print \"----- operations in graph -----\" for op in graph . get_operations (): print op . name , op . outputs # graph_def\u306enode\u3092\u8868\u793a\u3059\u308b def print_graph_nodes ( graph_def ): # print nodes print \"----- nodes in graph_def -----\" for node in graph_def . node : print ( node ) # checkpoint\u30d5\u30a1\u30a4\u30eb\u306e\u78ba\u8a8d\u3092\u884c\u3046 checkpoint = tf . train . get_checkpoint_state ( MODEL_DIR ) if not checkpoint : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304c\u898b\u3064\u304b\u3089\u306a\u3044 print ( \"cannot find checkpoint.\" ) else : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = checkpoint . model_checkpoint_path print (( \"load {0} \" . format ( last_model ))) # pb\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8a2d\u5b9a\u3059\u308b absolute_model_dir = \"/\" . join ( last_model . split ( '/' )[: - 1 ]) frozen_model = absolute_model_dir + \"/\" + FROZEN_MODEL_NAME # checkpoint\u306emeta\u30d5\u30a1\u30a4\u30eb\u304b\u3089Graph\u3092\u8aad\u307f\u8fbc\u3080 saver = tf . train . import_meta_graph ( last_model + '.meta' , clear_devices = CLEAR_DEVICES ) # graph\u5b9a\u7fa9\u3092\u53d6\u5f97\u3059\u308b graph = tf . get_default_graph () graph_def = graph . as_graph_def () # print operations print_graph_operations ( graph ) # print nodes #print_graph_nodes(graph_def) with tf . Session () as sess : # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u5024\u3092\u5fa9\u5143\u3059\u308b saver . restore ( sess , last_model ) # tf.Variable\u3092Const\u306b\u5909\u63db\u3057\u305fgraph\u3092\u53d6\u5f97\u3059\u308b output_graph_def = graph_util . convert_variables_to_constants ( sess , graph_def , OUTPUT_NODE_NAMES . split ( \",\" ) ) # pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58 tf . train . write_graph ( output_graph_def , MODEL_DIR , FROZEN_MODEL_NAME , as_text = False ) \u5b9f\u884c\u7d50\u679c\uff1aOUTPUT_NODE_NAMES\u304c\u308f\u304b\u3089\u306a\u3044\u6642\u306f\u8868\u793a\u3055\u308c\u305foperation\u304b\u3089\u30a2\u30bf\u30ea\u3092\u3064\u3051\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 load ./model_data/model.ckpt ----- operations in graph ----- this_is_my_v1/initial_value [<tf.Tensor 'this_is_my_v1/initial_value:0' shape=() dtype=float32>] this_is_my_v1 [<tf.Tensor 'this_is_my_v1:0' shape=() dtype=float32_ref>] this_is_my_v1/Assign [<tf.Tensor 'this_is_my_v1/Assign:0' shape=() dtype=float32_ref>] this_is_my_v1/read [<tf.Tensor 'this_is_my_v1/read:0' shape=() dtype=float32>] AssignAdd/value [<tf.Tensor 'AssignAdd/value:0' shape=() dtype=float32>] AssignAdd [<tf.Tensor 'AssignAdd:0' shape=() dtype=float32_ref>] save/Const [<tf.Tensor 'save/Const:0' shape=() dtype=string>] save/SaveV2/tensor_names [<tf.Tensor 'save/SaveV2/tensor_names:0' shape=(1,) dtype=string>] save/SaveV2/shape_and_slices [<tf.Tensor 'save/SaveV2/shape_and_slices:0' shape=(1,) dtype=string>] save/SaveV2 [] save/control_dependency [<tf.Tensor 'save/control_dependency:0' shape=() dtype=string>] save/RestoreV2/tensor_names [<tf.Tensor 'save/RestoreV2/tensor_names:0' shape=(1,) dtype=string>] save/RestoreV2/shape_and_slices [<tf.Tensor 'save/RestoreV2/shape_and_slices:0' shape=(1,) dtype=string>] save/RestoreV2 [<tf.Tensor 'save/RestoreV2:0' shape=<unknown> dtype=float32>] save/Assign [<tf.Tensor 'save/Assign:0' shape=() dtype=float32_ref>] save/restore_all [] init [] INFO:tensorflow:Froze 1 variables. Converted 1 variables to const ops. pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u307f\u3001v1\u3092\u8868\u793a\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # -*- coding: utf-8 -*- # sample_pb3.py # frozen_model.pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 # v1\u306e\u5024\u3092\u8868\u793a\u3059\u308b import tensorflow as tf import numpy as np tf . reset_default_graph () # checkpoint\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea MODEL_DIR = \"./model_data\" # \u4fdd\u5b58\u5148\u306epb\u30d5\u30a1\u30a4\u30eb\u540d FROZEN_MODEL_NAME = \"frozen_model.pb\" def print_graph_operations ( graph ): # print operations print \"----- operations in graph -----\" for op in graph . get_operations (): print op . name , op . outputs def print_graph_nodes ( graph_def ): # print nodes print \"----- nodes in graph_def -----\" for node in graph_def . node : print ( node ) def load_graph ( frozen_graph_filename ): # pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u307fgraph\u5b9a\u7fa9\u3092\u5fa9\u5143\u3059\u308b with tf . gfile . GFile ( frozen_graph_filename , \"rb\" ) as f : graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) # pb\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u5fa9\u5143\u3057\u305fgraph_def\u3092\u30ab\u30ec\u30f3\u30c8graph_def\u306b\u8a2d\u5b9a\u3059\u308b with tf . Graph () . as_default () as graph : tf . import_graph_def ( graph_def , input_map = None , return_elements = None , name = \"my_prefix\" , op_dict = None , producer_op_list = None ) return graph graph = load_graph ( MODEL_DIR + \"/\" + FROZEN_MODEL_NAME ) graph_def = graph . as_graph_def () # print operations print_graph_operations ( graph ) v1 = graph . get_tensor_by_name ( 'my_prefix/this_is_my_v1:0' ) # v1\u306eoperation\u306e\u51fa\u529bnode\u3092\u53d6\u5f97\u3059\u308b with tf . Session ( graph = graph ) as sess : _v1 = sess . run ( v1 ) # v1\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print ( \"v1= %d \" % ( _v1 )) \u5b9f\u884c\u7d50\u679c\uff1av1\u306b\u5909\u66f4\u3092\u52a0\u3048\u305f\u5024(10)\u304c\u5165\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002 1 2 3 ----- operations in graph ----- my_prefix/this_is_my_v1 [<tf.Tensor 'my_prefix/this_is_my_v1:0' shape=() dtype=float32>] v1=10 pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u969b\u306f\u3001\u5b66\u7fd2\u6e08\u307f\u5024\u3092Const\u306b\u5909\u63db\u3057\u3066\u4fdd\u5b58\u3059\u308b\u3053\u3068\u304c\u6700\u3082\u91cd\u8981\u306a\u70b9\u3068\u306a\u308b\u3002 pb\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5b9f\u884c\u6642\u306b\u300cAttempting to use uninitialized value\u300d\u304c\u51fa\u308b\u5834\u5408\u306f\u3053\u3053\u304c\u539f\u56e0\u3068\u306a\u308b\u3002 my_prefix\u3092\u3064\u3051\u3066\u3044\u308b\u305f\u3081\u3001v1\u306enode\u306f'my_prefix/this_is_my_v1:0'\u3068\u306a\u308b\u3002(:0\u306foperation\u306ereturn\u914d\u5217\u306e0\u756a\u76ee\u306e\u914d\u5217\u3092\u610f\u5473\u3059\u308b\u3002\u4f8b\u3048\u3070v1_add_op\u306ereturn\u304c return a,b\u3067\u3042\u308b\u5834\u5408\u306bb\u3092\u53d6\u308a\u305f\u3044\u3068\u304d\u306f:1\u3068\u306a\u308b\u3002\u3053\u3053\u3067\u306fv1=tf.Variable\u3067\u3042\u3063\u305f\u3082\u306e(\u4eca\u306fConst)\u306a\u306e\u3067:0\u3057\u304b\u306a\u3044) OUTPUT_NODE_NAMES\u3067\u5fc5\u8981\u306anode\u306e\u307f\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001pb\u30d5\u30a1\u30a4\u30eb\u306b\u306f\u6307\u5b9a\u3055\u308c\u305fnode\u3068\u305d\u306e\u7b97\u51fa\u306b\u5fc5\u8981\u306anode\u3057\u304b\u5165\u3089\u306a\u3044\u305f\u3081\u3001\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u3082\u5c0f\u3055\u304f\u306a\u308b\u3002 1 2 3 4 5 4 -rw-r--r-- 1 root root 77 4\u6708 3 18:38 checkpoint 4 -rw-r--r-- 1 root root 60 4\u6708 3 18:39 frozen_model.pb 4 -rw-r--r-- 1 root root 4 4\u6708 3 18:38 model.ckpt.data-00000-of-00001 4 -rw-r--r-- 1 root root 129 4\u6708 3 18:38 model.ckpt.index 4 -rw-r--r-- 1 root root 2866 4\u6708 3 18:38 model.ckpt.meta","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part4"},{"location":"building_graph/tensorflow_protocol_buffers_part4/#part4","text":"","title":"\u30d7\u30ed\u30c8\u30b3\u30eb\u30d0\u30c3\u30d5\u30a1 Part4"},{"location":"building_graph/tensorflow_protocol_buffers_part4/#tfvariablev1v1v1110checkpoint","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # -*- coding: utf-8 -*- # sample_pb1.py # v1(tf.Variable)\u306e\u5024\u3092\u66f4\u65b0\u3057\u3066checkpoint\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b import tensorflow as tf import os # jupyter\u5b9f\u884c\u7528\u306bGraph\u3092\u521d\u671f\u5316\u3059\u308b tf . reset_default_graph () # checkpoint\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u6e96\u5099 MODEL_DIR = \"./model_data\" if not os . path . exists ( MODEL_DIR ): os . makedirs ( MODEL_DIR ) # graph\u5b9a\u7fa9 # \u30dd\u30a4\u30f3\u30c81:\u91cd\u8981\u306a\u8981\u7d20\u306b\u306f\u5224\u308a\u3084\u3059\u3044\u540d\u524d\u3092\u4ed8\u3051\u3066\u304a\u304f v1 = tf . Variable ( initial_value = 0.0 , name = \"this_is_my_v1\" ) v1_add = v1 . assign_add ( 1 ) # v1=v1+1\u3092v1_add\u3068\u3057\u3066\u7528\u610f\u3059\u308b saver = tf . train . Saver () with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) for i in range ( 10 ): _v1 = sess . run ( v1_add ) # v1=v1+1\u3092\u5b9f\u884c\u3059\u308b print ( \"i: %d v1= %d \" % ( i , _v1 )) # \u30dd\u30a4\u30f3\u30c82:graph\u3068\u5b66\u7fd2\u6e08\u307fv1\u3092checkpoint\u306b\u4fdd\u5b58\u3059\u308b saver . save ( sess , MODEL_DIR + '/model.ckpt' ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 i : 0 v1 = 1 i : 1 v1 = 2 i : 2 v1 = 3 i : 3 v1 = 4 i : 4 v1 = 5 i : 5 v1 = 6 i : 6 v1 = 7 i : 7 v1 = 8 i : 8 v1 = 9 i : 9 v1 = 10","title":"tf.Variable\u3067\u5b9a\u7fa9\u3057\u305fv1\u3067\u3001v1=v1+1\u3092\u5b9f\u884c\u3059\u308b\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u300110\u56de\u5b9f\u884c\u5f8ccheckpoint\u306b\u4fdd\u5b58\u3059\u308b"},{"location":"building_graph/tensorflow_protocol_buffers_part4/#checkpointpb","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 # -*- coding: utf-8 -*- # sample_pb2.py # checkpoint\u304b\u3089meta\u3092\u8aad\u307f\u8fbc\u307fgraph\u3092\u5fa9\u5143 # operation\u3092\u8868\u793a # restore\u3067v1\u306e\u5024\u3092\u5fa9\u5143 # tf.Variable\u3092Const\u306b\u5909\u63db\u3057\u305fgraph\u3092pb\u306b\u51fa\u529b\u3059\u308b import tensorflow as tf from tensorflow.python.framework import graph_util # jupyter\u5b9f\u884c\u7528\u306bGraph\u3092\u521d\u671f\u5316\u3059\u308b tf . reset_default_graph () # checkpoint\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea MODEL_DIR = \"./model_data\" # \u4fdd\u5b58\u5148\u306epb\u30d5\u30a1\u30a4\u30eb\u540d FROZEN_MODEL_NAME = \"frozen_model.pb\" # \u30c7\u30d0\u30a4\u30b9\u60c5\u5831\u3092\u524a\u9664\u3059\u308b CLEAR_DEVICES = True # pb\u306b\u66f8\u304d\u51fa\u3059operation\u540d OUTPUT_NODE_NAMES = \"this_is_my_v1\" # graph\u306eoperation\u3092\u8868\u793a\u3059\u308b def print_graph_operations ( graph ): # print operations print \"----- operations in graph -----\" for op in graph . get_operations (): print op . name , op . outputs # graph_def\u306enode\u3092\u8868\u793a\u3059\u308b def print_graph_nodes ( graph_def ): # print nodes print \"----- nodes in graph_def -----\" for node in graph_def . node : print ( node ) # checkpoint\u30d5\u30a1\u30a4\u30eb\u306e\u78ba\u8a8d\u3092\u884c\u3046 checkpoint = tf . train . get_checkpoint_state ( MODEL_DIR ) if not checkpoint : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304c\u898b\u3064\u304b\u3089\u306a\u3044 print ( \"cannot find checkpoint.\" ) else : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = checkpoint . model_checkpoint_path print (( \"load {0} \" . format ( last_model ))) # pb\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8a2d\u5b9a\u3059\u308b absolute_model_dir = \"/\" . join ( last_model . split ( '/' )[: - 1 ]) frozen_model = absolute_model_dir + \"/\" + FROZEN_MODEL_NAME # checkpoint\u306emeta\u30d5\u30a1\u30a4\u30eb\u304b\u3089Graph\u3092\u8aad\u307f\u8fbc\u3080 saver = tf . train . import_meta_graph ( last_model + '.meta' , clear_devices = CLEAR_DEVICES ) # graph\u5b9a\u7fa9\u3092\u53d6\u5f97\u3059\u308b graph = tf . get_default_graph () graph_def = graph . as_graph_def () # print operations print_graph_operations ( graph ) # print nodes #print_graph_nodes(graph_def) with tf . Session () as sess : # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u5024\u3092\u5fa9\u5143\u3059\u308b saver . restore ( sess , last_model ) # tf.Variable\u3092Const\u306b\u5909\u63db\u3057\u305fgraph\u3092\u53d6\u5f97\u3059\u308b output_graph_def = graph_util . convert_variables_to_constants ( sess , graph_def , OUTPUT_NODE_NAMES . split ( \",\" ) ) # pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58 tf . train . write_graph ( output_graph_def , MODEL_DIR , FROZEN_MODEL_NAME , as_text = False ) \u5b9f\u884c\u7d50\u679c\uff1aOUTPUT_NODE_NAMES\u304c\u308f\u304b\u3089\u306a\u3044\u6642\u306f\u8868\u793a\u3055\u308c\u305foperation\u304b\u3089\u30a2\u30bf\u30ea\u3092\u3064\u3051\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 load ./model_data/model.ckpt ----- operations in graph ----- this_is_my_v1/initial_value [<tf.Tensor 'this_is_my_v1/initial_value:0' shape=() dtype=float32>] this_is_my_v1 [<tf.Tensor 'this_is_my_v1:0' shape=() dtype=float32_ref>] this_is_my_v1/Assign [<tf.Tensor 'this_is_my_v1/Assign:0' shape=() dtype=float32_ref>] this_is_my_v1/read [<tf.Tensor 'this_is_my_v1/read:0' shape=() dtype=float32>] AssignAdd/value [<tf.Tensor 'AssignAdd/value:0' shape=() dtype=float32>] AssignAdd [<tf.Tensor 'AssignAdd:0' shape=() dtype=float32_ref>] save/Const [<tf.Tensor 'save/Const:0' shape=() dtype=string>] save/SaveV2/tensor_names [<tf.Tensor 'save/SaveV2/tensor_names:0' shape=(1,) dtype=string>] save/SaveV2/shape_and_slices [<tf.Tensor 'save/SaveV2/shape_and_slices:0' shape=(1,) dtype=string>] save/SaveV2 [] save/control_dependency [<tf.Tensor 'save/control_dependency:0' shape=() dtype=string>] save/RestoreV2/tensor_names [<tf.Tensor 'save/RestoreV2/tensor_names:0' shape=(1,) dtype=string>] save/RestoreV2/shape_and_slices [<tf.Tensor 'save/RestoreV2/shape_and_slices:0' shape=(1,) dtype=string>] save/RestoreV2 [<tf.Tensor 'save/RestoreV2:0' shape=<unknown> dtype=float32>] save/Assign [<tf.Tensor 'save/Assign:0' shape=() dtype=float32_ref>] save/restore_all [] init [] INFO:tensorflow:Froze 1 variables. Converted 1 variables to const ops.","title":"checkpoint\u3092\u8aad\u307f\u8fbc\u3093\u3067pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b"},{"location":"building_graph/tensorflow_protocol_buffers_part4/#pbv1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # -*- coding: utf-8 -*- # sample_pb3.py # frozen_model.pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 # v1\u306e\u5024\u3092\u8868\u793a\u3059\u308b import tensorflow as tf import numpy as np tf . reset_default_graph () # checkpoint\u4fdd\u5b58\u5148\u30c7\u30a3\u30ec\u30af\u30c8\u30ea MODEL_DIR = \"./model_data\" # \u4fdd\u5b58\u5148\u306epb\u30d5\u30a1\u30a4\u30eb\u540d FROZEN_MODEL_NAME = \"frozen_model.pb\" def print_graph_operations ( graph ): # print operations print \"----- operations in graph -----\" for op in graph . get_operations (): print op . name , op . outputs def print_graph_nodes ( graph_def ): # print nodes print \"----- nodes in graph_def -----\" for node in graph_def . node : print ( node ) def load_graph ( frozen_graph_filename ): # pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u307fgraph\u5b9a\u7fa9\u3092\u5fa9\u5143\u3059\u308b with tf . gfile . GFile ( frozen_graph_filename , \"rb\" ) as f : graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) # pb\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u5fa9\u5143\u3057\u305fgraph_def\u3092\u30ab\u30ec\u30f3\u30c8graph_def\u306b\u8a2d\u5b9a\u3059\u308b with tf . Graph () . as_default () as graph : tf . import_graph_def ( graph_def , input_map = None , return_elements = None , name = \"my_prefix\" , op_dict = None , producer_op_list = None ) return graph graph = load_graph ( MODEL_DIR + \"/\" + FROZEN_MODEL_NAME ) graph_def = graph . as_graph_def () # print operations print_graph_operations ( graph ) v1 = graph . get_tensor_by_name ( 'my_prefix/this_is_my_v1:0' ) # v1\u306eoperation\u306e\u51fa\u529bnode\u3092\u53d6\u5f97\u3059\u308b with tf . Session ( graph = graph ) as sess : _v1 = sess . run ( v1 ) # v1\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print ( \"v1= %d \" % ( _v1 )) \u5b9f\u884c\u7d50\u679c\uff1av1\u306b\u5909\u66f4\u3092\u52a0\u3048\u305f\u5024(10)\u304c\u5165\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002 1 2 3 ----- operations in graph ----- my_prefix/this_is_my_v1 [<tf.Tensor 'my_prefix/this_is_my_v1:0' shape=() dtype=float32>] v1=10","title":"pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u307f\u3001v1\u3092\u8868\u793a\u3059\u308b"},{"location":"building_graph/tensorflow_protocol_buffers_part4/#pbconst","text":"pb\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5b9f\u884c\u6642\u306b\u300cAttempting to use uninitialized value\u300d\u304c\u51fa\u308b\u5834\u5408\u306f\u3053\u3053\u304c\u539f\u56e0\u3068\u306a\u308b\u3002 my_prefix\u3092\u3064\u3051\u3066\u3044\u308b\u305f\u3081\u3001v1\u306enode\u306f'my_prefix/this_is_my_v1:0'\u3068\u306a\u308b\u3002(:0\u306foperation\u306ereturn\u914d\u5217\u306e0\u756a\u76ee\u306e\u914d\u5217\u3092\u610f\u5473\u3059\u308b\u3002\u4f8b\u3048\u3070v1_add_op\u306ereturn\u304c return a,b\u3067\u3042\u308b\u5834\u5408\u306bb\u3092\u53d6\u308a\u305f\u3044\u3068\u304d\u306f:1\u3068\u306a\u308b\u3002\u3053\u3053\u3067\u306fv1=tf.Variable\u3067\u3042\u3063\u305f\u3082\u306e(\u4eca\u306fConst)\u306a\u306e\u3067:0\u3057\u304b\u306a\u3044) OUTPUT_NODE_NAMES\u3067\u5fc5\u8981\u306anode\u306e\u307f\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001pb\u30d5\u30a1\u30a4\u30eb\u306b\u306f\u6307\u5b9a\u3055\u308c\u305fnode\u3068\u305d\u306e\u7b97\u51fa\u306b\u5fc5\u8981\u306anode\u3057\u304b\u5165\u3089\u306a\u3044\u305f\u3081\u3001\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u3082\u5c0f\u3055\u304f\u306a\u308b\u3002 1 2 3 4 5 4 -rw-r--r-- 1 root root 77 4\u6708 3 18:38 checkpoint 4 -rw-r--r-- 1 root root 60 4\u6708 3 18:39 frozen_model.pb 4 -rw-r--r-- 1 root root 4 4\u6708 3 18:38 model.ckpt.data-00000-of-00001 4 -rw-r--r-- 1 root root 129 4\u6708 3 18:38 model.ckpt.index 4 -rw-r--r-- 1 root root 2866 4\u6708 3 18:38 model.ckpt.meta","title":"pb\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3059\u308b\u969b\u306f\u3001\u5b66\u7fd2\u6e08\u307f\u5024\u3092Const\u306b\u5909\u63db\u3057\u3066\u4fdd\u5b58\u3059\u308b\u3053\u3068\u304c\u6700\u3082\u91cd\u8981\u306a\u70b9\u3068\u306a\u308b\u3002"},{"location":"cloudml/cloudml/","text":"CloudML\u3067\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u6709\u52b9\u306b\u3059\u308b \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b https://console.cloud.google.com/iam-admin/projects?_ga=1.178290596.1433708546.1475329198 \u8ab2\u91d1\u3092\u6709\u52b9\u306b\u3059\u308b https://console.cloud.google.com/billing API\u3092\u6709\u52b9\u306b\u3059\u308b CloudML\u3067\u5fc5\u8981\u306a\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u4e00\u89a7 Google Cloud Machine Learning Google Dataflow API Google Compute Engine API Stackdriver Logging API Google Cloud Storage Google Cloud Storage JSON API BigQuery API \u6709\u52b9\u306b\u3059\u308b\u305f\u3081\u306e\u30ea\u30f3\u30af https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,dataflow,compute_component,logging,storage_component,storage_api,bigquery Console\u3067CloudML\u3092\u8868\u793a https://console.cloud.google.com/ml Google Cloud Shell\u3092\u6709\u52b9\u306b\u3059\u308b CloudML\u306e\u8a2d\u5b9a 1 2 3 4 $ curl https://raw.githubusercontent.com/GoogleCloudPlatform/cloudml-samples/master/tools/setup_cloud_shell.sh | bash $ export PATH = ${ HOME } /.local/bin: ${ PATH } $ curl https://raw.githubusercontent.com/GoogleCloudPlatform/cloudml-samples/master/tools/check_environment.py | python $ gcloud beta ml init-project \u30d0\u30b1\u30c3\u30c8\u306e\u8a2d\u5b9a\u3001your_bucket_name\u3092\u4efb\u610f\u306e\u540d\u524d\u306b\u3059\u308b 1 2 3 4 $ PROJECT_ID = $( gcloud config list project --format \"value(core.project)\" ) $ BUCKET_NAME = ${ PROJECT_ID } -ml $ BUCKET_NAME = \"your_bucket_name\" $ gsutil mb -l us-central1 gs:// $BUCKET_NAME HelloTensor 1 $ ipython 1 2 3 4 5 import tensorflow as tf hello = tf . constant ( 'Hello' ) sess = tf . Session () print sess . run ( hello )","title":"CloudML\u3067\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"cloudml/cloudml/#cloudml","text":"","title":"CloudML\u3067\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"cloudml/cloudml/#_1","text":"https://console.cloud.google.com/iam-admin/projects?_ga=1.178290596.1433708546.1475329198","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3059\u308b"},{"location":"cloudml/cloudml/#_2","text":"https://console.cloud.google.com/billing","title":"\u8ab2\u91d1\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"cloudml/cloudml/#api","text":"CloudML\u3067\u5fc5\u8981\u306a\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u4e00\u89a7 Google Cloud Machine Learning Google Dataflow API Google Compute Engine API Stackdriver Logging API Google Cloud Storage Google Cloud Storage JSON API BigQuery API \u6709\u52b9\u306b\u3059\u308b\u305f\u3081\u306e\u30ea\u30f3\u30af https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,dataflow,compute_component,logging,storage_component,storage_api,bigquery","title":"API\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"cloudml/cloudml/#consolecloudml","text":"https://console.cloud.google.com/ml","title":"Console\u3067CloudML\u3092\u8868\u793a"},{"location":"cloudml/cloudml/#google-cloud-shell","text":"","title":"Google Cloud Shell\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"cloudml/cloudml/#cloudml_1","text":"1 2 3 4 $ curl https://raw.githubusercontent.com/GoogleCloudPlatform/cloudml-samples/master/tools/setup_cloud_shell.sh | bash $ export PATH = ${ HOME } /.local/bin: ${ PATH } $ curl https://raw.githubusercontent.com/GoogleCloudPlatform/cloudml-samples/master/tools/check_environment.py | python $ gcloud beta ml init-project \u30d0\u30b1\u30c3\u30c8\u306e\u8a2d\u5b9a\u3001your_bucket_name\u3092\u4efb\u610f\u306e\u540d\u524d\u306b\u3059\u308b 1 2 3 4 $ PROJECT_ID = $( gcloud config list project --format \"value(core.project)\" ) $ BUCKET_NAME = ${ PROJECT_ID } -ml $ BUCKET_NAME = \"your_bucket_name\" $ gsutil mb -l us-central1 gs:// $BUCKET_NAME","title":"CloudML\u306e\u8a2d\u5b9a"},{"location":"cloudml/cloudml/#hellotensor","text":"1 $ ipython 1 2 3 4 5 import tensorflow as tf hello = tf . constant ( 'Hello' ) sess = tf . Session () print sess . run ( hello )","title":"HelloTensor"},{"location":"cloudml/emacs/","text":"Emacs\u306e\u64cd\u4f5c \u30ad\u30fc\u57fa\u672c \u30ad\u30fc M- ESC\u30ad\u30fc\u3092\u62bc\u3057\u305f\u5f8c C- Ctrl\u30ad\u30fc\u3092\u62bc\u3057\u306a\u304c\u3089 [RET] \u30ea\u30bf\u30fc\u30f3\u30ad\u30fc [SP] \u30b9\u30da\u30fc\u30b9\u30ad\u30fc C-x C-c \u7d42\u4e86 C-x C-s \u4fdd\u5b58 C-x C-f \u65e2\u5b58\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304f/\u65b0\u898f\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 ~~C-x C-w~~ ~~\u5225\u540d\u3067\u4fdd\u5b58~~ \u30d6\u30e9\u30a6\u30b6\u304c\u9589\u3058\u3066\u3057\u307e\u3046\u306e\u3067\u4f7f\u3048\u306a\u3044 C-g \u5165\u529b\u4e2d\u306e\u30b3\u30de\u30f3\u30c9\u3092\u30ad\u30e3\u30f3\u30bb\u30eb C-e \u884c\u672b\u306b\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u3092\u79fb\u52d5 C-a \u884c\u982d\u306b\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u3092\u79fb\u52d5 C-k \u884c\u3092\u30ab\u30c3\u30c8\u3059\u308b(\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u306b\u30b3\u30d4\u30fc\u3057\u3066\u524a\u9664) \u9023\u7d9a\u3057\u3066\u524a\u9664\u3059\u308b\u3068\u3001\u305d\u306e\u7bc4\u56f2\u5206\u3092\u30b3\u30d4\u30fc\u3059\u308b C-y \u30b3\u30d4\u30fc\u3055\u308c\u3066\u3044\u308b\u5185\u5bb9\u3092\u30da\u30fc\u30b9\u30c8 M-x goto-line [RET] \u6570\u5b57 \u30ea\u30bf\u30fc\u30f3\u5165\u529b\u5f8c\u3001\u884c\u6570\u3092\u5165\u529b\u3002\u6307\u5b9a\u3057\u305f\u884c\u306b\u98db\u3076 C-s \u691c\u7d22\u3057\u305f\u3044\u6587\u5b57\u5217\u3092\u5165\u529b\u3002\u6587\u5b57\u5165\u529b\u5f8c\u306bC-s\u3092\u7d9a\u3051\u308b\u3068\u6b21\u306e\u691c\u7d22\u30d2\u30c3\u30c8\u306b\u98db\u3076 C-u \u6570\u5b57 \u6570\u5b57\u3092\u5165\u529b\u5f8c\u3001\u6587\u5b57\u5165\u529bor\u30de\u30af\u30ed\u5b9f\u884c\u3092\u6307\u5b9a\u3057\u305f\u56de\u6570\u7e70\u308a\u8fd4\u3059 \u30bf\u30a4\u30c8\u30eb\u98fe\u308a\u3084\u6570\u5343\u56de\u5b9f\u884c\u3059\u308b\u64cd\u4f5c\u3092\u30de\u30af\u30ed\u767b\u9332\u3057\u3066\u5b9f\u884c\u3059\u308b\u306e\u306b\u4f7f\u3046 C-[SP] \u30de\u30fc\u30af\u958b\u59cb\u3002\u30de\u30fc\u30af\u7d42\u4e86\u306f\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e ~~C-w~~ ~~\u30de\u30fc\u30af\u7bc4\u56f2\u3092\u524a\u9664~~ \u30d6\u30e9\u30a6\u30b6\u304c\u9589\u3058\u3066\u3057\u307e\u3046\u306e\u3067\u4f7f\u3048\u306a\u3044 M-x delete-rectangle \u30de\u30fc\u30af\u958b\u59cb\u4f4d\u7f6e\u304b\u3089\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u307e\u3067\u306e\u56db\u89d2\u5f62\u7bc4\u56f2\u3092\u524a\u9664\u3059\u308b \u30b3\u30e1\u30f3\u30c8\u4e00\u62ec\u524a\u9664\u3084\u30a4\u30f3\u30c7\u30f3\u30c8\u4e00\u62ec\u524a\u9664\u306a\u3069\u306b\u4f7f\u3046 C-( \u30de\u30af\u30ed\u958b\u59cb\u3002\u30de\u30af\u30ed\u7d42\u4e86\u307e\u3067\u30ad\u30fc\u64cd\u4f5c\u3092\u8a18\u61b6\u3059\u308b \u884c\u982d\u304b\u3089\u30de\u30af\u30ed\u958b\u59cb\u3067\u3001\u30ab\u30f3\u30de4\u500b\u76ee\u306e\u691c\u7d22\u3092\u66f8\u304d\u63db\u3048\u3066\u6b21\u306e\u884c\u982d\u306b\u79fb\u52d5\u3057\u3066\u30de\u30af\u30ed\u7d42\u4e86\u3002\u3053\u308c\u30922000\u56de\u5b9f\u884c\u3059\u308b\u6642\u306a\u3069\u306b\u3002\u64cd\u4f5c\u306b\u5931\u6557\u3057\u305f\u3089C-g\u3067\u30ad\u30e3\u30f3\u30bb\u30eb\u3057\u3066\u3084\u308a\u76f4\u3057\u3002query-replace\u3067\u306f\u5bfe\u5fdc\u3067\u304d\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u306e\u6642\u306b\u3002 C-) \u30de\u30af\u30ed\u7d42\u4e86 C-x e \u30de\u30af\u30ed\u5b9f\u884c C-u 2000 C-x e\u3067\u767b\u9332\u3057\u305f\u30de\u30af\u30ed\u30922000\u56de\u5b9f\u884c\u3059\u308b M-j \u6539\u884c\u5165\u529b C-q j \u6539\u884c\u5165\u529b M-i \u30bf\u30d6\u5165\u529b C-q i \u30bf\u30d6\u5165\u529b C-q m CR\u6539\u884c\u30b3\u30fc\u30c9(\\r)\u5165\u529b \u884c\u672b\u306b\u898b\u3048\u308b^M\u30b3\u30fc\u30c9\u3092query-replace\u3067\u7f6e\u63db\u3059\u308b\u969b\u306b\u4f7f\u3046 M-x query-replace \u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u4ee5\u964d\u306b\u5bfe\u3059\u308b\u6587\u5b57\u5217\u7f6e\u63db\u3002\u30ea\u30bf\u30fc\u30f3\u5165\u529b\u5f8c\u3001\u30d3\u30d5\u30a9\u30fc\u3001\u30a2\u30d5\u30bf\u30fc\u3092\u5165\u529b\u3002n\u3067\u30b9\u30ad\u30c3\u30d7\u3001y\u3067\u7f6e\u63db\u3001!\u3067\u6b8b\u308a\u5168\u7f6e\u63db C-x 2 Window2\u5206\u5272 \u30b3\u30fc\u30c9\u6bd4\u8f03\u306b\u4f7f\u3046\u3053\u3068\u3082\u3042\u308b\u3051\u3069\u898b\u8f9b\u3044\u306e\u3067\u666e\u6bb5\u5229\u7528\u306f\u3057\u306a\u3044 C-x 0 Window\u5206\u5272\u89e3\u9664 C-x 5 2 \u5225Window\u8d77\u52d5(\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306a\u306e\u3067\u7121\u99c4\u306a\u8d77\u52d5) C-x 5 0 \u5225Window\u89e3\u9664 C-|japanese\u3067\u65e5\u672c\u8a9e\u5165\u529b\u3002\u6f22\u5b57\u5909\u63db\u306f\u30b9\u30da\u30fc\u30b9 M-x set-buffer-file-encoding-system utf-8-unix\u3067utf-8\u6539\u884c\u30b3\u30fc\u30c9\\n\u3002utf-8-dos\u3067utf-8\u6539\u884c\u30b3\u30fc\u30c9\\r\\n C-x [RET] c utf-8-unix [RET] C-x C-f \u6587\u5b57\u30b3\u30fc\u30c9utf-8-unix\u3092\u6307\u5b9a\u3057\u3066\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304f \u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3044\u305f\u3068\u304d\u306b\u6587\u5b57\u304c\u5316\u3051\u3066\u3057\u307e\u3046\u5834\u5408\u306b\u4f7f\u3046","title":"Emacs\u306e\u64cd\u4f5c"},{"location":"cloudml/emacs/#emacs","text":"\u30ad\u30fc\u57fa\u672c \u30ad\u30fc M- ESC\u30ad\u30fc\u3092\u62bc\u3057\u305f\u5f8c C- Ctrl\u30ad\u30fc\u3092\u62bc\u3057\u306a\u304c\u3089 [RET] \u30ea\u30bf\u30fc\u30f3\u30ad\u30fc [SP] \u30b9\u30da\u30fc\u30b9\u30ad\u30fc C-x C-c \u7d42\u4e86 C-x C-s \u4fdd\u5b58 C-x C-f \u65e2\u5b58\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304f/\u65b0\u898f\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 ~~C-x C-w~~ ~~\u5225\u540d\u3067\u4fdd\u5b58~~ \u30d6\u30e9\u30a6\u30b6\u304c\u9589\u3058\u3066\u3057\u307e\u3046\u306e\u3067\u4f7f\u3048\u306a\u3044 C-g \u5165\u529b\u4e2d\u306e\u30b3\u30de\u30f3\u30c9\u3092\u30ad\u30e3\u30f3\u30bb\u30eb C-e \u884c\u672b\u306b\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u3092\u79fb\u52d5 C-a \u884c\u982d\u306b\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u3092\u79fb\u52d5 C-k \u884c\u3092\u30ab\u30c3\u30c8\u3059\u308b(\u30af\u30ea\u30c3\u30d7\u30dc\u30fc\u30c9\u306b\u30b3\u30d4\u30fc\u3057\u3066\u524a\u9664) \u9023\u7d9a\u3057\u3066\u524a\u9664\u3059\u308b\u3068\u3001\u305d\u306e\u7bc4\u56f2\u5206\u3092\u30b3\u30d4\u30fc\u3059\u308b C-y \u30b3\u30d4\u30fc\u3055\u308c\u3066\u3044\u308b\u5185\u5bb9\u3092\u30da\u30fc\u30b9\u30c8 M-x goto-line [RET] \u6570\u5b57 \u30ea\u30bf\u30fc\u30f3\u5165\u529b\u5f8c\u3001\u884c\u6570\u3092\u5165\u529b\u3002\u6307\u5b9a\u3057\u305f\u884c\u306b\u98db\u3076 C-s \u691c\u7d22\u3057\u305f\u3044\u6587\u5b57\u5217\u3092\u5165\u529b\u3002\u6587\u5b57\u5165\u529b\u5f8c\u306bC-s\u3092\u7d9a\u3051\u308b\u3068\u6b21\u306e\u691c\u7d22\u30d2\u30c3\u30c8\u306b\u98db\u3076 C-u \u6570\u5b57 \u6570\u5b57\u3092\u5165\u529b\u5f8c\u3001\u6587\u5b57\u5165\u529bor\u30de\u30af\u30ed\u5b9f\u884c\u3092\u6307\u5b9a\u3057\u305f\u56de\u6570\u7e70\u308a\u8fd4\u3059 \u30bf\u30a4\u30c8\u30eb\u98fe\u308a\u3084\u6570\u5343\u56de\u5b9f\u884c\u3059\u308b\u64cd\u4f5c\u3092\u30de\u30af\u30ed\u767b\u9332\u3057\u3066\u5b9f\u884c\u3059\u308b\u306e\u306b\u4f7f\u3046 C-[SP] \u30de\u30fc\u30af\u958b\u59cb\u3002\u30de\u30fc\u30af\u7d42\u4e86\u306f\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e ~~C-w~~ ~~\u30de\u30fc\u30af\u7bc4\u56f2\u3092\u524a\u9664~~ \u30d6\u30e9\u30a6\u30b6\u304c\u9589\u3058\u3066\u3057\u307e\u3046\u306e\u3067\u4f7f\u3048\u306a\u3044 M-x delete-rectangle \u30de\u30fc\u30af\u958b\u59cb\u4f4d\u7f6e\u304b\u3089\u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u307e\u3067\u306e\u56db\u89d2\u5f62\u7bc4\u56f2\u3092\u524a\u9664\u3059\u308b \u30b3\u30e1\u30f3\u30c8\u4e00\u62ec\u524a\u9664\u3084\u30a4\u30f3\u30c7\u30f3\u30c8\u4e00\u62ec\u524a\u9664\u306a\u3069\u306b\u4f7f\u3046 C-( \u30de\u30af\u30ed\u958b\u59cb\u3002\u30de\u30af\u30ed\u7d42\u4e86\u307e\u3067\u30ad\u30fc\u64cd\u4f5c\u3092\u8a18\u61b6\u3059\u308b \u884c\u982d\u304b\u3089\u30de\u30af\u30ed\u958b\u59cb\u3067\u3001\u30ab\u30f3\u30de4\u500b\u76ee\u306e\u691c\u7d22\u3092\u66f8\u304d\u63db\u3048\u3066\u6b21\u306e\u884c\u982d\u306b\u79fb\u52d5\u3057\u3066\u30de\u30af\u30ed\u7d42\u4e86\u3002\u3053\u308c\u30922000\u56de\u5b9f\u884c\u3059\u308b\u6642\u306a\u3069\u306b\u3002\u64cd\u4f5c\u306b\u5931\u6557\u3057\u305f\u3089C-g\u3067\u30ad\u30e3\u30f3\u30bb\u30eb\u3057\u3066\u3084\u308a\u76f4\u3057\u3002query-replace\u3067\u306f\u5bfe\u5fdc\u3067\u304d\u306a\u3044\u30d1\u30bf\u30fc\u30f3\u306e\u6642\u306b\u3002 C-) \u30de\u30af\u30ed\u7d42\u4e86 C-x e \u30de\u30af\u30ed\u5b9f\u884c C-u 2000 C-x e\u3067\u767b\u9332\u3057\u305f\u30de\u30af\u30ed\u30922000\u56de\u5b9f\u884c\u3059\u308b M-j \u6539\u884c\u5165\u529b C-q j \u6539\u884c\u5165\u529b M-i \u30bf\u30d6\u5165\u529b C-q i \u30bf\u30d6\u5165\u529b C-q m CR\u6539\u884c\u30b3\u30fc\u30c9(\\r)\u5165\u529b \u884c\u672b\u306b\u898b\u3048\u308b^M\u30b3\u30fc\u30c9\u3092query-replace\u3067\u7f6e\u63db\u3059\u308b\u969b\u306b\u4f7f\u3046 M-x query-replace \u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u4ee5\u964d\u306b\u5bfe\u3059\u308b\u6587\u5b57\u5217\u7f6e\u63db\u3002\u30ea\u30bf\u30fc\u30f3\u5165\u529b\u5f8c\u3001\u30d3\u30d5\u30a9\u30fc\u3001\u30a2\u30d5\u30bf\u30fc\u3092\u5165\u529b\u3002n\u3067\u30b9\u30ad\u30c3\u30d7\u3001y\u3067\u7f6e\u63db\u3001!\u3067\u6b8b\u308a\u5168\u7f6e\u63db C-x 2 Window2\u5206\u5272 \u30b3\u30fc\u30c9\u6bd4\u8f03\u306b\u4f7f\u3046\u3053\u3068\u3082\u3042\u308b\u3051\u3069\u898b\u8f9b\u3044\u306e\u3067\u666e\u6bb5\u5229\u7528\u306f\u3057\u306a\u3044 C-x 0 Window\u5206\u5272\u89e3\u9664 C-x 5 2 \u5225Window\u8d77\u52d5(\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306a\u306e\u3067\u7121\u99c4\u306a\u8d77\u52d5) C-x 5 0 \u5225Window\u89e3\u9664 C-|japanese\u3067\u65e5\u672c\u8a9e\u5165\u529b\u3002\u6f22\u5b57\u5909\u63db\u306f\u30b9\u30da\u30fc\u30b9 M-x set-buffer-file-encoding-system utf-8-unix\u3067utf-8\u6539\u884c\u30b3\u30fc\u30c9\\n\u3002utf-8-dos\u3067utf-8\u6539\u884c\u30b3\u30fc\u30c9\\r\\n C-x [RET] c utf-8-unix [RET] C-x C-f \u6587\u5b57\u30b3\u30fc\u30c9utf-8-unix\u3092\u6307\u5b9a\u3057\u3066\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304f \u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3044\u305f\u3068\u304d\u306b\u6587\u5b57\u304c\u5316\u3051\u3066\u3057\u307e\u3046\u5834\u5408\u306b\u4f7f\u3046","title":"Emacs\u306e\u64cd\u4f5c"},{"location":"cloudml/versionup/","text":"TensorFlow\u306eVersion UP \u672c\u30c6\u30ad\u30b9\u30c8\u7528\u306b\u3001TensorFlow\u306eVersion\u30921.0.0\u306bUpdate\u3059\u308b\u3002 1 2 3 4 5 $ wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl $ pip install --user --upgrade tensorflow-1.0.0-cp27-none-linux_x86_64.whl $ python -c 'import tensorflow as tf; print tf.__version__' 1 .0.0","title":"TensorFlow\u306eVersion UP"},{"location":"cloudml/versionup/#tensorflowversion-up","text":"\u672c\u30c6\u30ad\u30b9\u30c8\u7528\u306b\u3001TensorFlow\u306eVersion\u30921.0.0\u306bUpdate\u3059\u308b\u3002 1 2 3 4 5 $ wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl $ pip install --user --upgrade tensorflow-1.0.0-cp27-none-linux_x86_64.whl $ python -c 'import tensorflow as tf; print tf.__version__' 1 .0.0","title":"TensorFlow\u306eVersion UP"},{"location":"datalab/datalab/","text":"Google Cloud DataLab Google Cloud DataLab\u306f\u3001\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3001\u8996\u899a\u5316\u3001\u5206\u6790\u3001\u5909\u63db\u3059\u308b\u305f\u3081\u306e\u3001\u751f\u7523\u6027\u306e\u9ad8\u3044\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u7d71\u5408\u30c4\u30fc\u30eb\u3067\u3059\u3002Google Cloud Datalab\u306f\u3001Docker\u3067\u8d77\u52d5\u3001GCP\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u8d77\u52d5\u304c\u53ef\u80fd\u3067\u3042\u308b\u3002\u4eca\u56de\u306f\u3001\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067Docker\u3067\u8d77\u52d5\u3057\u3066\u3044\u304f\u3002 Cloud SDK\u3092\u7d4c\u7531\u3057\u3066GCP\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068\u9023\u643a Datalab\u306f\u3001Cloud SDK\u3092\u7d4c\u7531\u3057\u3066\u3001GCP\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068\u9023\u643a\u304c\u53ef\u80fd\u306b\u306a\u308b\u3002\u3082\u3061\u308d\u3093\u3001\u9023\u643a\u3057\u306a\u3044\u3067\u8d77\u52d5\u3059\u308b\u4e8b\u3082\u53ef\u80fd\u3067\u3042\u308b\u3002GCP\u306e\u8a2d\u5b9a\u6642\u306b\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3057\u305f\u5834\u5408\u306f\u3001\u305d\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068\u9023\u643a\u3067\u304d\u308b\u306e\u3067\u3001GCP\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u3001Cloud Shell\u3092\u8d77\u52d5\u3057\u3001\u30b3\u30de\u30f3\u30c9\u3067\u53d6\u5f97\u3059\u308b\u3002 [CloudML\u3067\u5b9f\u884c] 1 $ gcloud projects list Datalab\u306e\u8d77\u52d5 \u4e0b\u8a18\u306fOS X\u7528\u3002Windows\u7528\u306eDocker\u8a2d\u5b9a\u306f\u3001 https://cloud.google.com/datalab/docs/quickstarts/quickstart-local \u3092\u53c2\u7167\u3002 [Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb] Docker\u516c\u5f0f\u30b5\u30a4\u30c8 \u3067 Get Started \u3092\u9078\u629e\u3001\u6b21\u306bDownload Docker for Mac\u3092\u9078\u629e\u3057\u672c\u4f53\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u3002\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u7d42\u4e86\u5f8c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u884c\u3044\u3001\u7d42\u4e86\u5f8c\u306b\u8868\u793a\u3055\u308c\u308b\u30a2\u30a4\u30b3\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3053\u3068\u3067Docker\u3092\u7acb\u3061\u4e0a\u3052\u308b\u3002\u305d\u306e\u5f8c\u4ee5\u4e0b\u306e\u3069\u3061\u3089\u304b\u3067Datalab\u3092\u8d77\u52d5\u3059\u308b\u3002 [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u6307\u5b9a] 1 2 3 4 5 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ -e \"PROJECT_ID=\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\" \\ gcr.io/cloud-datalab/datalab:local [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a] 1 2 3 4 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ gcr.io/cloud-datalab/datalab:local [windows\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a] - docker-toolbox\u3092\u8981\u6c42\u3055\u308c\u306a\u3044\u5834\u5408 OS X\u3067\u5b9f\u884c\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a \u3068 \u6050\u3089\u304f \u540c\u69d8\u3067\u3059\u3002 docker-toolbox\u3092\u8981\u6c42\u3055\u308c\u305f\u5834\u5408 boot2docker\u3092\u8981\u6c42\u3055\u308c\u305f\u5834\u5408\u306f\u3001 Docker Quickstart Terminal \u3092\u8d77\u52d5\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 1 2 3 4 $ cd ~ $ mkdir -p ./datalab $ docker run -p 8081 :8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ gcr.io/cloud-datalab/datalab:local \u305d\u306e\u5f8c\u3001\u30d6\u30e9\u30a6\u30b6\u3067 192.168.99.100:8081 \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3057\u3087\u3046\u3002 \u5f15\u6570 \u610f\u5473 -p 127.0.0.1:8081:8080 Datalab\u306b8081\u30dd\u30fc\u30c8\u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b -p 6006:6006 TensorBoard\u306b6006\u30dd\u30fc\u30c8\u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b -v \"${HOME}/datalab:/content\" OSX\u4e0a\u306e${HOME}/datalab\u30d5\u30a9\u30eb\u30c0\u3068Docker\u5185\u306e/contant\u3092\u9023\u643a\u3059\u308b gcr.io/cloud-datalab/datalab:local \u8d77\u52d5\u3059\u308bDocker Image Datalab\u306b\u30a2\u30af\u30bb\u30b9 Browser\u3067\u3001localhost:8081\u306b\u63a5\u7d9a\u3057\u307e\u3059\u3002 Hello World 1 2 3 4 5 6 7 8 9 import matplotlib.pyplot as plt import numpy as np % matplotlib inline x = np . arange ( 0 , 10 , 0.1 ) y = np . sin ( x ) plt . plot ( x , y ) plt . show ()","title":"Google Cloud DataLab"},{"location":"datalab/datalab/#google-cloud-datalab","text":"Google Cloud DataLab\u306f\u3001\u30c7\u30fc\u30bf\u3092\u63a2\u7d22\u3001\u8996\u899a\u5316\u3001\u5206\u6790\u3001\u5909\u63db\u3059\u308b\u305f\u3081\u306e\u3001\u751f\u7523\u6027\u306e\u9ad8\u3044\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u7d71\u5408\u30c4\u30fc\u30eb\u3067\u3059\u3002Google Cloud Datalab\u306f\u3001Docker\u3067\u8d77\u52d5\u3001GCP\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u8d77\u52d5\u304c\u53ef\u80fd\u3067\u3042\u308b\u3002\u4eca\u56de\u306f\u3001\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067Docker\u3067\u8d77\u52d5\u3057\u3066\u3044\u304f\u3002","title":"Google Cloud DataLab"},{"location":"datalab/datalab/#cloud-sdkgcp","text":"Datalab\u306f\u3001Cloud SDK\u3092\u7d4c\u7531\u3057\u3066\u3001GCP\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068\u9023\u643a\u304c\u53ef\u80fd\u306b\u306a\u308b\u3002\u3082\u3061\u308d\u3093\u3001\u9023\u643a\u3057\u306a\u3044\u3067\u8d77\u52d5\u3059\u308b\u4e8b\u3082\u53ef\u80fd\u3067\u3042\u308b\u3002GCP\u306e\u8a2d\u5b9a\u6642\u306b\u3001\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210\u3057\u305f\u5834\u5408\u306f\u3001\u305d\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068\u9023\u643a\u3067\u304d\u308b\u306e\u3067\u3001GCP\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u3001Cloud Shell\u3092\u8d77\u52d5\u3057\u3001\u30b3\u30de\u30f3\u30c9\u3067\u53d6\u5f97\u3059\u308b\u3002 [CloudML\u3067\u5b9f\u884c] 1 $ gcloud projects list","title":"Cloud SDK\u3092\u7d4c\u7531\u3057\u3066GCP\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3068\u9023\u643a"},{"location":"datalab/datalab/#datalab","text":"\u4e0b\u8a18\u306fOS X\u7528\u3002Windows\u7528\u306eDocker\u8a2d\u5b9a\u306f\u3001 https://cloud.google.com/datalab/docs/quickstarts/quickstart-local \u3092\u53c2\u7167\u3002 [Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb] Docker\u516c\u5f0f\u30b5\u30a4\u30c8 \u3067 Get Started \u3092\u9078\u629e\u3001\u6b21\u306bDownload Docker for Mac\u3092\u9078\u629e\u3057\u672c\u4f53\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u3002\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u7d42\u4e86\u5f8c\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u884c\u3044\u3001\u7d42\u4e86\u5f8c\u306b\u8868\u793a\u3055\u308c\u308b\u30a2\u30a4\u30b3\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3059\u308b\u3053\u3068\u3067Docker\u3092\u7acb\u3061\u4e0a\u3052\u308b\u3002\u305d\u306e\u5f8c\u4ee5\u4e0b\u306e\u3069\u3061\u3089\u304b\u3067Datalab\u3092\u8d77\u52d5\u3059\u308b\u3002 [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u6307\u5b9a] 1 2 3 4 5 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ -e \"PROJECT_ID=\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\" \\ gcr.io/cloud-datalab/datalab:local [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a] 1 2 3 4 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ gcr.io/cloud-datalab/datalab:local [windows\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a] - docker-toolbox\u3092\u8981\u6c42\u3055\u308c\u306a\u3044\u5834\u5408 OS X\u3067\u5b9f\u884c\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a \u3068 \u6050\u3089\u304f \u540c\u69d8\u3067\u3059\u3002 docker-toolbox\u3092\u8981\u6c42\u3055\u308c\u305f\u5834\u5408 boot2docker\u3092\u8981\u6c42\u3055\u308c\u305f\u5834\u5408\u306f\u3001 Docker Quickstart Terminal \u3092\u8d77\u52d5\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 1 2 3 4 $ cd ~ $ mkdir -p ./datalab $ docker run -p 8081 :8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ gcr.io/cloud-datalab/datalab:local \u305d\u306e\u5f8c\u3001\u30d6\u30e9\u30a6\u30b6\u3067 192.168.99.100:8081 \u306b\u30a2\u30af\u30bb\u30b9\u3057\u307e\u3057\u3087\u3046\u3002 \u5f15\u6570 \u610f\u5473 -p 127.0.0.1:8081:8080 Datalab\u306b8081\u30dd\u30fc\u30c8\u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b -p 6006:6006 TensorBoard\u306b6006\u30dd\u30fc\u30c8\u3067\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b -v \"${HOME}/datalab:/content\" OSX\u4e0a\u306e${HOME}/datalab\u30d5\u30a9\u30eb\u30c0\u3068Docker\u5185\u306e/contant\u3092\u9023\u643a\u3059\u308b gcr.io/cloud-datalab/datalab:local \u8d77\u52d5\u3059\u308bDocker Image","title":"Datalab\u306e\u8d77\u52d5"},{"location":"datalab/datalab/#datalab_1","text":"Browser\u3067\u3001localhost:8081\u306b\u63a5\u7d9a\u3057\u307e\u3059\u3002","title":"Datalab\u306b\u30a2\u30af\u30bb\u30b9"},{"location":"datalab/datalab/#hello-world","text":"1 2 3 4 5 6 7 8 9 import matplotlib.pyplot as plt import numpy as np % matplotlib inline x = np . arange ( 0 , 10 , 0.1 ) y = np . sin ( x ) plt . plot ( x , y ) plt . show ()","title":"Hello World"},{"location":"datalab/datalab_update_tf1/","text":"Google Cloud DataLab\u306eTensorFlow\u3092Update \uff08\u3053\u306e\u9805\u76ee\u306fTensorFlow r1.0\u30ea\u30ea\u30fc\u30b9\u524d\u306b\u3001\u6700\u65b0\u7248\u306eTensorFlow\u306b\u66f4\u65b0\u3057\u305f\u969b\u306e\u60c5\u5831\u3067\u3059\u3002\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u66f4\u65b0\u304c\u5fc5\u8981\u306a\u6642\u306b\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044) \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u5fc5\u8981\u306aTensorFlow\u306eVersion\u306f\u30011.0.0\u3067\u3059\u3002 Docker\u30b3\u30f3\u30c6\u30ca\u4e00\u89a7 docker ps -a \u3067Docker\u30b3\u30f3\u30c6\u30ca\u306e\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u3002(\u8a73\u7d30\u306fman docker-ps) 1 $ docker ps -a Docker\u30b3\u30f3\u30c6\u30ca\u3068\u306f\uff1aDocker\u30a4\u30e1\u30fc\u30b8\u3092docker run\u3059\u308b\u3053\u3068\u3067\u4f5c\u3089\u308c\u308bDocker\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u3053\u3068\u3002docker\u304c\u5b9f\u884c\u3059\u308b\u30d7\u30ed\u30bb\u30b9\u30a4\u30e1\u30fc\u30b8\u3002docker stop/start\u3067\u505c\u6b62/\u8d77\u52d5\u304c\u53ef\u80fd\u3002 Console\u3078\u30ed\u30b0\u30a4\u30f3 \u53d6\u5f97\u3057\u305fName\u3082\u3057\u304f\u306f\u3001\u30b3\u30f3\u30c6\u30caID\u3092\u6307\u5b9a\u3057\u3066\u3001\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3002 [\u30b3\u30f3\u30c6\u30caID\u3067\u8d77\u52d5ID\u3067\u8d77\u52d5] 1 $ docker exec -it \u30b3\u30f3\u30c6\u30caID /bin/bash [Name\u3067\u8d77\u52d5] 1 $ docker exec -it MAME /bin/bash Python\u306eVersion Console\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u305f\u3089Python\u306eVersion\u3092\u8abf\u3079\u308b\u3002 1 2 $ python -V Python 2 .7.9 TensorFlow 1.0.0\u3078Update Python 2.7\u7cfb\u3067\u3001TensorFlow 1.0.0\u306bUpdate\u3059\u308b\u3002 1 2 $ export TF_BINARY_URL = https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0rc0-cp27-none-linux_x86_64.whl $ pip install --ignore-installed --upgrade $TF_BINARY_URL \u307e\u305f\u306f 1 pip install tensorflow -U Update\u306e\u78ba\u8a8d \u4e00\u901a\u308a\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u304c\u7d42\u308f\u3063\u305f\u3089 1 $ pip list \u3067tensorflow\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u3001 tensorflow (1.0.0RC0) \u3000\u304c\u898b\u3064\u304b\u308c\u3070Update\u3055\u308c\u3066\u3044\u308b\u3002 Docker\u306b\u4fdd\u5b58 \u6700\u5f8c\u306b\u3001TF1.0.0RC0\u306bUpdate\u3055\u308c\u305f\u72b6\u614b\u3067Docker\u3092\u4fdd\u5b58\u3059\u308b\u3002 \u518d\u3073\u3001 docker ps \u3067\u3001\u8d77\u52d5\u3057\u3066\u3044\u308bDocker\u30b3\u30f3\u30c6\u30ca\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u3002 1 $ docker ps \u30b3\u30f3\u30c6\u30caID\u3092\u30b3\u30d4\u30fc\u3057\u3001\u4e0b\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u3067\u5909\u66f4\u3092\u30b3\u30df\u30c3\u30c8\u3059\u308b 1 $ docker commit -m \"tensorflow version up\" 19627749df78 datalab_tf1 \u5f15\u6570 \u610f\u5473 -m \"tensorflow version up\" \u597d\u304d\u306a\u30b3\u30e1\u30f3\u30c8\u3092\u8a18\u8f09 19627749df78 docker ps \u3067\u78ba\u8a8d\u3057\u305f\u5404\u81ea\u306eCONTAINER ID datalab_tf1 \u4efb\u610f\u306e\u540d\u524d \u30b3\u30df\u30c3\u30c8\u304c\u7121\u4e8b\u6210\u529f\u3057\u305f\u3089 1 $ docker images \u3067\u597d\u304d\u306a\u540d\u524d\u3092\u3064\u3051\u305fimage\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b Datalab\u306e\u518d\u8d77\u52d5 DONTAINER ID\u3092\u6307\u5b9a\u3057\u3066\u3001\u8d77\u52d5\u4e2d\u306eDocker\u3092\u505c\u6b62\u3059\u308b\u3002 1 $ docker stop 19627749df78 [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u6307\u5b9a] 1 2 3 4 5 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ -e \"PROJECT_ID=\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\" \\ datalab_tf1 [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a] 1 2 3 4 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ datalab_tf1 Console\u3078\u30ed\u30b0\u30a4\u30f3 \u53d6\u5f97\u3057\u305fName\u3082\u3057\u304f\u306f\u3001\u30b3\u30f3\u30c6\u30caID\u3092\u6307\u5b9a\u3057\u3066\u3001\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3002 [\u30b3\u30f3\u30c6\u30caID\u3067\u8d77\u52d5] 1 $ docker exec -it \u30b3\u30f3\u30c6\u30caID /bin/bash [Name\u3067\u8d77\u52d5] 1 $ docker exec -it MAME /bin/bash \u6700\u5f8c\u306b\u3001 1 $ pip list \u3092\u3057\u3001 tensorflow (1.0.0RC0) \u3092\u78ba\u8a8d\u3067\u304d\u308c\u3070\u3001\u6b63\u5e38\u306bDocker Image\u304c\u751f\u6210\u3055\u308c\u305f\u4e8b\u306b\u306a\u308b\u3002","title":"Google Cloud DataLab\u306eTensorFlow\u3092Update"},{"location":"datalab/datalab_update_tf1/#google-cloud-datalabtensorflowupdate","text":"\uff08\u3053\u306e\u9805\u76ee\u306fTensorFlow r1.0\u30ea\u30ea\u30fc\u30b9\u524d\u306b\u3001\u6700\u65b0\u7248\u306eTensorFlow\u306b\u66f4\u65b0\u3057\u305f\u969b\u306e\u60c5\u5831\u3067\u3059\u3002\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u66f4\u65b0\u304c\u5fc5\u8981\u306a\u6642\u306b\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044) \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u5fc5\u8981\u306aTensorFlow\u306eVersion\u306f\u30011.0.0\u3067\u3059\u3002","title":"Google Cloud DataLab\u306eTensorFlow\u3092Update"},{"location":"datalab/datalab_update_tf1/#docker","text":"docker ps -a \u3067Docker\u30b3\u30f3\u30c6\u30ca\u306e\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u3002(\u8a73\u7d30\u306fman docker-ps) 1 $ docker ps -a Docker\u30b3\u30f3\u30c6\u30ca\u3068\u306f\uff1aDocker\u30a4\u30e1\u30fc\u30b8\u3092docker run\u3059\u308b\u3053\u3068\u3067\u4f5c\u3089\u308c\u308bDocker\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u3053\u3068\u3002docker\u304c\u5b9f\u884c\u3059\u308b\u30d7\u30ed\u30bb\u30b9\u30a4\u30e1\u30fc\u30b8\u3002docker stop/start\u3067\u505c\u6b62/\u8d77\u52d5\u304c\u53ef\u80fd\u3002","title":"Docker\u30b3\u30f3\u30c6\u30ca\u4e00\u89a7"},{"location":"datalab/datalab_update_tf1/#console","text":"\u53d6\u5f97\u3057\u305fName\u3082\u3057\u304f\u306f\u3001\u30b3\u30f3\u30c6\u30caID\u3092\u6307\u5b9a\u3057\u3066\u3001\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3002 [\u30b3\u30f3\u30c6\u30caID\u3067\u8d77\u52d5ID\u3067\u8d77\u52d5] 1 $ docker exec -it \u30b3\u30f3\u30c6\u30caID /bin/bash [Name\u3067\u8d77\u52d5] 1 $ docker exec -it MAME /bin/bash","title":"Console\u3078\u30ed\u30b0\u30a4\u30f3"},{"location":"datalab/datalab_update_tf1/#pythonversion","text":"Console\u306b\u30ed\u30b0\u30a4\u30f3\u3057\u305f\u3089Python\u306eVersion\u3092\u8abf\u3079\u308b\u3002 1 2 $ python -V Python 2 .7.9","title":"Python\u306eVersion"},{"location":"datalab/datalab_update_tf1/#tensorflow-100update","text":"Python 2.7\u7cfb\u3067\u3001TensorFlow 1.0.0\u306bUpdate\u3059\u308b\u3002 1 2 $ export TF_BINARY_URL = https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0rc0-cp27-none-linux_x86_64.whl $ pip install --ignore-installed --upgrade $TF_BINARY_URL \u307e\u305f\u306f 1 pip install tensorflow -U","title":"TensorFlow 1.0.0\u3078Update"},{"location":"datalab/datalab_update_tf1/#update","text":"\u4e00\u901a\u308a\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u304c\u7d42\u308f\u3063\u305f\u3089 1 $ pip list \u3067tensorflow\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u3001 tensorflow (1.0.0RC0) \u3000\u304c\u898b\u3064\u304b\u308c\u3070Update\u3055\u308c\u3066\u3044\u308b\u3002","title":"Update\u306e\u78ba\u8a8d"},{"location":"datalab/datalab_update_tf1/#docker_1","text":"\u6700\u5f8c\u306b\u3001TF1.0.0RC0\u306bUpdate\u3055\u308c\u305f\u72b6\u614b\u3067Docker\u3092\u4fdd\u5b58\u3059\u308b\u3002 \u518d\u3073\u3001 docker ps \u3067\u3001\u8d77\u52d5\u3057\u3066\u3044\u308bDocker\u30b3\u30f3\u30c6\u30ca\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u3002 1 $ docker ps \u30b3\u30f3\u30c6\u30caID\u3092\u30b3\u30d4\u30fc\u3057\u3001\u4e0b\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u3067\u5909\u66f4\u3092\u30b3\u30df\u30c3\u30c8\u3059\u308b 1 $ docker commit -m \"tensorflow version up\" 19627749df78 datalab_tf1 \u5f15\u6570 \u610f\u5473 -m \"tensorflow version up\" \u597d\u304d\u306a\u30b3\u30e1\u30f3\u30c8\u3092\u8a18\u8f09 19627749df78 docker ps \u3067\u78ba\u8a8d\u3057\u305f\u5404\u81ea\u306eCONTAINER ID datalab_tf1 \u4efb\u610f\u306e\u540d\u524d \u30b3\u30df\u30c3\u30c8\u304c\u7121\u4e8b\u6210\u529f\u3057\u305f\u3089 1 $ docker images \u3067\u597d\u304d\u306a\u540d\u524d\u3092\u3064\u3051\u305fimage\u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b","title":"Docker\u306b\u4fdd\u5b58"},{"location":"datalab/datalab_update_tf1/#datalab","text":"DONTAINER ID\u3092\u6307\u5b9a\u3057\u3066\u3001\u8d77\u52d5\u4e2d\u306eDocker\u3092\u505c\u6b62\u3059\u308b\u3002 1 $ docker stop 19627749df78 [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u6307\u5b9a] 1 2 3 4 5 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ -e \"PROJECT_ID=\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\" \\ datalab_tf1 [OS X\u3067\u5b9f\u884c:\u30d7\u30ed\u30b8\u30a7\u30af\u30c8ID\u3092\u672a\u6307\u5b9a] 1 2 3 4 $ cd ~ $ mkdir -p ./datalab $ docker run -it -p 127 .0.0.1:8081:8080 -p 6006 :6006 -v \" ${ HOME } /datalab:/content\" \\ datalab_tf1","title":"Datalab\u306e\u518d\u8d77\u52d5"},{"location":"datalab/datalab_update_tf1/#console_1","text":"\u53d6\u5f97\u3057\u305fName\u3082\u3057\u304f\u306f\u3001\u30b3\u30f3\u30c6\u30caID\u3092\u6307\u5b9a\u3057\u3066\u3001\u30ed\u30b0\u30a4\u30f3\u3059\u308b\u3002 [\u30b3\u30f3\u30c6\u30caID\u3067\u8d77\u52d5] 1 $ docker exec -it \u30b3\u30f3\u30c6\u30caID /bin/bash [Name\u3067\u8d77\u52d5] 1 $ docker exec -it MAME /bin/bash \u6700\u5f8c\u306b\u3001 1 $ pip list \u3092\u3057\u3001 tensorflow (1.0.0RC0) \u3092\u78ba\u8a8d\u3067\u304d\u308c\u3070\u3001\u6b63\u5e38\u306bDocker Image\u304c\u751f\u6210\u3055\u308c\u305f\u4e8b\u306b\u306a\u308b\u3002","title":"Console\u3078\u30ed\u30b0\u30a4\u30f3"},{"location":"datalab/python_update/","text":"Python2\u304b\u3089Python3.x\u7cfb\u3078\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8 pyenv\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b \u8d77\u52d5\u3057\u3066\u3044\u308bDocker\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30b7\u30a7\u30eb\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b NAME\u306b\u306f\u8d77\u52d5\u4e2d\u306e\u30b3\u30f3\u30c6\u30caID\u304bName\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 1 $ docker exec -it NAME /bin/bash \u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u6700\u521d\u306bapt-get\u3092\u66f4\u65b0\u3059\u308b 1 apt-get upgrade \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb 1 apt-get install git gcc make openssl libssl-dev libbz2-dev libreadline-dev libsqlite3-dev pyenv\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b 1 git clone git://github.com/yyuu/pyenv.git ~/.pyenv \u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b echo $SHELL \u3000\u3067\u4f7f\u7528\u3057\u3066\u3044\u308b\u30b7\u30a7\u30eb\u3092\u78ba\u8a8d\u3057\u3001\u305d\u306e\u30b7\u30a7\u30eb\u306b\u4ee5\u4e0b\u3092\u8ffd\u52a0\u3059\u308b \u4e0b\u306e\u4f8b\u3060\u3068 bash \u3092\u4f7f\u7528\u3057\u3066\u3044\u305f\u306e\u3067 vi ~/.bashrc \u304b vim ~/.bashrc \u306b\u3066\u8ffd\u52a0\u3057\u305f 1 2 3 export PYENV_ROOT = $HOME /.pyenv export PATH = $PYENV_ROOT /bin: $PATH eval \" $( pyenv init - ) \" \u8ffd\u52a0\u3057\u305f\u3089\u8aad\u307f\u8fbc\u307f\u3001which\u3067\u30d1\u30b9\u304c\u51fa\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b 1 2 source ~/.bashrc which pyenv Python\u306e\u5225\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pyenv install -l \u306b\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u53ef\u80fd\u306a\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3059\u308b \u6b32\u3057\u3044\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u3042\u3063\u305f\u3089\u305d\u308c\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b \u3053\u3053\u3067\u306f\u4f8b\u3068\u3057\u30663.6.1\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u308b 1 2 3 pyenv install 3 .6.1 # \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u304b\u3092\u78ba\u8a8d\u3059\u308b pyenv versions \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u7121\u4e8b\u5b8c\u4e86\u3057\u305f\u3089 pyenv global 3.6.1 \u3067\u4f7f\u7528\u3059\u308bpython\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5207\u308a\u66ff\u3048\u308b \u4ed6\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3089\u305d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 python -V \u3067\u6307\u5b9a\u3057\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u51fa\u529b\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b \u6700\u5f8c\u306b\u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u305f\u72b6\u614b\u3092Docker\u3092\u4fdd\u5b58\u3059\u308b\u3002 \u30ed\u30fc\u30ab\u30eb\u3067 docker ps \u3067\u3001\u8d77\u52d5\u3057\u3066\u3044\u308bDocker\u30b3\u30f3\u30c6\u30ca\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u3002 1 $ docker ps \u30b3\u30f3\u30c6\u30caID\u3092\u30b3\u30d4\u30fc\u3057\u3001\u4e0b\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u3067\u5909\u66f4\u3092\u30b3\u30df\u30c3\u30c8\u3059\u308b 1 $ docker commit -m \"update the python2 to python3\" 19627749df78 tensorflow_python3 \u5f15\u6570 \u610f\u5473 -m \"update the python2 to python3\" \u597d\u304d\u306a\u30b3\u30e1\u30f3\u30c8\u3092\u8a18\u8f09 19627749df78 docker ps \u3067\u78ba\u8a8d\u3057\u305f\u5404\u81ea\u306eCONTAINER ID tensorflow_python3 \u4efb\u610f\u306edocker\u30a4\u30e1\u30fc\u30b8\u306e\u540d\u524d","title":"Python2\u304b\u3089Python3.x\u7cfb\u3078\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8"},{"location":"datalab/python_update/#python2python3x","text":"pyenv\u3092\u4f7f\u7528\u3057\u3066\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b \u8d77\u52d5\u3057\u3066\u3044\u308bDocker\u306e\u30b3\u30f3\u30c6\u30ca\u306e\u30b7\u30a7\u30eb\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b NAME\u306b\u306f\u8d77\u52d5\u4e2d\u306e\u30b3\u30f3\u30c6\u30caID\u304bName\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 1 $ docker exec -it NAME /bin/bash","title":"Python2\u304b\u3089Python3.x\u7cfb\u3078\u306e\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8"},{"location":"datalab/python_update/#_1","text":"\u6700\u521d\u306bapt-get\u3092\u66f4\u65b0\u3059\u308b 1 apt-get upgrade \u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb 1 apt-get install git gcc make openssl libssl-dev libbz2-dev libreadline-dev libsqlite3-dev pyenv\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b 1 git clone git://github.com/yyuu/pyenv.git ~/.pyenv \u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b echo $SHELL \u3000\u3067\u4f7f\u7528\u3057\u3066\u3044\u308b\u30b7\u30a7\u30eb\u3092\u78ba\u8a8d\u3057\u3001\u305d\u306e\u30b7\u30a7\u30eb\u306b\u4ee5\u4e0b\u3092\u8ffd\u52a0\u3059\u308b \u4e0b\u306e\u4f8b\u3060\u3068 bash \u3092\u4f7f\u7528\u3057\u3066\u3044\u305f\u306e\u3067 vi ~/.bashrc \u304b vim ~/.bashrc \u306b\u3066\u8ffd\u52a0\u3057\u305f 1 2 3 export PYENV_ROOT = $HOME /.pyenv export PATH = $PYENV_ROOT /bin: $PATH eval \" $( pyenv init - ) \" \u8ffd\u52a0\u3057\u305f\u3089\u8aad\u307f\u8fbc\u307f\u3001which\u3067\u30d1\u30b9\u304c\u51fa\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b 1 2 source ~/.bashrc which pyenv","title":"\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"datalab/python_update/#python","text":"pyenv install -l \u306b\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u53ef\u80fd\u306a\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3059\u308b \u6b32\u3057\u3044\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u3042\u3063\u305f\u3089\u305d\u308c\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b \u3053\u3053\u3067\u306f\u4f8b\u3068\u3057\u30663.6.1\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u307f\u308b 1 2 3 pyenv install 3 .6.1 # \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u305f\u304b\u3092\u78ba\u8a8d\u3059\u308b pyenv versions \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u7121\u4e8b\u5b8c\u4e86\u3057\u305f\u3089 pyenv global 3.6.1 \u3067\u4f7f\u7528\u3059\u308bpython\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u5207\u308a\u66ff\u3048\u308b \u4ed6\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\u3089\u305d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 python -V \u3067\u6307\u5b9a\u3057\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u51fa\u529b\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b \u6700\u5f8c\u306b\u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3057\u305f\u72b6\u614b\u3092Docker\u3092\u4fdd\u5b58\u3059\u308b\u3002 \u30ed\u30fc\u30ab\u30eb\u3067 docker ps \u3067\u3001\u8d77\u52d5\u3057\u3066\u3044\u308bDocker\u30b3\u30f3\u30c6\u30ca\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\u3002 1 $ docker ps \u30b3\u30f3\u30c6\u30caID\u3092\u30b3\u30d4\u30fc\u3057\u3001\u4e0b\u8a18\u306e\u30b3\u30de\u30f3\u30c9\u3067\u5909\u66f4\u3092\u30b3\u30df\u30c3\u30c8\u3059\u308b 1 $ docker commit -m \"update the python2 to python3\" 19627749df78 tensorflow_python3 \u5f15\u6570 \u610f\u5473 -m \"update the python2 to python3\" \u597d\u304d\u306a\u30b3\u30e1\u30f3\u30c8\u3092\u8a18\u8f09 19627749df78 docker ps \u3067\u78ba\u8a8d\u3057\u305f\u5404\u81ea\u306eCONTAINER ID tensorflow_python3 \u4efb\u610f\u306edocker\u30a4\u30e1\u30fc\u30b8\u306e\u540d\u524d","title":"Python\u306e\u5225\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"develop-Jetson/","text":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9 [JetPack 3.1 \u74b0\u5883\u8a2d\u5b9a] CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 SPI\u6709\u52b9\u5316 Python 3.6 OpenCV 3.4.0 Tensorflow r1.6.0 [JetPack 3.2 \u74b0\u5883\u8a2d\u5b9a] CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 Python 3.6 OpenCV 3.4.1 Tensorflow r1.4.1","title":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9"},{"location":"develop-Jetson/#jetson-tx2","text":"","title":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9"},{"location":"develop-Jetson/#jetpack-31","text":"CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 SPI\u6709\u52b9\u5316 Python 3.6 OpenCV 3.4.0 Tensorflow r1.6.0","title":"[JetPack 3.1 \u74b0\u5883\u8a2d\u5b9a]"},{"location":"develop-Jetson/#jetpack-32","text":"CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 Python 3.6 OpenCV 3.4.1 Tensorflow r1.4.1","title":"[JetPack 3.2 \u74b0\u5883\u8a2d\u5b9a]"},{"location":"develop-Jetson/Jetson-TX2/JetpackforMac/","text":"Mac\u3067\u306eJetson Tx2 \u306b\u5bfe\u3059\u308bJetPack3.X \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5 Ubuntu\u3067\u4f5c\u696d\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001 Virtual Box , VMware \u306a\u3069\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 Virtual Box \u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u3067\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u308b\u3002 https://www.virtualbox.org/wiki/Downloads Virtual Box \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u7d42\u4e86\u3057\u305f\u3089\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u3067Ubuntu\u306eiso\u30a4\u30e1\u30fc\u30b8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u3002(ubuntu-ja-16.04-desktop-amd64.iso\u3067\u52d5\u304f\u3053\u3068\u3092\u78ba\u8a8d) http://www.ubuntulinux.jp/download/ja-remix iso\u30a4\u30e1\u30fc\u30b8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u304c\u7d42\u4e86\u3057\u305f\u3089 Virtual Box \u3092\u8d77\u52d5\u3057\u3001\u5de6\u4e0a\u306e \u65b0\u898f \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u8a2d\u5b9a\u3092\u884c\u3046 \u540d\u524d\u3068\u30e1\u30e2\u30ea\u30fc\u30b5\u30a4\u30ba\u306f\u4efb\u610f\u306e\u8a2d\u5b9a \u4f5c\u6210\u304c\u7d42\u4e86\u3057\u305f\u3089\u4f5c\u6210\u3057\u305f\u3082\u306e\u3092\u8d77\u52d5\u3057\u3001\u305d\u306e\u969b\u306b\u4e8b\u524d\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305fiso\u30a4\u30e1\u30fc\u30b8\u3092\u8a2d\u5b9a\u3057\u3066 Start \u30dc\u30bf\u30f3\u3092\u62bc\u3059\u3002 Ubuntu\u304c\u8d77\u52d5\u3059\u308b\u306e\u3067\u3001\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u884c\u3046\u3002 \u4f5c\u6210\u3057\u305f\u4eee\u60f3\u30de\u30b7\u30f3\u306e \u8a2d\u5b9a \u3092\u958b\u304d\u3001 \u30dd\u30fc\u30c8 > USB \u306b\u79fb\u52d5\u3059\u308b\u3002 USB\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u30fc\u3092\u6709\u52b9\u5316 \u306b\u304a\u3044\u3066\u3001 USB2.0 \u306e\u30c1\u30a7\u30c3\u30af\u3092 USB3.0 \u306b\u5fc5\u305a\u5909\u66f4\u3059\u308b\u3002 \u5f8c\u306f\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u306b\u5f93\u3044\u3001JetPack\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u884c\u3046\u3002 http://docs.nvidia.com/jetpack-l4t/index.html#developertools/mobile/jetpack/l4t/3.1/jetpack_l4t_install.htm","title":"JetpackforMac"},{"location":"develop-Jetson/Jetson-TX2/JetpackforMac/#macjetson-tx2-jetpack3x","text":"Ubuntu\u3067\u4f5c\u696d\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\u3001 Virtual Box , VMware \u306a\u3069\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 Virtual Box \u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u306f\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u3067\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u308b\u3002 https://www.virtualbox.org/wiki/Downloads Virtual Box \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u7d42\u4e86\u3057\u305f\u3089\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u3067Ubuntu\u306eiso\u30a4\u30e1\u30fc\u30b8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u3002(ubuntu-ja-16.04-desktop-amd64.iso\u3067\u52d5\u304f\u3053\u3068\u3092\u78ba\u8a8d) http://www.ubuntulinux.jp/download/ja-remix iso\u30a4\u30e1\u30fc\u30b8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u304c\u7d42\u4e86\u3057\u305f\u3089 Virtual Box \u3092\u8d77\u52d5\u3057\u3001\u5de6\u4e0a\u306e \u65b0\u898f \u30dc\u30bf\u30f3\u3092\u30af\u30ea\u30c3\u30af\u3057\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u8a2d\u5b9a\u3092\u884c\u3046 \u540d\u524d\u3068\u30e1\u30e2\u30ea\u30fc\u30b5\u30a4\u30ba\u306f\u4efb\u610f\u306e\u8a2d\u5b9a \u4f5c\u6210\u304c\u7d42\u4e86\u3057\u305f\u3089\u4f5c\u6210\u3057\u305f\u3082\u306e\u3092\u8d77\u52d5\u3057\u3001\u305d\u306e\u969b\u306b\u4e8b\u524d\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305fiso\u30a4\u30e1\u30fc\u30b8\u3092\u8a2d\u5b9a\u3057\u3066 Start \u30dc\u30bf\u30f3\u3092\u62bc\u3059\u3002 Ubuntu\u304c\u8d77\u52d5\u3059\u308b\u306e\u3067\u3001\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3092\u884c\u3046\u3002 \u4f5c\u6210\u3057\u305f\u4eee\u60f3\u30de\u30b7\u30f3\u306e \u8a2d\u5b9a \u3092\u958b\u304d\u3001 \u30dd\u30fc\u30c8 > USB \u306b\u79fb\u52d5\u3059\u308b\u3002 USB\u30b3\u30f3\u30c8\u30ed\u30fc\u30e9\u30fc\u3092\u6709\u52b9\u5316 \u306b\u304a\u3044\u3066\u3001 USB2.0 \u306e\u30c1\u30a7\u30c3\u30af\u3092 USB3.0 \u306b\u5fc5\u305a\u5909\u66f4\u3059\u308b\u3002 \u5f8c\u306f\u4e0b\u8a18\u306e\u30b5\u30a4\u30c8\u306b\u5f93\u3044\u3001JetPack\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3092\u884c\u3046\u3002 http://docs.nvidia.com/jetpack-l4t/index.html#developertools/mobile/jetpack/l4t/3.1/jetpack_l4t_install.htm","title":"Mac\u3067\u306eJetson Tx2 \u306b\u5bfe\u3059\u308bJetPack3.X \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/","text":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9 Jetson TX2 JetPack 3.1 \u74b0\u5883\u69cb\u7bc9\u65b9\u6cd5 # After JetPack 3.1 installation # ssh login to TX2 ssh ubuntu@192.168.x.x sudo su cd setup_scripts ./setup.sh #wait reboot sudo su cd install_scripts ./install.sh #wait 2 hours \u5185\u5bb9 \u521d\u671f\u8a2d\u5b9a (setup_scripts/setup.sh) CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 (setup_cpufun.sh) Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 (setup_update.sh) .bashrc\u66f8\u304d\u63db\u3048 (setup_bash.sh) .dircolors\u8ffd\u52a0 (setup_dircolors.sh) SPIDev\u6709\u52b9\u5316 (setup_spi.sh) reboot TensorFlow r1.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_scripts/install.sh) Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 Python 3.6.3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_python3.6.sh) pip3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_pip3.sh) jupyter \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_jupyter.sh) Java8 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_java8.sh) Build Tools \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_build_tools.sh) CUDA deviceQuery \u30d3\u30eb\u30c9 (install_cuda_deviceQuery.sh) patch\u3092\u5f53\u3066\u308b (cv_patch.sh) OpenCV 3.3.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_opencv-3.3.1.sh) bazel 0.5.4 \u30d3\u30eb\u30c9 (build_bazel-0.5.4.sh) TensorFlow r1.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_tensorflow-r1.4.1.sh) TensorFlow c++ r1.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_tensorflow-cpp-r1.4.1.sh) \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 OpenCV 3.3.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_opencv-3.3.1.sh) TensorFlow r1.4.1 pip\u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_tensorflow-r1.4.1.sh) TensorFlow c++ api r1.4.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_tensorflow-cpp-api-r1.4.1.sh) Jupyter \u8d77\u52d5\u65b9\u6cd5 install_scripts/install_jupyter.sh\u3067TX2\u8d77\u52d5\u6642\u306b\u81ea\u52d5\u8d77\u52d5\u3059\u308b\u3088\u3046\u306b\u8a2d\u5b9a\u3057\u3066\u3042\u308b\u3002 \u521d\u671f\u30d1\u30b9\u30ef\u30fc\u30c9\u306fmypassword /etc/init.d/jupyterd env PASSWORD=mypassword jupyter notebook --allow-root --NotebookApp.iopub_data_rate_limit=10000000 Jupyter \u30a2\u30af\u30bb\u30b9\u65b9\u6cd5 \u30d6\u30e9\u30a6\u30b6\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3002 \u30d1\u30b9\u30ef\u30fc\u30c9\u306f\u8d77\u52d5\u6642\u306b\u74b0\u5883\u5909\u6570\u306b\u6307\u5b9a\u3057\u305fmypassword http://IP\u30a2\u30c9\u30ec\u30b9:8888/ SPI\u52d5\u4f5c\u78ba\u8a8d Fabo \u306e #104 Angle Brick \u3068#509 OUT/IN Shield for JETSON TX2 beta3\u3067\u78ba\u8a8d\u3059\u308b\u3002 spi-sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # coding: utf-8 # pip3 install spidev import spidev import time import sys spi = spidev . SpiDev () # open: Connects the object to the specified SPI device. open(X,Y) will open /dev/spidev-X.Y bus = 3 device = 0 spi . open ( bus , device ) # SPI Settings # max_speed_hz: Property that gets / sets the maximum bus speed in Hz. # mode: Property that gets /sets the SPI mode as two bit pattern of Clock Polarity and Phase [CPOL|CPHA]. Range:0b00..0b11 (0..3) spi . max_speed_hz = 5000 spi . mode = 0b01 # A0\u30b3\u30cd\u30af\u30bf\u306b\u6a5f\u5668\u3092\u63a5\u7d9a SPI_PIN_A0 = 0 #read SPI from the ADC(MCP3008 chip), 8 possible chanels def readadc ( channel ): \"\"\" Analog Data Converter\u306e\u5024\u3092\u8aad\u307f\u8fbc\u3080 @channel \u30c1\u30e3\u30f3\u30cd\u30eb\u756a\u53f7 \"\"\" #Writes a list of values to SPI device. #bits_per_word: Property that gets / sets the bits per word. #xfer2(list of values[, speed_hz, delay_usec, bits_per_word]) speed_hz = 1 delay_usec = ( 8 + channel ) << 4 bits_per_word = 0 to_send = [ speed_hz , delay_usec , bits_per_word ] adc = spi . xfer2 ( to_send ) data = (( adc [ 1 ] & 3 ) << 8 ) + adc [ 2 ] return data def map ( x , in_min , in_max , out_min , out_max ): \"\"\" map\u95a2\u6570 @x \u5909\u63db\u3057\u305f\u3044\u5024 @in_min \u5909\u63db\u524d\u306e\u6700\u5c0f\u5024 @in_max \u5909\u63db\u524d\u306e\u6700\u5927\u5024 @out_min \u5909\u63db\u5f8c\u306e\u6700\u5c0f @out_max \u5909\u63db\u5f8c\u306e\u6700\u5927\u5024 @return \u5909\u63db\u3055\u308c\u305f\u5024 \"\"\" return ( x - in_min ) * ( out_max - out_min ) // ( in_max - in_min ) + out_min try : while True : data = readadc ( SPI_PIN_A0 ) value = map ( data , 0 , 1023 , 0 , 100 ) print ( \"adc : {:8} \" . format ( data )) time . sleep ( 0.1 ) except KeyboardInterrupt : # close: Disconnects the object from the interface spi . close () sys . exit ( 0 ) python spi-sample.py \u8b70\u8ad6 TX2: \u30e1\u30e2\u30ea\u304c\u8c4a\u5bcc\u3067\u306f\u306a\u3044\u306e\u3067\u5b66\u7fd2\u306b\u306f\u5411\u304b\u306a\u3044 TX2: GPU\u3057\u304b\u4f7f\u308f\u306a\u3044\u306e\u3067SWAP\u306f\u8981\u3089\u306a\u3044 TensorFlow: \u30e1\u30e2\u30ea\u6d88\u8cbb\u6291\u5236\u3001\u30a8\u30e9\u30fc\u56de\u907f\u306e\u305f\u3081\u306bJEMALLOC,CUDA\u306e\u307f\u6709\u52b9 TensorFlow: MKL\u306fIntel\u306a\u306e\u3067ARM\u306eTX2\u3067\u306f\u4f7f\u308f\u306a\u3044 TX2: Denver\u30b3\u30a2\u306fOpenCV\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3059\u308b\u306e\u3067\u4f7f\u308f\u306a\u3044 TX2: \u30d1\u30c3\u30b1\u30fc\u30b8\u306fARM64\u3067\u4f5c\u6210\u3059\u308b TX2: JetPack 3.1\u306b\u306fCUDA\u30e9\u30a4\u30d6\u30e9\u30ea\u30d0\u30fc\u30b8\u30e7\u30f38.0.84\u3068nvcc\u30d0\u30fc\u30b8\u30e7\u30f38.0.72\u3067\u30d1\u30c3\u30c1\u30ec\u30d9\u30eb\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u7570\u306a\u308b\u305f\u3081\u3001TF_CUDA_VERSION\u306f8.0\u307e\u3067\u306e\u6307\u5b9a\u3068\u3059\u308b TensorFlow: \u30e2\u30c7\u30eb\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3092\u884c\u3046\u305f\u3081\u306b\u306f\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u30d3\u30eb\u30c9\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 build_tensorflow-r1.4.1.sh","title":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#jetson-tx2","text":"","title":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#jetson-tx2-jetpack-31","text":"# After JetPack 3.1 installation # ssh login to TX2 ssh ubuntu@192.168.x.x sudo su cd setup_scripts ./setup.sh #wait reboot sudo su cd install_scripts ./install.sh #wait 2 hours","title":"Jetson TX2 JetPack 3.1 \u74b0\u5883\u69cb\u7bc9\u65b9\u6cd5"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#_1","text":"","title":"\u5185\u5bb9"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#setup_scriptssetupsh","text":"CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 (setup_cpufun.sh) Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 (setup_update.sh) .bashrc\u66f8\u304d\u63db\u3048 (setup_bash.sh) .dircolors\u8ffd\u52a0 (setup_dircolors.sh) SPIDev\u6709\u52b9\u5316 (setup_spi.sh) reboot","title":"\u521d\u671f\u8a2d\u5b9a (setup_scripts/setup.sh)"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#tensorflow-r141-install_scriptsinstallsh","text":"Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 Python 3.6.3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_python3.6.sh) pip3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_pip3.sh) jupyter \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_jupyter.sh) Java8 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_java8.sh) Build Tools \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(install_build_tools.sh) CUDA deviceQuery \u30d3\u30eb\u30c9 (install_cuda_deviceQuery.sh) patch\u3092\u5f53\u3066\u308b (cv_patch.sh) OpenCV 3.3.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_opencv-3.3.1.sh) bazel 0.5.4 \u30d3\u30eb\u30c9 (build_bazel-0.5.4.sh) TensorFlow r1.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_tensorflow-r1.4.1.sh) TensorFlow c++ r1.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_tensorflow-cpp-r1.4.1.sh)","title":"TensorFlow r1.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_scripts/install.sh)"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#_2","text":"OpenCV 3.3.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_opencv-3.3.1.sh) TensorFlow r1.4.1 pip\u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_tensorflow-r1.4.1.sh) TensorFlow c++ api r1.4.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_tensorflow-cpp-api-r1.4.1.sh)","title":"\u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#jupyter","text":"install_scripts/install_jupyter.sh\u3067TX2\u8d77\u52d5\u6642\u306b\u81ea\u52d5\u8d77\u52d5\u3059\u308b\u3088\u3046\u306b\u8a2d\u5b9a\u3057\u3066\u3042\u308b\u3002 \u521d\u671f\u30d1\u30b9\u30ef\u30fc\u30c9\u306fmypassword","title":"Jupyter \u8d77\u52d5\u65b9\u6cd5"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#etcinitdjupyterd","text":"env PASSWORD=mypassword jupyter notebook --allow-root --NotebookApp.iopub_data_rate_limit=10000000","title":"/etc/init.d/jupyterd"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#jupyter_1","text":"\u30d6\u30e9\u30a6\u30b6\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3002 \u30d1\u30b9\u30ef\u30fc\u30c9\u306f\u8d77\u52d5\u6642\u306b\u74b0\u5883\u5909\u6570\u306b\u6307\u5b9a\u3057\u305fmypassword http://IP\u30a2\u30c9\u30ec\u30b9:8888/","title":"Jupyter \u30a2\u30af\u30bb\u30b9\u65b9\u6cd5"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#spi","text":"Fabo \u306e #104 Angle Brick \u3068#509 OUT/IN Shield for JETSON TX2 beta3\u3067\u78ba\u8a8d\u3059\u308b\u3002 spi-sample.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # coding: utf-8 # pip3 install spidev import spidev import time import sys spi = spidev . SpiDev () # open: Connects the object to the specified SPI device. open(X,Y) will open /dev/spidev-X.Y bus = 3 device = 0 spi . open ( bus , device ) # SPI Settings # max_speed_hz: Property that gets / sets the maximum bus speed in Hz. # mode: Property that gets /sets the SPI mode as two bit pattern of Clock Polarity and Phase [CPOL|CPHA]. Range:0b00..0b11 (0..3) spi . max_speed_hz = 5000 spi . mode = 0b01 # A0\u30b3\u30cd\u30af\u30bf\u306b\u6a5f\u5668\u3092\u63a5\u7d9a SPI_PIN_A0 = 0 #read SPI from the ADC(MCP3008 chip), 8 possible chanels def readadc ( channel ): \"\"\" Analog Data Converter\u306e\u5024\u3092\u8aad\u307f\u8fbc\u3080 @channel \u30c1\u30e3\u30f3\u30cd\u30eb\u756a\u53f7 \"\"\" #Writes a list of values to SPI device. #bits_per_word: Property that gets / sets the bits per word. #xfer2(list of values[, speed_hz, delay_usec, bits_per_word]) speed_hz = 1 delay_usec = ( 8 + channel ) << 4 bits_per_word = 0 to_send = [ speed_hz , delay_usec , bits_per_word ] adc = spi . xfer2 ( to_send ) data = (( adc [ 1 ] & 3 ) << 8 ) + adc [ 2 ] return data def map ( x , in_min , in_max , out_min , out_max ): \"\"\" map\u95a2\u6570 @x \u5909\u63db\u3057\u305f\u3044\u5024 @in_min \u5909\u63db\u524d\u306e\u6700\u5c0f\u5024 @in_max \u5909\u63db\u524d\u306e\u6700\u5927\u5024 @out_min \u5909\u63db\u5f8c\u306e\u6700\u5c0f @out_max \u5909\u63db\u5f8c\u306e\u6700\u5927\u5024 @return \u5909\u63db\u3055\u308c\u305f\u5024 \"\"\" return ( x - in_min ) * ( out_max - out_min ) // ( in_max - in_min ) + out_min try : while True : data = readadc ( SPI_PIN_A0 ) value = map ( data , 0 , 1023 , 0 , 100 ) print ( \"adc : {:8} \" . format ( data )) time . sleep ( 0.1 ) except KeyboardInterrupt : # close: Disconnects the object from the interface spi . close () sys . exit ( 0 ) python spi-sample.py","title":"SPI\u52d5\u4f5c\u78ba\u8a8d"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/#_3","text":"TX2: \u30e1\u30e2\u30ea\u304c\u8c4a\u5bcc\u3067\u306f\u306a\u3044\u306e\u3067\u5b66\u7fd2\u306b\u306f\u5411\u304b\u306a\u3044 TX2: GPU\u3057\u304b\u4f7f\u308f\u306a\u3044\u306e\u3067SWAP\u306f\u8981\u3089\u306a\u3044 TensorFlow: \u30e1\u30e2\u30ea\u6d88\u8cbb\u6291\u5236\u3001\u30a8\u30e9\u30fc\u56de\u907f\u306e\u305f\u3081\u306bJEMALLOC,CUDA\u306e\u307f\u6709\u52b9 TensorFlow: MKL\u306fIntel\u306a\u306e\u3067ARM\u306eTX2\u3067\u306f\u4f7f\u308f\u306a\u3044 TX2: Denver\u30b3\u30a2\u306fOpenCV\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3059\u308b\u306e\u3067\u4f7f\u308f\u306a\u3044 TX2: \u30d1\u30c3\u30b1\u30fc\u30b8\u306fARM64\u3067\u4f5c\u6210\u3059\u308b TX2: JetPack 3.1\u306b\u306fCUDA\u30e9\u30a4\u30d6\u30e9\u30ea\u30d0\u30fc\u30b8\u30e7\u30f38.0.84\u3068nvcc\u30d0\u30fc\u30b8\u30e7\u30f38.0.72\u3067\u30d1\u30c3\u30c1\u30ec\u30d9\u30eb\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u7570\u306a\u308b\u305f\u3081\u3001TF_CUDA_VERSION\u306f8.0\u307e\u3067\u306e\u6307\u5b9a\u3068\u3059\u308b TensorFlow: \u30e2\u30c7\u30eb\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3092\u884c\u3046\u305f\u3081\u306b\u306f\u3001\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u30d3\u30eb\u30c9\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 build_tensorflow-r1.4.1.sh","title":"\u8b70\u8ad6"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/","text":"Jetson TX2 SPIdev\u3092\u6709\u52b9\u5316 \u30e1\u30e2 Jetson TX2\u306ftegra186 Kernel Module compile DTB decompile DTS\u7de8\u96c6 DTB compile JetPack 3.1: /dev/mmcblk0p15\u306b\u66f8\u304d\u8fbc\u3080 reboot spi@3240000\u306fspi@c260000\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u540c\u4e00PIN\u306b\u306a\u308b\u3002 check cmmands 1 2 3 4 lsmod ls /proc/device-tree/ | grep spi cat /proc/modules ls /dev/spi* Install SPIdev module [*1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 wget --no-check-certificate https://developer.nvidia.com/embedded/dlc/l4t-sources-28-1 -O sources_r28.1.tbz2 tar - xvf sources_r28 . 1 . tbz2 cd sources tar - xvf kernel_src - tx2 . tbz2 cd kernel / kernel - 4 . 4 zcat / proc / config . gz > . config # less . config\u3067\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u3066\u304b\u3089\u5b9f\u884c\u3059\u308b\u3053\u3068 sed - i 's/# CONFIG_SPI_SPIDEV is not set/CONFIG_SPI_SPIDEV=m/' . config sed - i 's/CONFIG_LOCALVERSION=\"\"/CONFIG_LOCALVERSION=\"-tegra\"/' . config make prepare make modules_prepare make M = drivers / spi / cp drivers / spi / spidev . ko / lib / modules / $ ( uname - r ) / kernel / drivers depmod # reboot \u5f8c\u3067\u307e\u3068\u3081\u3066\u5b9f\u884c Device tree \u30e1\u30e2 1 2 3 4 5 6 # /boot/dtb/tegra186-quill-p3310-1000-c03-00-base.dtb \u3092\u30c7\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b # tegra186-quill-p3310-1000-c03-00-base.dts \u30d5\u30a1\u30a4\u30eb\u304c\u51fa\u6765\u4e0a\u304c\u308b #spi0 = \"/spi@3210000\"; #spi1 = \"/spi@c260000\"; #spi2 = \"/spi@3230000\"; #spi3 = \"/spi@3240000\"; DTC Tool [*2] 1 2 apt-get update apt-get install device-tree-compiler SPI Configuration [*3] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 cd / boot / dtb # backup cp tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb . bak # decompile dtc - I dtb - O dts - o tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dts tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb # edit tegra186-quill-p3310-1000-c03-00-base.dts # spi@3240000 \u306b\u66f8\u304d\u52a0\u3048\u308b # The SPI pin group in the J21 looks like map to TX2 SPI4, You may need enable the spidev node to spi@3240000 [*3] vi tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dts spi @3240000 { compatible = \"nvidia,tegra186-spi\" ; reg = < 0x0 0x3240000 0x0 0x10000 > ; interrupts = < 0x0 0x27 0x4 > ; nvidia , dma - request - selector = < 0x19 0x12 > ; #address-cells = <0x1>; #size-cells = <0x0>; #stream-id-cells = <0x1>; dmas = < 0x19 0x12 0x19 0x12 > ; dma - names = \"rx\" , \"tx\" ; nvidia , clk - parents = \"pll_p\" , \"clk_m\" ; clocks = < 0xd 0x4a 0xd 0x10d 0xd 0x261 > ; clock - names = \"spi\" , \"pll_p\" , \"clk_m\" ; resets = < 0xd 0x2b > ; reset - names = \"spi\" ; status = \"okay\" ; linux , phandle = < 0x80 > ; phandle = < 0x80 > ; # add this spidev @0 { compatible = \"spidev\" ; reg = < 0 > ; spi - max - frequency =< 25000000 > ; }; spidev @1 { compatible = \"spidev\" ; reg = < 1 > ; spi - max - frequency =< 25000000 > ; }; }; # compile dtc - I dts - O dtb - o tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dts dtb into /dev/mmcblk0p15 [*4] 1 dd if=/boot/dtb/tegra186-quill-p3310-1000-c03-00-base.dtb of=/dev/mmcblk0p15 reboot 1 reboot check spidev 1 lsmod Module Size Used by fuse 89008 2 bcmdhd 7625819 0 spidev 10966 0 \u2190\u3053\u308c\u304c\u51fa\u73fe\u3057\u305f pci_tegra 74691 0 bluedroid_pm 13564 0 1 ls /dev/spi* /dev/spidev3.0 /dev/spidev3.1 \u53c2\u8003 [*1] https://elinux.org/Jetson/TX1_SPI#Installing_SPIdev_Kernel_Module [*2] https://elinux.org/Jetson/TX1_SPI#Installing_DTC_Tool [*3] https://devtalk.nvidia.com/default/topic/1008929/jetson-tx2/enabling-spi-and-spidev-on-the-tx2/ [*4] https://devtalk.nvidia.com/default/topic/1023007/how-to-use-uart0-as-normal-uart-port-on-r28-1-/?offset=12","title":"Jetson TX2 SPIdev\u3092\u6709\u52b9\u5316"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#jetson-tx2-spidev","text":"","title":"Jetson TX2 SPIdev\u3092\u6709\u52b9\u5316"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#_1","text":"Jetson TX2\u306ftegra186 Kernel Module compile DTB decompile DTS\u7de8\u96c6 DTB compile JetPack 3.1: /dev/mmcblk0p15\u306b\u66f8\u304d\u8fbc\u3080 reboot spi@3240000\u306fspi@c260000\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u540c\u4e00PIN\u306b\u306a\u308b\u3002 check cmmands 1 2 3 4 lsmod ls /proc/device-tree/ | grep spi cat /proc/modules ls /dev/spi*","title":"\u30e1\u30e2"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#install-spidev-module-1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 wget --no-check-certificate https://developer.nvidia.com/embedded/dlc/l4t-sources-28-1 -O sources_r28.1.tbz2 tar - xvf sources_r28 . 1 . tbz2 cd sources tar - xvf kernel_src - tx2 . tbz2 cd kernel / kernel - 4 . 4 zcat / proc / config . gz > . config # less . config\u3067\u5185\u5bb9\u3092\u78ba\u8a8d\u3057\u3066\u304b\u3089\u5b9f\u884c\u3059\u308b\u3053\u3068 sed - i 's/# CONFIG_SPI_SPIDEV is not set/CONFIG_SPI_SPIDEV=m/' . config sed - i 's/CONFIG_LOCALVERSION=\"\"/CONFIG_LOCALVERSION=\"-tegra\"/' . config make prepare make modules_prepare make M = drivers / spi / cp drivers / spi / spidev . ko / lib / modules / $ ( uname - r ) / kernel / drivers depmod # reboot \u5f8c\u3067\u307e\u3068\u3081\u3066\u5b9f\u884c","title":"Install SPIdev module [*1]"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#device-tree","text":"\u30e1\u30e2 1 2 3 4 5 6 # /boot/dtb/tegra186-quill-p3310-1000-c03-00-base.dtb \u3092\u30c7\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b # tegra186-quill-p3310-1000-c03-00-base.dts \u30d5\u30a1\u30a4\u30eb\u304c\u51fa\u6765\u4e0a\u304c\u308b #spi0 = \"/spi@3210000\"; #spi1 = \"/spi@c260000\"; #spi2 = \"/spi@3230000\"; #spi3 = \"/spi@3240000\";","title":"Device tree"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#dtc-tool-2","text":"1 2 apt-get update apt-get install device-tree-compiler","title":"DTC Tool [*2]"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#spi-configuration-3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 cd / boot / dtb # backup cp tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb . bak # decompile dtc - I dtb - O dts - o tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dts tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb # edit tegra186-quill-p3310-1000-c03-00-base.dts # spi@3240000 \u306b\u66f8\u304d\u52a0\u3048\u308b # The SPI pin group in the J21 looks like map to TX2 SPI4, You may need enable the spidev node to spi@3240000 [*3] vi tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dts spi @3240000 { compatible = \"nvidia,tegra186-spi\" ; reg = < 0x0 0x3240000 0x0 0x10000 > ; interrupts = < 0x0 0x27 0x4 > ; nvidia , dma - request - selector = < 0x19 0x12 > ; #address-cells = <0x1>; #size-cells = <0x0>; #stream-id-cells = <0x1>; dmas = < 0x19 0x12 0x19 0x12 > ; dma - names = \"rx\" , \"tx\" ; nvidia , clk - parents = \"pll_p\" , \"clk_m\" ; clocks = < 0xd 0x4a 0xd 0x10d 0xd 0x261 > ; clock - names = \"spi\" , \"pll_p\" , \"clk_m\" ; resets = < 0xd 0x2b > ; reset - names = \"spi\" ; status = \"okay\" ; linux , phandle = < 0x80 > ; phandle = < 0x80 > ; # add this spidev @0 { compatible = \"spidev\" ; reg = < 0 > ; spi - max - frequency =< 25000000 > ; }; spidev @1 { compatible = \"spidev\" ; reg = < 1 > ; spi - max - frequency =< 25000000 > ; }; }; # compile dtc - I dts - O dtb - o tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dtb tegra186 - quill - p3310 - 1000 - c03 - 00 - base . dts","title":"SPI Configuration [*3]"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#dtb-into-devmmcblk0p15-4","text":"1 dd if=/boot/dtb/tegra186-quill-p3310-1000-c03-00-base.dtb of=/dev/mmcblk0p15","title":"dtb into /dev/mmcblk0p15 [*4]"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#reboot","text":"1 reboot","title":"reboot"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#check-spidev","text":"1 lsmod Module Size Used by fuse 89008 2 bcmdhd 7625819 0 spidev 10966 0 \u2190\u3053\u308c\u304c\u51fa\u73fe\u3057\u305f pci_tegra 74691 0 bluedroid_pm 13564 0 1 ls /dev/spi* /dev/spidev3.0 /dev/spidev3.1","title":"check spidev"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.1/with_python3.6_custom_packages/spidev_jetpack3.1/#_2","text":"[*1] https://elinux.org/Jetson/TX1_SPI#Installing_SPIdev_Kernel_Module [*2] https://elinux.org/Jetson/TX1_SPI#Installing_DTC_Tool [*3] https://devtalk.nvidia.com/default/topic/1008929/jetson-tx2/enabling-spi-and-spidev-on-the-tx2/ [*4] https://devtalk.nvidia.com/default/topic/1023007/how-to-use-uart0-as-normal-uart-port-on-r28-1-/?offset=12","title":"\u53c2\u8003"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/","text":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9 Jetson TX2 JetPack 3.2 \u74b0\u5883\u69cb\u7bc9\u65b9\u6cd5 # After JetPack 3.2 installation # ssh login to TX2 ssh ubuntu@192.168.x.x sudo su cd setup_scripts ./setup.sh #wait reboot sudo su cd install_scripts ./install.sh #wait hours \u5185\u5bb9 \u521d\u671f\u8a2d\u5b9a (setup_scripts/setup.sh) CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 (setup_cpufun.sh) Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 (setup_update.sh) .bashrc\u66f8\u304d\u63db\u3048 (setup_bash.sh) .dircolors\u8ffd\u52a0 (setup_dircolors.sh) reboot TensorFlow r1.6.0 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_scripts/install.sh) Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 Python 3.6.3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_python3.6.sh) pip3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_pip3.sh) jupyter \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_jupyter.sh) Java8 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_java8.sh) Build Tools \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_build_tools.sh) CUDA deviceQuery \u30d3\u30eb\u30c9 (install_cuda_deviceQuery.sh) OpenCV\u7528\u306bCUDA\u30d8\u30c3\u30c0\u30fc\u30d1\u30c3\u30c1\u9069\u7528 (cv_patch.sh) OpenCV 3.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_opencv-3.4.1.sh) bazel 0.10.0 \u30d3\u30eb\u30c9 (build_bazel-0.10.0.sh) TensorFlow r1.6.0 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_tensorflow-r1.6.0.sh) \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 OpenCV 3.4.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_opencv-3.4.1.sh) OpenMPI 3.4.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_openmpi-3.4.1.sh) TensorFlow r1.6.0 pip\u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_tensorflow-r1.6.0.sh) Jupyter \u8d77\u52d5\u65b9\u6cd5 install_scripts/install_jupyter.sh\u3067TX2\u8d77\u52d5\u6642\u306b\u81ea\u52d5\u8d77\u52d5\u3059\u308b\u3088\u3046\u306b\u8a2d\u5b9a\u3057\u3066\u3042\u308b\u3002 \u521d\u671f\u30d1\u30b9\u30ef\u30fc\u30c9\u306fmypassword /etc/init.d/jupyterd env PASSWORD=mypassword jupyter notebook --allow-root --NotebookApp.iopub_data_rate_limit=10000000 Jupyter \u30a2\u30af\u30bb\u30b9\u65b9\u6cd5 \u30d6\u30e9\u30a6\u30b6\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3002 \u30d1\u30b9\u30ef\u30fc\u30c9\u306f\u8d77\u52d5\u6642\u306b\u74b0\u5883\u5909\u6570\u306b\u6307\u5b9a\u3057\u305fmypassword http://IP\u30a2\u30c9\u30ec\u30b9:8888/ \u8b70\u8ad6 TX2: \u30e1\u30e2\u30ea\u304c\u8c4a\u5bcc\u3067\u306f\u306a\u3044\u306e\u3067\u5b66\u7fd2\u306b\u306f\u5411\u304b\u306a\u3044 TX2: GPU\u3057\u304b\u4f7f\u308f\u306a\u3044\u306e\u3067SWAP\u306f\u8981\u3089\u306a\u3044 TensorFlow: \u30e1\u30e2\u30ea\u6d88\u8cbb\u6291\u5236\u3001\u30a8\u30e9\u30fc\u56de\u907f\u306e\u305f\u3081\u306bJEMALLOC,CUDA\u306e\u307f\u6709\u52b9 TensorFlow: MKL\u306fIntel\u306a\u306e\u3067ARM\u306eTX2\u3067\u306f\u4f7f\u308f\u306a\u3044 TX2: Denver\u30b3\u30a2\u306fOpenCV\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3059\u308b\u306e\u3067\u4f7f\u308f\u306a\u3044 TX2: \u30d1\u30c3\u30b1\u30fc\u30b8\u306fARM64\u3067\u4f5c\u6210\u3059\u308b TensorFlow: XLA\u3092\u7121\u52b9 <- \u6709\u52b9\u3060\u3068JetPack 3.2\u3067\u306fObject Detection\u306eobject_detection_tutorial.ipynb\u3092Jupyter\u3067\u5b9f\u884c\u3059\u308b\u3068The kernel appears to have died. It will restart automatically.\u3067\u843d\u3061\u308b\u3002\u7121\u52b9\u3060\u3068\u5b9f\u884c\u3067\u304d\u308b\u3002 TX2: JetPack 3.1\u306fCUDA 8.0.84, nvcc 8.0.72\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u9055\u3046 TX2: JetPack 3.2\u306fCUDA 9.0.252, nvcc 9.0.252\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u4e00\u81f4 TX2: \u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u95a2\u9023\u306fJetPack 3.1\u304c\u5b89\u5b9a TX2: JetPack 3.2\u306f\u307e\u3060\u65b0\u3057\u3044\u305f\u3081\u30ab\u30b9\u30bf\u30e0\u30dc\u30fc\u30c9\u7528\u30ab\u30fc\u30cd\u30eb\u975e\u5bfe\u5fdc","title":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#jetson-tx2","text":"","title":"Jetson TX2 \u74b0\u5883\u69cb\u7bc9"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#jetson-tx2-jetpack-32","text":"# After JetPack 3.2 installation # ssh login to TX2 ssh ubuntu@192.168.x.x sudo su cd setup_scripts ./setup.sh #wait reboot sudo su cd install_scripts ./install.sh #wait hours","title":"Jetson TX2 JetPack 3.2 \u74b0\u5883\u69cb\u7bc9\u65b9\u6cd5"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#_1","text":"","title":"\u5185\u5bb9"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#setup_scriptssetupsh","text":"CPU\u30d5\u30a1\u30f3\u81ea\u52d5\u8d77\u52d5 (setup_cpufun.sh) Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 (setup_update.sh) .bashrc\u66f8\u304d\u63db\u3048 (setup_bash.sh) .dircolors\u8ffd\u52a0 (setup_dircolors.sh) reboot","title":"\u521d\u671f\u8a2d\u5b9a (setup_scripts/setup.sh)"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#tensorflow-r160-install_scriptsinstallsh","text":"Ubuntu 16.04 \u30d1\u30c3\u30b1\u30fc\u30b8\u66f4\u65b0 Python 3.6.3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_python3.6.sh) pip3 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_pip3.sh) jupyter \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_jupyter.sh) Java8 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_java8.sh) Build Tools \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_build_tools.sh) CUDA deviceQuery \u30d3\u30eb\u30c9 (install_cuda_deviceQuery.sh) OpenCV\u7528\u306bCUDA\u30d8\u30c3\u30c0\u30fc\u30d1\u30c3\u30c1\u9069\u7528 (cv_patch.sh) OpenCV 3.4.1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_opencv-3.4.1.sh) bazel 0.10.0 \u30d3\u30eb\u30c9 (build_bazel-0.10.0.sh) TensorFlow r1.6.0 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_tensorflow-r1.6.0.sh)","title":"TensorFlow r1.6.0 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb (install_scripts/install.sh)"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#_2","text":"OpenCV 3.4.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_opencv-3.4.1.sh) OpenMPI 3.4.1 \u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_openmpi-3.4.1.sh) TensorFlow r1.6.0 pip\u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210 (build_tensorflow-r1.6.0.sh)","title":"\u30d1\u30c3\u30b1\u30fc\u30b8\u4f5c\u6210"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#jupyter","text":"install_scripts/install_jupyter.sh\u3067TX2\u8d77\u52d5\u6642\u306b\u81ea\u52d5\u8d77\u52d5\u3059\u308b\u3088\u3046\u306b\u8a2d\u5b9a\u3057\u3066\u3042\u308b\u3002 \u521d\u671f\u30d1\u30b9\u30ef\u30fc\u30c9\u306fmypassword","title":"Jupyter \u8d77\u52d5\u65b9\u6cd5"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#etcinitdjupyterd","text":"env PASSWORD=mypassword jupyter notebook --allow-root --NotebookApp.iopub_data_rate_limit=10000000","title":"/etc/init.d/jupyterd"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#jupyter_1","text":"\u30d6\u30e9\u30a6\u30b6\u3067\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3002 \u30d1\u30b9\u30ef\u30fc\u30c9\u306f\u8d77\u52d5\u6642\u306b\u74b0\u5883\u5909\u6570\u306b\u6307\u5b9a\u3057\u305fmypassword http://IP\u30a2\u30c9\u30ec\u30b9:8888/","title":"Jupyter \u30a2\u30af\u30bb\u30b9\u65b9\u6cd5"},{"location":"develop-Jetson/Jetson-TX2/JetPack3.2/#_3","text":"TX2: \u30e1\u30e2\u30ea\u304c\u8c4a\u5bcc\u3067\u306f\u306a\u3044\u306e\u3067\u5b66\u7fd2\u306b\u306f\u5411\u304b\u306a\u3044 TX2: GPU\u3057\u304b\u4f7f\u308f\u306a\u3044\u306e\u3067SWAP\u306f\u8981\u3089\u306a\u3044 TensorFlow: \u30e1\u30e2\u30ea\u6d88\u8cbb\u6291\u5236\u3001\u30a8\u30e9\u30fc\u56de\u907f\u306e\u305f\u3081\u306bJEMALLOC,CUDA\u306e\u307f\u6709\u52b9 TensorFlow: MKL\u306fIntel\u306a\u306e\u3067ARM\u306eTX2\u3067\u306f\u4f7f\u308f\u306a\u3044 TX2: Denver\u30b3\u30a2\u306fOpenCV\u30d3\u30eb\u30c9\u306b\u5931\u6557\u3059\u308b\u306e\u3067\u4f7f\u308f\u306a\u3044 TX2: \u30d1\u30c3\u30b1\u30fc\u30b8\u306fARM64\u3067\u4f5c\u6210\u3059\u308b TensorFlow: XLA\u3092\u7121\u52b9 <- \u6709\u52b9\u3060\u3068JetPack 3.2\u3067\u306fObject Detection\u306eobject_detection_tutorial.ipynb\u3092Jupyter\u3067\u5b9f\u884c\u3059\u308b\u3068The kernel appears to have died. It will restart automatically.\u3067\u843d\u3061\u308b\u3002\u7121\u52b9\u3060\u3068\u5b9f\u884c\u3067\u304d\u308b\u3002 TX2: JetPack 3.1\u306fCUDA 8.0.84, nvcc 8.0.72\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u9055\u3046 TX2: JetPack 3.2\u306fCUDA 9.0.252, nvcc 9.0.252\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u304c\u4e00\u81f4 TX2: \u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u95a2\u9023\u306fJetPack 3.1\u304c\u5b89\u5b9a TX2: JetPack 3.2\u306f\u307e\u3060\u65b0\u3057\u3044\u305f\u3081\u30ab\u30b9\u30bf\u30e0\u30dc\u30fc\u30c9\u7528\u30ab\u30fc\u30cd\u30eb\u975e\u5bfe\u5fdc","title":"\u8b70\u8ad6"},{"location":"develop-ubuntu/","text":"TensorFlow Ubuntu \u958b\u767a\u74b0\u5883 Ubuntu 16.04 LTS - AWS EC2 AWS EC2 P3/P2 Instance \u4ee5\u4e0b\u53e4\u3044\u60c5\u5831 Ubuntu 16.04 LTS - TensorFlow r1.2 GPU/XLA/MKL Python/C++ TensorFlow r1.2\u304b\u3089Intel Math Kernel Library\u304c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002AWS p2\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306fMKL\u306b\u5bfe\u5fdc\u3057\u305fCPU\u306a\u306e\u3067\u3001MKL\u3092\u6709\u52b9\u306b\u3057\u305fTensorFlow\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002 * AWS EC2 p2.8xlarge Docker + git source compile (GPU) Ubuntu 16.04 LTS - TensorFlow r1.1.0 GPU/XLA Python/C++ TensorFlow r1.1.0\u306fpip install --upgrade tensorflow\u3067\u666e\u901a\u306b\u4f7f\u3048\u308b\u306e\u3067\u3059\u304c\u3001C++\u3067\u3082\u5b9f\u884c\u3057\u305f\u3044\u306e\u3067\u30bd\u30fc\u30b9\u304b\u3089\u30d3\u30eb\u30c9\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002 * AWS EC2 p2.xlarge Docker + git source compile (GPU) Ubuntu 16.04 LTS - TensorFlow r1.0.1 TensorFlow r1.0\u306b\u306a\u3063\u3066\u304b\u3089pip install tensorflow\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u51fa\u6765\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u306e\u3067\u3001Docker\u306fjupyter\u3092\u4f7f\u3044\u305f\u3044\u4eba\u3084\u8907\u6570\u306e\u74b0\u5883\u3092\u5207\u308a\u66ff\u3048\u305f\u3044\u65b9\u5411\u3051\u3002(GPU\u7248\u306ftensorflow-gpu) Docker command AWS EC2 c4.large Docker (CPU) AWS EC2 p2.xlarge Docker (GPU) VM Docker (CPU) VM Docker Google Cloud Datalab (CPU) VM git source compile (CPU) - r1.0.0-rc2","title":"TensorFlow Ubuntu \u958b\u767a\u74b0\u5883"},{"location":"develop-ubuntu/#tensorflow-ubuntu","text":"","title":"TensorFlow Ubuntu \u958b\u767a\u74b0\u5883"},{"location":"develop-ubuntu/#ubuntu-1604-lts-aws-ec2","text":"AWS EC2 P3/P2 Instance","title":"Ubuntu 16.04 LTS - AWS EC2"},{"location":"develop-ubuntu/#_1","text":"","title":"\u4ee5\u4e0b\u53e4\u3044\u60c5\u5831"},{"location":"develop-ubuntu/#ubuntu-1604-lts-tensorflow-r12-gpuxlamkl-pythonc","text":"TensorFlow r1.2\u304b\u3089Intel Math Kernel Library\u304c\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002AWS p2\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306fMKL\u306b\u5bfe\u5fdc\u3057\u305fCPU\u306a\u306e\u3067\u3001MKL\u3092\u6709\u52b9\u306b\u3057\u305fTensorFlow\u3092\u30d3\u30eb\u30c9\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002 * AWS EC2 p2.8xlarge Docker + git source compile (GPU)","title":"Ubuntu 16.04 LTS - TensorFlow r1.2 GPU/XLA/MKL Python/C++"},{"location":"develop-ubuntu/#ubuntu-1604-lts-tensorflow-r110-gpuxla-pythonc","text":"TensorFlow r1.1.0\u306fpip install --upgrade tensorflow\u3067\u666e\u901a\u306b\u4f7f\u3048\u308b\u306e\u3067\u3059\u304c\u3001C++\u3067\u3082\u5b9f\u884c\u3057\u305f\u3044\u306e\u3067\u30bd\u30fc\u30b9\u304b\u3089\u30d3\u30eb\u30c9\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002 * AWS EC2 p2.xlarge Docker + git source compile (GPU)","title":"Ubuntu 16.04 LTS - TensorFlow r1.1.0 GPU/XLA Python/C++"},{"location":"develop-ubuntu/#ubuntu-1604-lts-tensorflow-r101","text":"TensorFlow r1.0\u306b\u306a\u3063\u3066\u304b\u3089pip install tensorflow\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u51fa\u6765\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u306e\u3067\u3001Docker\u306fjupyter\u3092\u4f7f\u3044\u305f\u3044\u4eba\u3084\u8907\u6570\u306e\u74b0\u5883\u3092\u5207\u308a\u66ff\u3048\u305f\u3044\u65b9\u5411\u3051\u3002(GPU\u7248\u306ftensorflow-gpu) Docker command AWS EC2 c4.large Docker (CPU) AWS EC2 p2.xlarge Docker (GPU) VM Docker (CPU) VM Docker Google Cloud Datalab (CPU) VM git source compile (CPU) - r1.0.0-rc2","title":"Ubuntu 16.04 LTS - TensorFlow r1.0.1"},{"location":"develop-ubuntu/docker-command/","text":"Docker Command\u306b\u3064\u3044\u3066 \u5bfe\u8c61OS:Ubuntu\u3002\u3059\u3079\u3066root\u3067\u5b9f\u884c\u3002 docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # Docker install # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world \u5229\u7528\u3067\u304d\u308bdocker\u30a4\u30e1\u30fc\u30b8\u3092\u691c\u7d22 1 2 3 4 ######################################## # docker\u30a4\u30e1\u30fc\u30b8\u3092\u691c\u7d22 ######################################## docker search tensorflow Docker\u30a4\u30e1\u30fc\u30b8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 1 2 3 4 5 6 7 8 ######################################## # TensorFlow Docker image download ######################################## # https://hub.docker.com/r/tensorflow/tensorflow/ # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md # \u516c\u5f0f\u7248docker: tensorflow/tensorflow # google cloud\u304c\u51fa\u3057\u3066\u3044\u308bdocker: gcr.io/tensorflow/tensorflow docker pull tensorflow/tensorflow Docker\u30b3\u30f3\u30c6\u30ca\u3092\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u4f5c\u6210\u3057\u8d77\u52d5\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ######################################## # TensorFlow Docker launch from image ( make new docker container = process ) ######################################## # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs # Tensorflow\u516c\u5f0fDocker\u306eport\u756a\u53f7 # jupyter port : 8888 , tensorboard port : 6006 docker run - itd - v / home / ubuntu / notebooks : / notebooks \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" docker ps - a # jupyter\u3078\u306e\u30a2\u30af\u30bb\u30b9\u65b9\u6cd5 : \u30d6\u30e9\u30a6\u30b6\u3067 http : // localhost : 8888 # password\u306f\u8d77\u52d5\u6642\u306b\u8a2d\u5b9a\u3057\u305fmypassword # pull\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u304b\u3089\u8d77\u52d5\u307e\u3067\u5b9f\u884c\u306b\u306a\u308b # tensorflow / tensorflow\u306fTensorFlow\u516c\u5f0fCPU\u7248Docker # \u81ea\u5206\u3067 pip install\u7b49\u3092\u3057\u305f\u5834\u5408\u306f\u5225\u30ec\u30dd\u30b8\u30c8\u30ea\u306bcommit\u3057\u3066\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3057\u3066\u4f7f\u3046 Docker\u30b3\u30f3\u30c6\u30ca\u3092container_id\u304b\u3089\u8d77\u52d5\u3059\u308b container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5 ######################################## docker ps -a docker start 437620a48fb7 \u8d77\u52d5\u4e2d\u306eDocker\u30b3\u30f3\u30c6\u30ca\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3059\u308b container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3 ######################################## docker ps -a docker exec -it 437620a48fb7 /bin/bash Docker\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3059\u308b container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62 ######################################## docker ps -a docker stop 437620a48fb7 Docker\u30b3\u30f3\u30c6\u30ca\u3092\u524a\u9664\u3059\u308b container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u3092\u524a\u9664 ######################################## docker ps -a docker rm 437620a48fb7 Docker\u30a4\u30e1\u30fc\u30b8\u3092\u524a\u9664\u3059\u308b image_id:cd88548da3c5 1 2 3 4 5 ######################################## # Docker\u30a4\u30e1\u30fc\u30b8\u3092\u524a\u9664 ######################################## docker images docker rmi cd88548da3c5 Docker\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b Docker\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057\u3066\u304a\u304f\u3053\u3068 container_id:437620a48fb7 1 2 3 4 5 6 ######################################## # Docker\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210 ######################################## docker ps -a docker commit 437620a48fb7 cpu/tensorflow # \u30b3\u30f3\u30c6\u30ca\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3057\u3066pip install\u7b49\u3092\u884c\u3063\u305f\u30b3\u30f3\u30c6\u30ca\u306f\u3001OS\u3092\u518d\u8d77\u52d5\u3057\u3066\u3082\u6d88\u3048\u306a\u3044\u3051\u308c\u3069\u3082\u30a4\u30e1\u30fc\u30b8\u3068\u3057\u3066\u4f5c\u6210\u3057\u3066\u304a\u304f\u3068\u3088\u3044 Docker\u30a4\u30e1\u30fc\u30b8\u3092\u6700\u65b0\u306b\u66f4\u65b0\u3059\u308b pip install\u7b49\u306f\u3084\u308a\u76f4\u3057\u306b\u306a\u308b\u304c\u3001\u81ea\u5206\u3067\u66f8\u3044\u305f\u30b3\u30fc\u30c9\u306fdocker run -itd -v /home/ubuntu/notebooks:/notebooks\u3067\u6307\u5b9a\u3057\u305f/home/utuntu/notebooks/\u4ee5\u4e0b\u306b\u4fdd\u5b58\u3057\u3066\u3042\u308b 1 2 3 4 ######################################## # Docker\u30a4\u30e1\u30fc\u30b8\u6700\u65b0\u306b\u66f4\u65b0 ######################################## docker images | cut -d ' ' -f1 | tail -n +2 | sort | uniq | egrep -v '^(<none>|ubuntu)$' | xargs -P0 -L1 sudo docker pull","title":"Docker Command\u306b\u3064\u3044\u3066"},{"location":"develop-ubuntu/docker-command/#docker-command","text":"\u5bfe\u8c61OS:Ubuntu\u3002\u3059\u3079\u3066root\u3067\u5b9f\u884c\u3002","title":"Docker Command\u306b\u3064\u3044\u3066"},{"location":"develop-ubuntu/docker-command/#docker","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # Docker install # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world","title":"docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002"},{"location":"develop-ubuntu/docker-command/#docker_1","text":"1 2 3 4 ######################################## # docker\u30a4\u30e1\u30fc\u30b8\u3092\u691c\u7d22 ######################################## docker search tensorflow","title":"\u5229\u7528\u3067\u304d\u308bdocker\u30a4\u30e1\u30fc\u30b8\u3092\u691c\u7d22"},{"location":"develop-ubuntu/docker-command/#docker_2","text":"1 2 3 4 5 6 7 8 ######################################## # TensorFlow Docker image download ######################################## # https://hub.docker.com/r/tensorflow/tensorflow/ # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md # \u516c\u5f0f\u7248docker: tensorflow/tensorflow # google cloud\u304c\u51fa\u3057\u3066\u3044\u308bdocker: gcr.io/tensorflow/tensorflow docker pull tensorflow/tensorflow","title":"Docker\u30a4\u30e1\u30fc\u30b8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"},{"location":"develop-ubuntu/docker-command/#docker_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ######################################## # TensorFlow Docker launch from image ( make new docker container = process ) ######################################## # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs # Tensorflow\u516c\u5f0fDocker\u306eport\u756a\u53f7 # jupyter port : 8888 , tensorboard port : 6006 docker run - itd - v / home / ubuntu / notebooks : / notebooks \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" docker ps - a # jupyter\u3078\u306e\u30a2\u30af\u30bb\u30b9\u65b9\u6cd5 : \u30d6\u30e9\u30a6\u30b6\u3067 http : // localhost : 8888 # password\u306f\u8d77\u52d5\u6642\u306b\u8a2d\u5b9a\u3057\u305fmypassword # pull\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u304b\u3089\u8d77\u52d5\u307e\u3067\u5b9f\u884c\u306b\u306a\u308b # tensorflow / tensorflow\u306fTensorFlow\u516c\u5f0fCPU\u7248Docker # \u81ea\u5206\u3067 pip install\u7b49\u3092\u3057\u305f\u5834\u5408\u306f\u5225\u30ec\u30dd\u30b8\u30c8\u30ea\u306bcommit\u3057\u3066\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3057\u3066\u4f7f\u3046","title":"Docker\u30b3\u30f3\u30c6\u30ca\u3092\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u4f5c\u6210\u3057\u8d77\u52d5\u3059\u308b"},{"location":"develop-ubuntu/docker-command/#dockercontainer_id","text":"container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5 ######################################## docker ps -a docker start 437620a48fb7","title":"Docker\u30b3\u30f3\u30c6\u30ca\u3092container_id\u304b\u3089\u8d77\u52d5\u3059\u308b"},{"location":"develop-ubuntu/docker-command/#dockerbash","text":"container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3 ######################################## docker ps -a docker exec -it 437620a48fb7 /bin/bash","title":"\u8d77\u52d5\u4e2d\u306eDocker\u30b3\u30f3\u30c6\u30ca\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3059\u308b"},{"location":"develop-ubuntu/docker-command/#docker_4","text":"container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62 ######################################## docker ps -a docker stop 437620a48fb7","title":"Docker\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3059\u308b"},{"location":"develop-ubuntu/docker-command/#docker_5","text":"container_id:437620a48fb7 1 2 3 4 5 ######################################## # Docker\u30b3\u30f3\u30c6\u30ca\u3092\u524a\u9664 ######################################## docker ps -a docker rm 437620a48fb7","title":"Docker\u30b3\u30f3\u30c6\u30ca\u3092\u524a\u9664\u3059\u308b"},{"location":"develop-ubuntu/docker-command/#docker_6","text":"image_id:cd88548da3c5 1 2 3 4 5 ######################################## # Docker\u30a4\u30e1\u30fc\u30b8\u3092\u524a\u9664 ######################################## docker images docker rmi cd88548da3c5","title":"Docker\u30a4\u30e1\u30fc\u30b8\u3092\u524a\u9664\u3059\u308b"},{"location":"develop-ubuntu/docker-command/#docker_7","text":"Docker\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057\u3066\u304a\u304f\u3053\u3068 container_id:437620a48fb7 1 2 3 4 5 6 ######################################## # Docker\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210 ######################################## docker ps -a docker commit 437620a48fb7 cpu/tensorflow # \u30b3\u30f3\u30c6\u30ca\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3057\u3066pip install\u7b49\u3092\u884c\u3063\u305f\u30b3\u30f3\u30c6\u30ca\u306f\u3001OS\u3092\u518d\u8d77\u52d5\u3057\u3066\u3082\u6d88\u3048\u306a\u3044\u3051\u308c\u3069\u3082\u30a4\u30e1\u30fc\u30b8\u3068\u3057\u3066\u4f5c\u6210\u3057\u3066\u304a\u304f\u3068\u3088\u3044","title":"Docker\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b"},{"location":"develop-ubuntu/docker-command/#docker_8","text":"pip install\u7b49\u306f\u3084\u308a\u76f4\u3057\u306b\u306a\u308b\u304c\u3001\u81ea\u5206\u3067\u66f8\u3044\u305f\u30b3\u30fc\u30c9\u306fdocker run -itd -v /home/ubuntu/notebooks:/notebooks\u3067\u6307\u5b9a\u3057\u305f/home/utuntu/notebooks/\u4ee5\u4e0b\u306b\u4fdd\u5b58\u3057\u3066\u3042\u308b 1 2 3 4 ######################################## # Docker\u30a4\u30e1\u30fc\u30b8\u6700\u65b0\u306b\u66f4\u65b0 ######################################## docker images | cut -d ' ' -f1 | tail -n +2 | sort | uniq | egrep -v '^(<none>|ubuntu)$' | xargs -P0 -L1 sudo docker pull","title":"Docker\u30a4\u30e1\u30fc\u30b8\u3092\u6700\u65b0\u306b\u66f4\u65b0\u3059\u308b"},{"location":"develop-ubuntu/aws-ec2/","text":"\u958b\u767a\u74b0\u5883 AWS p3.2xlarge/p2.xlarge Ubuntu 16.04 LTS Docker image - TensorFlow r1.4.1 AWS EC2 p3.2xlarge/p2.xlarge docker (GPU) AWS Ubuntu 16.04 LTS - Base instance settings AWS EC2 base instance settings","title":"\u958b\u767a\u74b0\u5883 AWS p3.2xlarge/p2.xlarge Ubuntu 16.04 LTS"},{"location":"develop-ubuntu/aws-ec2/#aws-p32xlargep2xlarge-ubuntu-1604-lts","text":"","title":"\u958b\u767a\u74b0\u5883 AWS p3.2xlarge/p2.xlarge Ubuntu 16.04 LTS"},{"location":"develop-ubuntu/aws-ec2/#docker-image-tensorflow-r141","text":"AWS EC2 p3.2xlarge/p2.xlarge docker (GPU)","title":"Docker image - TensorFlow r1.4.1"},{"location":"develop-ubuntu/aws-ec2/#aws-ubuntu-1604-lts-base-instance-settings","text":"AWS EC2 base instance settings","title":"AWS Ubuntu 16.04 LTS - Base instance settings"},{"location":"develop-ubuntu/aws-ec2/aws-instance/","text":"AWS INSTANCE OS INFO # AWS Ubuntu 16.04 64bit # \u666e\u901a\u306eUbuntu\u3092\u4f7f\u3046 # Ubuntu Server 16.04 LTS (HVM), SSD Volume Type - ami-aa2ea6d0 # \uff08\u4eca\u6642\u306eAWS\u306fDeep Learning AMI(Ubuntu/Amazon Linux)\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\uff09 OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c\u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b # Package configuration\u306finstall the package maintainer's version\u3092\u9078\u629e\u3057\u305f apt-get update apt-get upgrade -y apt-get dist-upgrade -y # https://docs.docker.com/engine/installation/linux/ubuntu/ - curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt-get install -y curl linux-image-extra-$(uname -r) linux-image-extra-virtual \u8a8d\u8a3c\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt-get install -y apt-transport-https ca-certificates docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl -fsSL https://yum.dockerproject.org/gpg | sudo apt-key add - docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt-key fingerprint 58118E89F3A912897C070ADBF76221572C52609D docker stable repository\u3092\u767b\u9332\u3059\u308b add-apt-repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt-get update docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt-get install -y docker-engine docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8\u3092\u78ba\u8a8d\u3059\u308b apt-cache madison docker-engine docker\u52d5\u4f5c\u78ba\u8a8d docker run --rm hello-world Setup Nvidia Drivers on EC2 Instance Host # https://github.com/fluxcapacitor/pipeline/wiki/AWS-GPU-TensorFlow-Docker apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub sh -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' apt-get update && apt-get install -y --no-install-recommends cuda-drivers NVIDIA docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b for Ubuntu 16.04 LTS # https://github.com/NVIDIA/nvidia-docker/ # Install nvidia-docker and nvidia-docker-plugin wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb dpkg -i /tmp/nvidia-docker .deb && rm /tmp/nvidia-docker .deb Test nvidia-smi nvidia-docker run --rm nvidia/cuda nvidia-smi \u4e0d\u8981\u306b\u306a\u3063\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u524a\u9664 apt autoremove -y reboot reboot # uninstall nvidia docker #apt-get purge -y nvidia #apt-get purge -y cuda","title":"AWS INSTANCE OS INFO"},{"location":"develop-ubuntu/aws-ec2/aws-instance/#aws-instance-os-info","text":"# AWS Ubuntu 16.04 64bit # \u666e\u901a\u306eUbuntu\u3092\u4f7f\u3046 # Ubuntu Server 16.04 LTS (HVM), SSD Volume Type - ami-aa2ea6d0 # \uff08\u4eca\u6642\u306eAWS\u306fDeep Learning AMI(Ubuntu/Amazon Linux)\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\uff09 OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c\u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b # Package configuration\u306finstall the package maintainer's version\u3092\u9078\u629e\u3057\u305f apt-get update apt-get upgrade -y apt-get dist-upgrade -y # https://docs.docker.com/engine/installation/linux/ubuntu/ - curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt-get install -y curl linux-image-extra-$(uname -r) linux-image-extra-virtual \u8a8d\u8a3c\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt-get install -y apt-transport-https ca-certificates docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl -fsSL https://yum.dockerproject.org/gpg | sudo apt-key add - docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt-key fingerprint 58118E89F3A912897C070ADBF76221572C52609D docker stable repository\u3092\u767b\u9332\u3059\u308b add-apt-repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt-get update docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt-get install -y docker-engine docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8\u3092\u78ba\u8a8d\u3059\u308b apt-cache madison docker-engine docker\u52d5\u4f5c\u78ba\u8a8d docker run --rm hello-world Setup Nvidia Drivers on EC2 Instance Host # https://github.com/fluxcapacitor/pipeline/wiki/AWS-GPU-TensorFlow-Docker apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub sh -c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' apt-get update && apt-get install -y --no-install-recommends cuda-drivers NVIDIA docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b for Ubuntu 16.04 LTS # https://github.com/NVIDIA/nvidia-docker/ # Install nvidia-docker and nvidia-docker-plugin wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb dpkg -i /tmp/nvidia-docker .deb && rm /tmp/nvidia-docker .deb Test nvidia-smi nvidia-docker run --rm nvidia/cuda nvidia-smi \u4e0d\u8981\u306b\u306a\u3063\u305f\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u524a\u9664 apt autoremove -y reboot reboot # uninstall nvidia docker #apt-get purge -y nvidia #apt-get purge -y cuda","title":"AWS INSTANCE OS INFO"},{"location":"develop-ubuntu/aws-ec2/aws-tensorflow-gpu-r1.4.1/","text":"LAUNCH make $HOME/notebook directory mkdir /home/ubuntu/notebooks pull docker image nvidia-docker pull naisy/aws-tensorflow-gpu-x86_64:r1.4.1 make docker container and start. (change mypassword for jupyter login) nvidia-docker run -itd -v /home/ubuntu/notebooks:/notebooks -e \"PASSWORD=mypassword\" -p 6006:6006 -p 8888:8888 naisy/aws-tensorflow-gpu-x86_64:r1.4.1 /bin/bash -c \"jupyter notebook --allow-root --NotebookApp.iopub_data_rate_limit=10000000\" NEED jupyter setting, edit for YOUR DOMAIN nvidia-docker ps -a nvidia-docker exec -it CONTAINER_ID /bin/bash vi /root/.jupyter/jupyter_notebook_config.py c.NotebookApp.allow_origin='YOUR DOMAIN:8888' SEE ALSO: naisy/aws-tensorflow-gpu-x86_64 (Docker Hub)","title":"LAUNCH"},{"location":"develop-ubuntu/aws-ec2/aws-tensorflow-gpu-r1.4.1/#launch","text":"make $HOME/notebook directory mkdir /home/ubuntu/notebooks pull docker image nvidia-docker pull naisy/aws-tensorflow-gpu-x86_64:r1.4.1 make docker container and start. (change mypassword for jupyter login) nvidia-docker run -itd -v /home/ubuntu/notebooks:/notebooks -e \"PASSWORD=mypassword\" -p 6006:6006 -p 8888:8888 naisy/aws-tensorflow-gpu-x86_64:r1.4.1 /bin/bash -c \"jupyter notebook --allow-root --NotebookApp.iopub_data_rate_limit=10000000\"","title":"LAUNCH"},{"location":"develop-ubuntu/aws-ec2/aws-tensorflow-gpu-r1.4.1/#need","text":"jupyter setting, edit for YOUR DOMAIN nvidia-docker ps -a nvidia-docker exec -it CONTAINER_ID /bin/bash vi /root/.jupyter/jupyter_notebook_config.py c.NotebookApp.allow_origin='YOUR DOMAIN:8888'","title":"NEED"},{"location":"develop-ubuntu/aws-ec2/aws-tensorflow-gpu-r1.4.1/#see-also","text":"naisy/aws-tensorflow-gpu-x86_64 (Docker Hub)","title":"SEE ALSO:"},{"location":"develop-ubuntu/r1.0.1/aws-ec2-docker-cpu/","text":"AWS Ubuntu 16.04 1 2 3 4 5 6 7 8 9 kernel\u306f root @ip - 172 - 21 - 2 - 7 : / home / ubuntu # uname - a Linux ip - 172 - 21 - 2 - 7 4.4.0 - 64 - generic #85 - Ubuntu SMP Mon Feb 20 11 : 50 : 30 UTC 2017 x86_64 x86_64 x86_64 GNU / Linux # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001 \u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade reboot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ######################################## # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248 docker : tensorflow / tensorflow docker search tensorflow docker pull tensorflow / tensorflow docker run - it - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow # Blocking Cross Origin WebSocket Attempt . \u304c\u51fa\u3066 kernel\u306b\u63a5\u7d9a\u3067\u304d\u306a\u3044\u72b6\u614b\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u3092\u9632\u3050\u305f\u3081\u306b \u3001 Origin\u901a\u904e\u8a31\u53ef\u3092\u8ffd\u52a0\u3059\u308b apt - get update apt - get vim vi / root / . jupyter / jupyter_notebook_config . py c . NotebookApp . allow_origin = \"http://YOURDOMAIN:8888\" # \u4e00\u5ea6 docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057 \u3001 cpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop 000 e24300884 docker commit 000 e24300884 cpu / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 cpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" # jupyter\u306f http : // localhost : 8888 # tensorboard\u306f http : // localhost : 6006 1 2 3 4 5 6 7 8 ######################################## # pip\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u308b ######################################## pip install pandas pip install seaborn pip install requests # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U","title":"AWS Ubuntu 16.04"},{"location":"develop-ubuntu/r1.0.1/aws-ec2-docker-cpu/#aws-ubuntu-1604","text":"1 2 3 4 5 6 7 8 9 kernel\u306f root @ip - 172 - 21 - 2 - 7 : / home / ubuntu # uname - a Linux ip - 172 - 21 - 2 - 7 4.4.0 - 64 - generic #85 - Ubuntu SMP Mon Feb 20 11 : 50 : 30 UTC 2017 x86_64 x86_64 x86_64 GNU / Linux # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001 \u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade reboot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ######################################## # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248 docker : tensorflow / tensorflow docker search tensorflow docker pull tensorflow / tensorflow docker run - it - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow # Blocking Cross Origin WebSocket Attempt . \u304c\u51fa\u3066 kernel\u306b\u63a5\u7d9a\u3067\u304d\u306a\u3044\u72b6\u614b\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u3092\u9632\u3050\u305f\u3081\u306b \u3001 Origin\u901a\u904e\u8a31\u53ef\u3092\u8ffd\u52a0\u3059\u308b apt - get update apt - get vim vi / root / . jupyter / jupyter_notebook_config . py c . NotebookApp . allow_origin = \"http://YOURDOMAIN:8888\" # \u4e00\u5ea6 docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057 \u3001 cpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop 000 e24300884 docker commit 000 e24300884 cpu / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 cpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" # jupyter\u306f http : // localhost : 8888 # tensorboard\u306f http : // localhost : 6006 1 2 3 4 5 6 7 8 ######################################## # pip\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u308b ######################################## pip install pandas pip install seaborn pip install requests # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U","title":"AWS Ubuntu 16.04"},{"location":"develop-ubuntu/r1.0.1/aws-ec2-docker-gpu/","text":"AWS Ubuntu 16.04 1 2 3 4 5 6 7 8 9 10 AWS Ubuntu 16.04 kernel\u306f root @ip - 172 - 21 - 2 - 7 : / home / ubuntu # uname - a Linux ip - 172 - 21 - 2 - 7 4.4.0 - 64 - generic #85 - Ubuntu SMP Mon Feb 20 11 : 50 : 30 UTC 2017 x86_64 x86_64 x86_64 GNU / Linux # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001 \u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade reboot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 ######################################## # Setup Nvidia Drivers on EC2 Instance Host ######################################## # https : // github . com / fluxcapacitor / pipeline / wiki / AWS - GPU - TensorFlow - Docker apt - key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub sh - c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' apt - get update && apt - get install - y --no-install-recommends cuda-drivers 1 2 3 4 5 6 7 8 9 10 ######################################## # NVIDIA docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b ######################################## # https : // github . com / NVIDIA / nvidia - docker / # Install nvidia - docker and nvidia - docker - plugin wget - P / tmp https : // github . com / NVIDIA / nvidia - docker / releases / download / v1 . 0 . 1 / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb sudo dpkg - i / tmp / nvidia - docker * . deb && rm / tmp / nvidia - docker * . deb # Test nvidia - smi nvidia - docker run --rm nvidia/cuda nvidia-smi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ######################################## # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248 docker : tensorflow / tensorflow # \u516c\u5f0f GPU\u7248 docker : tensorflow / tensorflow : latest - gpu docker search tensorflow docker pull tensorflow / tensorflow : latest - gpu nvidia - docker run - it - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow : latest - gpu # Blocking Cross Origin WebSocket Attempt . \u304c\u51fa\u3066 kernel\u306b\u63a5\u7d9a\u3067\u304d\u306a\u3044\u72b6\u614b\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u3092\u9632\u3050\u305f\u3081\u306b \u3001 Origin\u901a\u904e\u8a31\u53ef\u3092\u8ffd\u52a0\u3059\u308b apt - get update apt - get vim vi / root / . jupyter / jupyter_notebook_config . py c . NotebookApp . allow_origin = \"http://YOURDOMAIN:8888\" # \u4e00\u5ea6 docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057 \u3001 gpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop d0b399c9ff83 docker commit d0b399c9ff83 gpu / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs nvidia - docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 gpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" # jupyter\u306f http : // localhost : 8888 # tensorboard\u306f http : // localhost : 6006 1 2 3 4 5 6 7 8 ######################################## # pip\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u308b ######################################## pip install pandas pip install seaborn pip install requests # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U","title":"AWS Ubuntu 16.04"},{"location":"develop-ubuntu/r1.0.1/aws-ec2-docker-gpu/#aws-ubuntu-1604","text":"1 2 3 4 5 6 7 8 9 10 AWS Ubuntu 16.04 kernel\u306f root @ip - 172 - 21 - 2 - 7 : / home / ubuntu # uname - a Linux ip - 172 - 21 - 2 - 7 4.4.0 - 64 - generic #85 - Ubuntu SMP Mon Feb 20 11 : 50 : 30 UTC 2017 x86_64 x86_64 x86_64 GNU / Linux # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001 \u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade reboot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 ######################################## # Setup Nvidia Drivers on EC2 Instance Host ######################################## # https : // github . com / fluxcapacitor / pipeline / wiki / AWS - GPU - TensorFlow - Docker apt - key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub sh - c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' apt - get update && apt - get install - y --no-install-recommends cuda-drivers 1 2 3 4 5 6 7 8 9 10 ######################################## # NVIDIA docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b ######################################## # https : // github . com / NVIDIA / nvidia - docker / # Install nvidia - docker and nvidia - docker - plugin wget - P / tmp https : // github . com / NVIDIA / nvidia - docker / releases / download / v1 . 0 . 1 / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb sudo dpkg - i / tmp / nvidia - docker * . deb && rm / tmp / nvidia - docker * . deb # Test nvidia - smi nvidia - docker run --rm nvidia/cuda nvidia-smi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ######################################## # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248 docker : tensorflow / tensorflow # \u516c\u5f0f GPU\u7248 docker : tensorflow / tensorflow : latest - gpu docker search tensorflow docker pull tensorflow / tensorflow : latest - gpu nvidia - docker run - it - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow : latest - gpu # Blocking Cross Origin WebSocket Attempt . \u304c\u51fa\u3066 kernel\u306b\u63a5\u7d9a\u3067\u304d\u306a\u3044\u72b6\u614b\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u3092\u9632\u3050\u305f\u3081\u306b \u3001 Origin\u901a\u904e\u8a31\u53ef\u3092\u8ffd\u52a0\u3059\u308b apt - get update apt - get vim vi / root / . jupyter / jupyter_notebook_config . py c . NotebookApp . allow_origin = \"http://YOURDOMAIN:8888\" # \u4e00\u5ea6 docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057 \u3001 gpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop d0b399c9ff83 docker commit d0b399c9ff83 gpu / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs nvidia - docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 gpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" # jupyter\u306f http : // localhost : 8888 # tensorboard\u306f http : // localhost : 6006 1 2 3 4 5 6 7 8 ######################################## # pip\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u308b ######################################## pip install pandas pip install seaborn pip install requests # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U","title":"AWS Ubuntu 16.04"},{"location":"develop-ubuntu/r1.0.1/vm-docker-cpu/","text":"\u4eee\u60f3\u30de\u30b7\u30f3\u4e0a\u306eUbuntu 16.04 1 2 3 4 5 6 UbuntuOS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248 docker : tensorflow / tensorflow docker search tensorflow docker pull tensorflow / tensorflow docker run - it - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow # \u4e00\u5ea6 docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057 \u3001 cpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop 9 a40dcc45724 docker commit 9 a40dcc45724 cpu / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 cpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" # jupyter\u306f http : // localhost : 8888 # tensorboard\u306f http : // localhost : 6006 1 2 3 4 5 6 7 8 ######################################## # pip\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u308b ######################################## pip install pandas pip install seaborn pip install requests # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U","title":"\u4eee\u60f3\u30de\u30b7\u30f3\u4e0a\u306eUbuntu 16.04"},{"location":"develop-ubuntu/r1.0.1/vm-docker-cpu/#ubuntu-1604","text":"1 2 3 4 5 6 UbuntuOS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248 docker : tensorflow / tensorflow docker search tensorflow docker pull tensorflow / tensorflow docker run - it - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow # \u4e00\u5ea6 docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057 \u3001 cpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop 9 a40dcc45724 docker commit 9 a40dcc45724 cpu / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081 \u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b # tensorboard\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u9069\u5f53\u306b\u4f5c\u308b mkdir - p / home / ubuntu / notebooks / logs docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=mypassword\" - p 6006 : 6006 - p 8888 : 8888 cpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh\" # jupyter\u306f http : // localhost : 8888 # tensorboard\u306f http : // localhost : 6006 1 2 3 4 5 6 7 8 ######################################## # pip\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5165\u308c\u308b ######################################## pip install pandas pip install seaborn pip install requests # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U","title":"\u4eee\u60f3\u30de\u30b7\u30f3\u4e0a\u306eUbuntu 16.04"},{"location":"develop-ubuntu/r1.0.1/vm-docker-datalab-cpu/","text":"\u4eee\u60f3\u30de\u30b7\u30f3\u4e0a\u306eUbuntu 16.04 1 2 3 4 5 6 UbuntuOS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 ######################################## # Docker image datalab\u7248 ######################################## # \u4e88\u3081Google Cloud Platform\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u4f5c\u6210\u3092\u7d42\u308f\u3089\u305b\u3066\u304a\u304f\u3053\u3068 # datalab\u7248 # https : // cloud . google . com / ml / docs / how - tos / getting - set - up?hl = ja # datalab\u7248: gcr . io / cloud - datalab / datalab : local # git : https : // github . com / googledatalab / datalab # git\u7248\u304b\u3089docker image\u3092\u4f5c\u308b: https : // github . com / googledatalab / datalab / wiki / Getting - Started docker pull gcr . io / cloud - datalab / datalab : local docker run - it - p 8081 : 8080 gcr . io / cloud - datalab / datalab : local cd curl https : // raw . githubusercontent . com / GoogleCloudPlatform / cloudml - samples / master / tools / setup_docker . sh | bash gcloud auth login # URL\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u30ea\u30f3\u30af\u306b\u98db\u3093\u3067\u30b3\u30fc\u30c9\u3092\u53d6\u5f97\u3057 \u3001\u30bf\u30fc\u30df\u30ca\u30eb\u306b\u5f35\u308a\u4ed8\u3051\u308b gcloud config set project myproject - 123456 root@ 04065 bfe6d0a:~# curl https : // raw . githubusercontent . com / GoogleCloudPlatform / cloudml - samples / master / tools / check_environment . py | python #################### % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 3557 100 3557 0 0 84912 0 --:--:-- --:--:-- --:--:-- 86756 WARNING : root : Couldn 't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be. Your active configuration is: [default] Success! Your environment is configured correctly. #################### gcloud ml-engine init-project # Google Cloud Storage\u306b\u30d0\u30b1\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b(\u4e00\u5ea6\u4f5c\u3063\u305f\u3053\u3068\u304c\u3042\u308c\u3070\u4e0d\u8981) PROJECT_ID=$(gcloud config list project --format \"value(core.project)\") BUCKET_NAME=${PROJECT_ID}-ml gsutil mb -l ASIA-EAST1 gs://$BUCKET_NAME # \u3053\u3053\u307e\u3067\u3067datalab\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u8a2d\u5b9a\u7d42\u4e86 # project_id\u304c\u5fc5\u8981\u3060\u3063\u305f\u3002DeepFishing\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f7f\u3063\u305f\u3002 # pip\u3092\u66f4\u65b0\u3057\u3066\u304a\u304f pip freeze --local | grep -v '^\\-e ' | cut - d = - f 1 | xargs - n1 pip install - U # \u4e00\u5ea6docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057\u3001cpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop 04065 bfe6d0a docker commit 04065 bfe6d0a lab / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b mkdir - p / home / ubuntu / notebooks # datalab\u7248docker\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5 docker run - itd - v / home / ubuntu / notebooks : / content / datalab / notebooks - e DATALAB_DEBUG = true - p 8081 : 8080 lab / tensorflow # jupyter\u306f http : // localhost : 8081 # datalab\u306ejupyter\u306fdocker\u30b3\u30f3\u30c6\u30ca\u3067\u306f8080\u3067\u52d5\u4f5c\u3057\u3066\u3044\u308b\u304c\u3001sign - in\u306eauth\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u304clocalhost: 8081 \u306b\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001 localhost : 8081 \u3092docker: 8080 \u306b\u8ee2\u9001\u3057\u3066\u4f7f\u3046 # tensorboard\u3082\u5165\u3063\u3066\u3044\u308b\u3051\u308c\u3069\u3082\u3001\u5b9f\u884c\u306fCloud ML\u306b\u3059\u308b\u306e\u3067\u3053\u3053\u3067\u306f\u8d77\u52d5\u3057\u3066\u3044\u306a\u3044","title":"\u4eee\u60f3\u30de\u30b7\u30f3\u4e0a\u306eUbuntu 16.04"},{"location":"develop-ubuntu/r1.0.1/vm-docker-datalab-cpu/#ubuntu-1604","text":"1 2 3 4 5 6 UbuntuOS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb # OS\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u5f8c \u3001\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u3059\u308b apt - get upgrade apt - get update apt - get dist - upgrade 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ######################################## # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b # https : // docs . docker . com / engine / installation / linux / ubuntu / ######################################## # curl\u4ed6\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install curl linux - image - extra - $ ( uname - r ) linux - image - extra - virtual # docker repository\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install apt - transport - https ca - certificates # docker GPG key\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b curl - fsSL https : // yum . dockerproject . org / gpg | sudo apt - key add - # docker\u30ad\u30fc\u3092\u78ba\u8a8d\u3059\u308b apt - key fingerprint 58118 E89F3A912897C070ADBF76221572C52609D # docker stable repository add - apt - repository \"deb https://apt.dockerproject.org/repo/ ubuntu-$(lsb_release -cs) main\" apt - get update # docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b apt - get install docker - engine # docker\u30d0\u30fc\u30b8\u30e7\u30f3\u30ea\u30b9\u30c8 apt - cache madison docker - engine # docker\u52d5\u4f5c\u78ba\u8a8d docker run hello - world 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 ######################################## # Docker image datalab\u7248 ######################################## # \u4e88\u3081Google Cloud Platform\u3067\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u4f5c\u6210\u3092\u7d42\u308f\u3089\u305b\u3066\u304a\u304f\u3053\u3068 # datalab\u7248 # https : // cloud . google . com / ml / docs / how - tos / getting - set - up?hl = ja # datalab\u7248: gcr . io / cloud - datalab / datalab : local # git : https : // github . com / googledatalab / datalab # git\u7248\u304b\u3089docker image\u3092\u4f5c\u308b: https : // github . com / googledatalab / datalab / wiki / Getting - Started docker pull gcr . io / cloud - datalab / datalab : local docker run - it - p 8081 : 8080 gcr . io / cloud - datalab / datalab : local cd curl https : // raw . githubusercontent . com / GoogleCloudPlatform / cloudml - samples / master / tools / setup_docker . sh | bash gcloud auth login # URL\u304c\u8868\u793a\u3055\u308c\u308b\u306e\u3067\u30ea\u30f3\u30af\u306b\u98db\u3093\u3067\u30b3\u30fc\u30c9\u3092\u53d6\u5f97\u3057 \u3001\u30bf\u30fc\u30df\u30ca\u30eb\u306b\u5f35\u308a\u4ed8\u3051\u308b gcloud config set project myproject - 123456 root@ 04065 bfe6d0a:~# curl https : // raw . githubusercontent . com / GoogleCloudPlatform / cloudml - samples / master / tools / check_environment . py | python #################### % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 3557 100 3557 0 0 84912 0 --:--:-- --:--:-- --:--:-- 86756 WARNING : root : Couldn 't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be. Your active configuration is: [default] Success! Your environment is configured correctly. #################### gcloud ml-engine init-project # Google Cloud Storage\u306b\u30d0\u30b1\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b(\u4e00\u5ea6\u4f5c\u3063\u305f\u3053\u3068\u304c\u3042\u308c\u3070\u4e0d\u8981) PROJECT_ID=$(gcloud config list project --format \"value(core.project)\") BUCKET_NAME=${PROJECT_ID}-ml gsutil mb -l ASIA-EAST1 gs://$BUCKET_NAME # \u3053\u3053\u307e\u3067\u3067datalab\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u8a2d\u5b9a\u7d42\u4e86 # project_id\u304c\u5fc5\u8981\u3060\u3063\u305f\u3002DeepFishing\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u4f7f\u3063\u305f\u3002 # pip\u3092\u66f4\u65b0\u3057\u3066\u304a\u304f pip freeze --local | grep -v '^\\-e ' | cut - d = - f 1 | xargs - n1 pip install - U # \u4e00\u5ea6docker\u3092\u629c\u3051\u3066\u30b3\u30f3\u30c6\u30ca\u3092\u505c\u6b62\u3057\u3001cpu / tensorflow\u3068\u3057\u3066image\u3092\u4f5c\u3063\u3066\u304a\u304f docker ps - a docker stop 04065 bfe6d0a docker commit 04065 bfe6d0a lab / tensorflow # localhost\u306e / home / ubuntu / notebooks\u3092docker\u306e / notebooks\u306b\u30de\u30a6\u30f3\u30c8\u3059\u308b\u305f\u3081\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4f5c\u308b mkdir - p / home / ubuntu / notebooks # datalab\u7248docker\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5 docker run - itd - v / home / ubuntu / notebooks : / content / datalab / notebooks - e DATALAB_DEBUG = true - p 8081 : 8080 lab / tensorflow # jupyter\u306f http : // localhost : 8081 # datalab\u306ejupyter\u306fdocker\u30b3\u30f3\u30c6\u30ca\u3067\u306f8080\u3067\u52d5\u4f5c\u3057\u3066\u3044\u308b\u304c\u3001sign - in\u306eauth\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u304clocalhost: 8081 \u306b\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001 localhost : 8081 \u3092docker: 8080 \u306b\u8ee2\u9001\u3057\u3066\u4f7f\u3046 # tensorboard\u3082\u5165\u3063\u3066\u3044\u308b\u3051\u308c\u3069\u3082\u3001\u5b9f\u884c\u306fCloud ML\u306b\u3059\u308b\u306e\u3067\u3053\u3053\u3067\u306f\u8d77\u52d5\u3057\u3066\u3044\u306a\u3044","title":"\u4eee\u60f3\u30de\u30b7\u30f3\u4e0a\u306eUbuntu 16.04"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/","text":"AWS Ubuntu 16.04 - TensorFlor GPU/XLA Python/C++ Python\u3068C++\u306e\u4e21\u65b9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u6e96\u5099\u306f AWS EC2 p2.xlarge Docker (GPU) \u3068\u540c\u3058\u3067\u3001Docker\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u7570\u306a\u308a\u307e\u3059\u3002 AWS EC2 p2.xlarge Docker (GPU)\u3068\u540c\u3058\u90e8\u5206 NVIDIA Docker\u304c\u4f7f\u3048\u308b\u72b6\u614b\u306b\u3042\u308b\u306a\u3089\u4e0d\u8981\u3067\u3059\u3002 1 2 3 4 AWS Ubuntu 16.04 kernel\u306f root@ip-172-21-2-8:/home/ubuntu# uname -a Linux ip-172-21-2-8 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 1 2 3 4 5 6 7 8 9 ######################################## # Setup Nvidia Drivers on EC2 Instance Host ######################################## # https : // github . com / fluxcapacitor / pipeline / wiki / AWS - GPU - TensorFlow - Docker apt - key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub sh - c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' apt - get update && apt - get install - y --no-install-recommends cuda-drivers 1 2 3 4 5 6 7 8 9 10 ######################################## # NVIDIA docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b ######################################## # https : // github . com / NVIDIA / nvidia - docker / # Install nvidia - docker and nvidia - docker - plugin wget - P / tmp https : // github . com / NVIDIA / nvidia - docker / releases / download / v1 . 0 . 1 / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb sudo dpkg - i / tmp / nvidia - docker * . deb && rm / tmp / nvidia - docker * . deb # Test nvidia - smi nvidia - docker run --rm nvidia/cuda nvidia-smi \u3053\u3053\u304b\u3089\u65b0\u3057\u3044\u90e8\u5206 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ####################################### # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248\u30b3\u30fc\u30c9\u4ed8\u304d docker : tensorflow / tensorflow : latest - develop # \u516c\u5f0f GPU\u7248\u30b3\u30fc\u30c9\u4ed8\u304d docker : tensorflow / tensorflow : latest - develop - gpu # docker\u30b3\u30de\u30f3\u30c9\u306falias\u3092 . bashrc\u306b\u66f8\u3044\u3066\u304a\u304f\u3068\u3088\u3044 # alias docker = 'nvidia-docker' docker pull tensorflow / tensorflow : latest - devel - gpu nvidia - docker run - it - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow : latest - devel - gpu # docker\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3057\u305f\u72b6\u614b apt - get update apt - get install vim # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U # docker\u5185\u3067jupyter home dir\u3092\u5909\u66f4\u3059\u308b # \u305d\u306e\u307e\u307e\u3060\u3068 / \u304c\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u306a\u3063\u3066\u3044\u3066\u3084\u3070\u3044\u306e\u3067\u5909\u66f4\u3059\u308b # \u3055\u3089\u306b Blocking Cross Origin WebSocket Attempt . \u304c\u51fa\u3066 kernel\u306b\u63a5\u7d9a\u3067\u304d\u306a\u3044\u72b6\u614b\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u3092\u9632\u3050\u305f\u3081\u306b \u3001 Origin\u901a\u904e\u8a31\u53ef\u3092\u8ffd\u52a0\u3059\u308b vi / root / . jupyter / jupyter_notebook_config . py c . NotebookApp . ip = '*' c . NotebookApp . notebook_dir = u '/notebooks/' c . NotebookApp . allow_origin = \"http://MY-DOMAIN.com:8888\" # exit\u3067docker\u3092\u629c\u3051 \u3001 docker commit\u3067\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b nvidia - docker ps - a nvidia - docker commit \u30b3\u30f3\u30c6\u30ca ID gpu / tensorflow nvidia - docker rm \u30b3\u30f3\u30c6\u30ca ID # \u30b4\u30df\u524a\u9664 mkdir - p / home / ubuntu / notebooks / logs # \u3059\u3067\u306b\u4f5c\u3063\u3066\u3042\u308c\u3070\u4e0d\u8981 # \u4f5c\u6210\u3057\u305f docker\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u30b3\u30f3\u30c6\u30ca\u3092\u4f5c\u6210\u3057\u8d77\u52d5\u3059\u308b # jupyter\u304c5 . 0 \u306b\u306a\u3063\u3066\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u304c\u5909\u308f\u3063\u305f\u306e\u3067\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u4ed8\u3051\u3066\u8d77\u52d5\u3059\u308b nvidia - docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=MYPASSWORD\" - p 6006 : 6006 - p 8888 : 8888 gpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh --allow-root --NotebookApp.iopub_data_rate_limit=10000000\" \u3053\u3053\u307e\u3067\u3067\u901a\u5e38\u306eTensorFlow GPU\u7248\u304c\u52d5\u4f5c\u5b8c\u4e86\u3002 git\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092build\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # docker\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3059\u308b nvidia - docker ps - a nvidia - docker exec - it \u30b3\u30f3\u30c6\u30ca ID / bin / bash cd / tensorflow git pull # XLA just - in - time compiler \u306f\u304a\u597d\u307f\u3067\u3001\u305d\u308c\u4ee5\u5916\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067 . / configure Please specify the location of python . [ Default is / usr / bin / python ]: Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [ Default is - march = native ]: Do you wish to use jemalloc as the malloc implementation ? [ Y / n ] jemalloc enabled Do you wish to build TensorFlow with Google Cloud Platform support ? [ y / N ] No Google Cloud Platform support will be enabled for TensorFlow Do you wish to build TensorFlow with Hadoop File System support ? [ y / N ] No Hadoop File System support will be enabled for TensorFlow Do you wish to build TensorFlow with the XLA just - in - time compiler ( experimental ) ? [ y / N ] Y XLA JIT support will be enabled for TensorFlow Found possible Python library paths : / usr / local / lib / python2 . 7 / dist - packages / usr / lib / python2 . 7 / dist - packages Please input the desired Python library path to use . Default is [ / usr / local / lib / python2 . 7 / dist - packages ] Using python library path : / usr / local / lib / python2 . 7 / dist - packages Do you wish to build TensorFlow with OpenCL support ? [ y / N ] No OpenCL support will be enabled for TensorFlow Please specify which gcc should be used by nvcc as the host compiler . [ Default is / usr / bin / gcc ]: Please specify the CUDA SDK version you want to use , e . g . 7 . 0 . [ Leave empty to use system default ]: Please specify the location where CUDA toolkit is installed . Refer to README . md for more details . [ Default is / usr / local / cuda ]: Please specify the Cudnn version you want to use . [ Leave empty to use system default ]: Please specify the location where cuDNN library is installed . Refer to README . md for more details . [ Default is / usr / local / cuda ]: Extracting Bazel installation ... .......... #################### # GPU\u7248\u30d3\u30eb\u30c9 #################### # AWS EC2 p2 . xlarge\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 time bazel build - c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package real 95 m46 . 429 s user 0 m0 . 220 s sys 0 m0 . 388 s # whl\u3092\u751f\u6210 bazel - bin / tensorflow / tools / pip_package / build_pip_package / tmp / tensorflow_pkg # TensorFlow \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install / tmp / tensorflow_pkg / tensorflow - 1 . 1 . 0 - cp27 - cp27mu - linux_x86_64 . whl # . so\u3092\u4f5c\u6210 time bazel build - c opt --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so real 4 m25 . 759 s user 0 m0 . 016 s sys 0 m0 . 024 s # / usr / local / lib \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b install - m 0644 bazel - bin / tensorflow / libtensorflow_cc . so / usr / local / lib / ldconfig \u30d8\u30c3\u30c0\u30fc\u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b \u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092/tensorflow/\u76f4\u4e0b\u3067\u5b9f\u884c\u3059\u308b \u53c2\u8003\uff1ahttp://memo.saitodev.com/home/tensorflow/build/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash -eu # -*- coding: utf-8 -*- # tensorflow\u3092\u5229\u7528\u3059\u308bC++\u30bd\u30fc\u30b9\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\u969b\u306b\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30a4\u30f3\u30af\u30eb\u30fc\u30c9\u30d1\u30b9\u306b\u8ffd\u52a0\u3059\u308b\u3002 HEADER_DIR = /usr/local/tensorflow/include if [ ! -e $HEADER_DIR ] ; then mkdir -p $HEADER_DIR fi find tensorflow/core -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/cc -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/c -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find third_party/eigen3 -follow -type f -exec cp --parents {} $HEADER_DIR \\; pushd bazel-genfiles find tensorflow -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/protobuf/src find google -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/eigen_archive find Eigen -follow -type f -exec cp --parents {} $HEADER_DIR \\; find unsupported -follow -type f -exec cp --parents {} $HEADER_DIR \\; popd \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u30b3\u30f3\u30d1\u30a4\u30eb 1 2 3 4 5 # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb time bazel build - c opt // tensorflow / cc : tutorials_example_trainer # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u5b9f\u884c time . / bazel - bin / tensorflow / cc / tutorials_example_trainer \u81ea\u524dC++\u30b3\u30fc\u30c9\u306e\u30b3\u30f3\u30d1\u30a4\u30eb 1 2 3 4 5 ######################################## # \u30b3\u30f3\u30d1\u30a4\u30eb\u65b9\u6cd5 ######################################## # loadGraph.cpp g++ loadGraph.cpp -std=c++11 -I/usr/local/tensorflow/include -L. -ltensorflow_cc -Wl,-rpath=. Docker\u30a4\u30e1\u30fc\u30b8\u4f5c\u6210 \u4f5c\u6210\u3057\u305f\u30b3\u30f3\u30c6\u30ca\u3092\u30a4\u30e1\u30fc\u30b8\u306b\u4fdd\u5b58\u3057\u3066\u304a\u304f 1 2 3 nvidia-docker ps -a nvidia-docker commit \u30b3\u30f3\u30c6\u30caID gpu/tensorflow nvidia-docker rm \u30b3\u30f3\u30c6\u30caID Docker\u30b3\u30f3\u30c6\u30ca\u4f5c\u6210-\u8d77\u52d5 1 nvidia-docker run -itd -v /home/ubuntu/notebooks:/notebooks -e \"PASSWORD=MYPASSWORD\" -p 6006:6006 -p 8888:8888 gpu/tensorflow /bin/bash -c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh --allow-root --NotebookApp.iopub_data_rate_limit=10000000\" Docker\u30b3\u30f3\u30c6\u30ca\u505c\u6b62-\u8d77\u52d5 \u901a\u5e38\u306e\u8d77\u52d5\u3068\u505c\u6b62 1 2 nvidia-docker stop \u30b3\u30f3\u30c6\u30caID nvidia-docker start \u30b3\u30f3\u30c6\u30caID","title":"AWS Ubuntu 16.04 - TensorFlor GPU/XLA Python/C++"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#aws-ubuntu-1604-tensorflor-gpuxla-pythonc","text":"Python\u3068C++\u306e\u4e21\u65b9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u6e96\u5099\u306f AWS EC2 p2.xlarge Docker (GPU) \u3068\u540c\u3058\u3067\u3001Docker\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u7570\u306a\u308a\u307e\u3059\u3002","title":"AWS Ubuntu 16.04 - TensorFlor GPU/XLA Python/C++"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#aws-ec2-p2xlarge-docker-gpu","text":"NVIDIA Docker\u304c\u4f7f\u3048\u308b\u72b6\u614b\u306b\u3042\u308b\u306a\u3089\u4e0d\u8981\u3067\u3059\u3002 1 2 3 4 AWS Ubuntu 16.04 kernel\u306f root@ip-172-21-2-8:/home/ubuntu# uname -a Linux ip-172-21-2-8 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux 1 2 3 4 5 6 7 8 9 ######################################## # Setup Nvidia Drivers on EC2 Instance Host ######################################## # https : // github . com / fluxcapacitor / pipeline / wiki / AWS - GPU - TensorFlow - Docker apt - key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub sh - c 'echo \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/cuda.list' apt - get update && apt - get install - y --no-install-recommends cuda-drivers 1 2 3 4 5 6 7 8 9 10 ######################################## # NVIDIA docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b ######################################## # https : // github . com / NVIDIA / nvidia - docker / # Install nvidia - docker and nvidia - docker - plugin wget - P / tmp https : // github . com / NVIDIA / nvidia - docker / releases / download / v1 . 0 . 1 / nvidia - docker_1 . 0 . 1 - 1 _amd64 . deb sudo dpkg - i / tmp / nvidia - docker * . deb && rm / tmp / nvidia - docker * . deb # Test nvidia - smi nvidia - docker run --rm nvidia/cuda nvidia-smi","title":"AWS EC2 p2.xlarge Docker (GPU)\u3068\u540c\u3058\u90e8\u5206"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ####################################### # TensorFlow Docker \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb ######################################## # https : // hub . docker . com / r / tensorflow / tensorflow / # https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / tools / docker / README . md # \u516c\u5f0f CPU\u7248\u30b3\u30fc\u30c9\u4ed8\u304d docker : tensorflow / tensorflow : latest - develop # \u516c\u5f0f GPU\u7248\u30b3\u30fc\u30c9\u4ed8\u304d docker : tensorflow / tensorflow : latest - develop - gpu # docker\u30b3\u30de\u30f3\u30c9\u306falias\u3092 . bashrc\u306b\u66f8\u3044\u3066\u304a\u304f\u3068\u3088\u3044 # alias docker = 'nvidia-docker' docker pull tensorflow / tensorflow : latest - devel - gpu nvidia - docker run - it - p 6006 : 6006 - p 8888 : 8888 tensorflow / tensorflow : latest - devel - gpu # docker\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3057\u305f\u72b6\u614b apt - get update apt - get install vim # pip\u30d5\u30eb\u66f4\u65b0 pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip install -U # docker\u5185\u3067jupyter home dir\u3092\u5909\u66f4\u3059\u308b # \u305d\u306e\u307e\u307e\u3060\u3068 / \u304c\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u306a\u3063\u3066\u3044\u3066\u3084\u3070\u3044\u306e\u3067\u5909\u66f4\u3059\u308b # \u3055\u3089\u306b Blocking Cross Origin WebSocket Attempt . \u304c\u51fa\u3066 kernel\u306b\u63a5\u7d9a\u3067\u304d\u306a\u3044\u72b6\u614b\u304c\u767a\u751f\u3059\u308b\u3053\u3068\u3092\u9632\u3050\u305f\u3081\u306b \u3001 Origin\u901a\u904e\u8a31\u53ef\u3092\u8ffd\u52a0\u3059\u308b vi / root / . jupyter / jupyter_notebook_config . py c . NotebookApp . ip = '*' c . NotebookApp . notebook_dir = u '/notebooks/' c . NotebookApp . allow_origin = \"http://MY-DOMAIN.com:8888\" # exit\u3067docker\u3092\u629c\u3051 \u3001 docker commit\u3067\u30a4\u30e1\u30fc\u30b8\u3092\u4f5c\u6210\u3059\u308b nvidia - docker ps - a nvidia - docker commit \u30b3\u30f3\u30c6\u30ca ID gpu / tensorflow nvidia - docker rm \u30b3\u30f3\u30c6\u30ca ID # \u30b4\u30df\u524a\u9664 mkdir - p / home / ubuntu / notebooks / logs # \u3059\u3067\u306b\u4f5c\u3063\u3066\u3042\u308c\u3070\u4e0d\u8981 # \u4f5c\u6210\u3057\u305f docker\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u30b3\u30f3\u30c6\u30ca\u3092\u4f5c\u6210\u3057\u8d77\u52d5\u3059\u308b # jupyter\u304c5 . 0 \u306b\u306a\u3063\u3066\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u304c\u5909\u308f\u3063\u305f\u306e\u3067\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u4ed8\u3051\u3066\u8d77\u52d5\u3059\u308b nvidia - docker run - itd - v / home / ubuntu / notebooks : / notebooks - e \"PASSWORD=MYPASSWORD\" - p 6006 : 6006 - p 8888 : 8888 gpu / tensorflow / bin / bash - c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh --allow-root --NotebookApp.iopub_data_rate_limit=10000000\" \u3053\u3053\u307e\u3067\u3067\u901a\u5e38\u306eTensorFlow GPU\u7248\u304c\u52d5\u4f5c\u5b8c\u4e86\u3002","title":"\u3053\u3053\u304b\u3089\u65b0\u3057\u3044\u90e8\u5206"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#gitbuild","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # docker\u306bbash\u3067\u30ed\u30b0\u30a4\u30f3\u3059\u308b nvidia - docker ps - a nvidia - docker exec - it \u30b3\u30f3\u30c6\u30ca ID / bin / bash cd / tensorflow git pull # XLA just - in - time compiler \u306f\u304a\u597d\u307f\u3067\u3001\u305d\u308c\u4ee5\u5916\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067 . / configure Please specify the location of python . [ Default is / usr / bin / python ]: Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [ Default is - march = native ]: Do you wish to use jemalloc as the malloc implementation ? [ Y / n ] jemalloc enabled Do you wish to build TensorFlow with Google Cloud Platform support ? [ y / N ] No Google Cloud Platform support will be enabled for TensorFlow Do you wish to build TensorFlow with Hadoop File System support ? [ y / N ] No Hadoop File System support will be enabled for TensorFlow Do you wish to build TensorFlow with the XLA just - in - time compiler ( experimental ) ? [ y / N ] Y XLA JIT support will be enabled for TensorFlow Found possible Python library paths : / usr / local / lib / python2 . 7 / dist - packages / usr / lib / python2 . 7 / dist - packages Please input the desired Python library path to use . Default is [ / usr / local / lib / python2 . 7 / dist - packages ] Using python library path : / usr / local / lib / python2 . 7 / dist - packages Do you wish to build TensorFlow with OpenCL support ? [ y / N ] No OpenCL support will be enabled for TensorFlow Please specify which gcc should be used by nvcc as the host compiler . [ Default is / usr / bin / gcc ]: Please specify the CUDA SDK version you want to use , e . g . 7 . 0 . [ Leave empty to use system default ]: Please specify the location where CUDA toolkit is installed . Refer to README . md for more details . [ Default is / usr / local / cuda ]: Please specify the Cudnn version you want to use . [ Leave empty to use system default ]: Please specify the location where cuDNN library is installed . Refer to README . md for more details . [ Default is / usr / local / cuda ]: Extracting Bazel installation ... .......... #################### # GPU\u7248\u30d3\u30eb\u30c9 #################### # AWS EC2 p2 . xlarge\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 time bazel build - c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package real 95 m46 . 429 s user 0 m0 . 220 s sys 0 m0 . 388 s # whl\u3092\u751f\u6210 bazel - bin / tensorflow / tools / pip_package / build_pip_package / tmp / tensorflow_pkg # TensorFlow \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install / tmp / tensorflow_pkg / tensorflow - 1 . 1 . 0 - cp27 - cp27mu - linux_x86_64 . whl # . so\u3092\u4f5c\u6210 time bazel build - c opt --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so real 4 m25 . 759 s user 0 m0 . 016 s sys 0 m0 . 024 s # / usr / local / lib \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b install - m 0644 bazel - bin / tensorflow / libtensorflow_cc . so / usr / local / lib / ldconfig","title":"git\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092build\u3059\u308b"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#_2","text":"\u3053\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092/tensorflow/\u76f4\u4e0b\u3067\u5b9f\u884c\u3059\u308b \u53c2\u8003\uff1ahttp://memo.saitodev.com/home/tensorflow/build/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #!/bin/bash -eu # -*- coding: utf-8 -*- # tensorflow\u3092\u5229\u7528\u3059\u308bC++\u30bd\u30fc\u30b9\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\u969b\u306b\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30a4\u30f3\u30af\u30eb\u30fc\u30c9\u30d1\u30b9\u306b\u8ffd\u52a0\u3059\u308b\u3002 HEADER_DIR = /usr/local/tensorflow/include if [ ! -e $HEADER_DIR ] ; then mkdir -p $HEADER_DIR fi find tensorflow/core -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/cc -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/c -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find third_party/eigen3 -follow -type f -exec cp --parents {} $HEADER_DIR \\; pushd bazel-genfiles find tensorflow -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/protobuf/src find google -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/eigen_archive find Eigen -follow -type f -exec cp --parents {} $HEADER_DIR \\; find unsupported -follow -type f -exec cp --parents {} $HEADER_DIR \\; popd","title":"\u30d8\u30c3\u30c0\u30fc\u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#_3","text":"1 2 3 4 5 # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb time bazel build - c opt // tensorflow / cc : tutorials_example_trainer # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u5b9f\u884c time . / bazel - bin / tensorflow / cc / tutorials_example_trainer","title":"\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u30b3\u30f3\u30d1\u30a4\u30eb"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#c","text":"1 2 3 4 5 ######################################## # \u30b3\u30f3\u30d1\u30a4\u30eb\u65b9\u6cd5 ######################################## # loadGraph.cpp g++ loadGraph.cpp -std=c++11 -I/usr/local/tensorflow/include -L. -ltensorflow_cc -Wl,-rpath=.","title":"\u81ea\u524dC++\u30b3\u30fc\u30c9\u306e\u30b3\u30f3\u30d1\u30a4\u30eb"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#docker","text":"\u4f5c\u6210\u3057\u305f\u30b3\u30f3\u30c6\u30ca\u3092\u30a4\u30e1\u30fc\u30b8\u306b\u4fdd\u5b58\u3057\u3066\u304a\u304f 1 2 3 nvidia-docker ps -a nvidia-docker commit \u30b3\u30f3\u30c6\u30caID gpu/tensorflow nvidia-docker rm \u30b3\u30f3\u30c6\u30caID","title":"Docker\u30a4\u30e1\u30fc\u30b8\u4f5c\u6210"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#docker-","text":"1 nvidia-docker run -itd -v /home/ubuntu/notebooks:/notebooks -e \"PASSWORD=MYPASSWORD\" -p 6006:6006 -p 8888:8888 gpu/tensorflow /bin/bash -c \"tensorboard --logdir=/notebooks/logs& /run_jupyter.sh --allow-root --NotebookApp.iopub_data_rate_limit=10000000\"","title":"Docker\u30b3\u30f3\u30c6\u30ca\u4f5c\u6210-\u8d77\u52d5"},{"location":"develop-ubuntu/r1.1.0%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#docker-_1","text":"\u901a\u5e38\u306e\u8d77\u52d5\u3068\u505c\u6b62 1 2 nvidia-docker stop \u30b3\u30f3\u30c6\u30caID nvidia-docker start \u30b3\u30f3\u30c6\u30caID","title":"Docker\u30b3\u30f3\u30c6\u30ca\u505c\u6b62-\u8d77\u52d5"},{"location":"develop-ubuntu/r1.2%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/","text":"AWS Ubuntu 16.04 - TensorFlor r1.2 GPU/XLA/MKL Python/C++ Python\u3068C++\u306e\u4e21\u65b9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059 Docker\u306fr.1.1\u306e\u6642\u306eDocker\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 \u4eca\u56de\u306f\u74b0\u5883\u306fr1.1\u53c2\u8003\u306b\u3002 \u66f4\u65b0\u306f\u30d3\u30eb\u30c9\u95a2\u9023\u90e8\u5206\u3060\u3051\u3002 \u4eca\u56de\u30d3\u30eb\u30c9\u306b\u306fp2.8xlarge Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz 32\u30b3\u30a2\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 MKL\u5bfe\u5fdcCPU\u306f\u3001 https://github.com/01org/mkl-dnn Intel MKL-DNN supports Intel(R) 64 architecture processors and is optimized for 1 2 3 4 5 6 7 Intel Atom(R) processor with Intel(R) SSE4.1 support 4th, 5th, 6th and 7th generation Intel(R) Core processor Intel(R) Xeon(R) processor E5 v3 family (code named Haswell) Intel(R) Xeon(R) processor E5 v4 family (code named Broadwell) Intel(R) Xeon(R) Platinum processor family (code name Skylake) Intel(R) Xeon Phi(TM) product family x200 (code named Knights Landing) Future Intel(R) Xeon Phi(TM) processor (code named Knights Mill) \u3068\u306a\u3063\u3066\u3044\u3066\u3001p2.xlarge\u3068p2.8xlarge\u306f\u5171\u306bBroadwell\u306a\u306e\u3067MKL\u3092\u5229\u7528\u3067\u304d\u308b\u3002 \u4f5c\u696d\u306f\u3059\u3079\u3066Docker\u30b3\u30f3\u30c6\u30ca\u5185\u3067\u884c\u3046 1 2 apt-get update apt-get dist-upgrade Docker\u30b3\u30f3\u30c6\u30ca\u518d\u8d77\u52d5 Intel Math Kernel Library MKL\u3092\u6709\u52b9\u306b\u3059\u308b\u3068\u3001configure\u3067locate\u3092\u4f7f\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u691c\u7d22\u304c\u5b9f\u884c\u3055\u308c\u308b\u3002Docker\u306blocate\u304c\u7121\u3044\u306e\u3067\u3001apt-get install locate\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304c\u3001locate\u306fupdatedb\u3092\u5b9f\u884c\u3057\u3066DB\u306b\u8a18\u9332\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u691c\u7d22\u3059\u308b\u305f\u3081\u3001updatedb\u306e\u5b9f\u884c\u3082\u5fc5\u8981\u306b\u306a\u308b TensorFlow\u306econfigure\u6642\u306bMKLML\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u305f\u3081\u3001Intel\u304b\u3089MKL\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9/\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u306f\u306a\u3044\u3002\u3053\u308c\u305f\u3076\u3093\u5225\u7269\u3002 1 2 apt-get install locate cpio updatedb updatedb\u3067locate\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9DB\u3092\u66f4\u65b0\u3059\u308b bazel\u306f\u30d3\u30eb\u30c9\u3057\u76f4\u3059 1 2 3 4 5 6 7 8 9 10 11 12 ######################################## # bazel-0.5.1 \u30d3\u30eb\u30c9 ######################################## cd /bazel wget --no-check-certificate https://github.com/bazelbuild/bazel/releases/download/0.5.1/bazel-0.5.1-dist.zip unzip bazel-0.5.1-dist.zip -d bazel-0.5.1 cd bazel-0.5.1 time ./compile.sh real 2m31.561s user 19m59.060s sys 2m32.900s cp output/bazel /usr/local/bin/ TensorFlow r1.2 \u30b3\u30f3\u30d1\u30a4\u30eb r1.1\u306e\u6642\u306egit\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u3042\u308b\u5834\u5408\u306ffetch/checkout\u3067\u66f4\u65b0\u3059\u308b \u7121\u3051\u308c\u3070git clone\u3067\u30b3\u30fc\u30c9\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068 1 2 3 cd /tensorflow git fetch git checkout r1.2 CPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5834\u5408\u3001 Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you wish to build TensorFlow with CUDA support? [y/N] No CUDA support will be enabled for TensorFlow OpenCL\u30b5\u30dd\u30fc\u30c8\uff1f\u306e\u6b21\u306bCUDA\u30b5\u30dd\u30fc\u30c8\uff1f\u304c\u6765\u308b\u3002CPU\u306a\u306e\u3067CUDA\u4f7f\u308f\u306a\u3044\u3067\u9032\u3080\u3002 \u3068\u3053\u308d\u304c\u3001GPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5834\u5408\u3001 No OpenCL support will be enabled for TensorFlow Do you want to use clang as CUDA compiler? [y/N] nvcc will be used as CUDA compiler OpenCL\u30b5\u30dd\u30fc\u30c8\uff1f\u306e\u6b21\u306b\u3001CUDA\u30b3\u30f3\u30d1\u30a4\u30e9\u306bclang\u4f7f\u3046\uff1f\u3068\u6765\u3066\u3044\u308b\u3002CUDA\u4f7f\u3046\u3053\u3068\u524d\u63d0\u3067\u8a71\u304c\u9032\u3093\u3067\u3044\u308b\u306e\u3067\u3001\u3053\u3053\u3067Y\u3092\u9078\u629e\u3057\u306a\u3044\u3088\u3046\u306b\u6ce8\u610f\uff01clang\u306f\u5341\u5206\u306a\u30c6\u30b9\u30c8\u304c\u3055\u308c\u3066\u3044\u306a\u3044\u3002 1 ./configure Please specify the location of python. [Default is /usr/bin/python]: Found possible Python library paths: /usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages] Using python library path: /usr/local/lib/python2.7/dist-packages Do you wish to build TensorFlow with MKL support? [y/N] Y MKL support will be enabled for TensorFlow Do you wish to download MKL LIB from the web? [Y/n] Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: Do you wish to use jemalloc as the malloc implementation? [Y/n] jemalloc enabled Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] No Google Cloud Platform support will be enabled for TensorFlow Do you wish to build TensorFlow with Hadoop File System support? [y/N] No Hadoop File System support will be enabled for TensorFlow Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] Y XLA JIT support will be enabled for TensorFlow Do you wish to build TensorFlow with VERBS support? [y/N] No VERBS support will be enabled for TensorFlow Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you want to use clang as CUDA compiler? [y/N] nvcc will be used as CUDA compiler Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: Please specify the location where CUDA toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: Please specify the cuDNN version you want to use. [Leave empty to use system default]: Please specify the location where cuDNN library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: Extracting Bazel installation... . INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes. Configuration finished 32\u30b3\u30a2\u306ep2.8xlarge\u306fTensorFlow\u30b3\u30f3\u30d1\u30a4\u30eb\u304c\u65e9\u3044\uff01 1 time bazel build -c opt --output_filter='^//tensorflow' --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package real 20m21.467s user 0m0.044s sys 0m0.144s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # whl\u3092\u751f\u6210 bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg # TensorFlow \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install /tmp/tensorflow_pkg/tensorflow-1.2.0-cp27-cp27mu-linux_x86_64.whl # .so\u3092\u4f5c\u6210 time bazel build -c opt --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so real 0m44.869s user 0m0.004s sys 0m0.052s # /usr/local/lib \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b install -m 0644 bazel-bin/tensorflow/libtensorflow_cc.so /usr/local/lib/ ldconfig # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb time bazel build -c opt //tensorflow/cc:tutorials_example_trainer real 4m12.963s user 0m0.012s sys 0m0.068s # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u5b9f\u884c ./bazel-bin/tensorflow/cc/tutorials_example_trainer header \u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b vi ./tensorflow_header_install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash -eu # -*- coding: utf-8 -*- # http://memo.saitodev.com/home/tensorflow/build/ # tensorflow\u3092\u5229\u7528\u3059\u308bC++\u30bd\u30fc\u30b9\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\u969b\u306b\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30a4\u30f3\u30af\u30eb\u30fc\u30c9\u30d1\u30b9\u306b\u8ffd\u52a0\u3059\u308b\u3002 HEADER_DIR = /usr/local/tensorflow/include if [ ! -e $HEADER_DIR ] ; then mkdir -p $HEADER_DIR fi find tensorflow/core -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/cc -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/c -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find third_party/eigen3 -follow -type f -exec cp --parents {} $HEADER_DIR \\; pushd bazel-genfiles find tensorflow -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/protobuf/src find google -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/eigen_archive find Eigen -follow -type f -exec cp --parents {} $HEADER_DIR \\; find unsupported -follow -type f -exec cp --parents {} $HEADER_DIR \\; popd GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\u306f\u3001bazel build\u30aa\u30d7\u30b7\u30e7\u30f3\u306e --config=cuda \u304c\u3042\u308b\u304b\u3069\u3046\u304b\u3060\u3051\u3002 configure\u3082\u6ce8\u610f\u304c\u5fc5\u8981\u3060\u3051\u3069\u3082\u3002","title":"AWS Ubuntu 16.04 - TensorFlor r1.2 GPU/XLA/MKL Python/C++"},{"location":"develop-ubuntu/r1.2%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#aws-ubuntu-1604-tensorflor-r12-gpuxlamkl-pythonc","text":"Python\u3068C++\u306e\u4e21\u65b9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059 Docker\u306fr.1.1\u306e\u6642\u306eDocker\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 \u4eca\u56de\u306f\u74b0\u5883\u306fr1.1\u53c2\u8003\u306b\u3002 \u66f4\u65b0\u306f\u30d3\u30eb\u30c9\u95a2\u9023\u90e8\u5206\u3060\u3051\u3002 \u4eca\u56de\u30d3\u30eb\u30c9\u306b\u306fp2.8xlarge Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz 32\u30b3\u30a2\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 MKL\u5bfe\u5fdcCPU\u306f\u3001 https://github.com/01org/mkl-dnn Intel MKL-DNN supports Intel(R) 64 architecture processors and is optimized for 1 2 3 4 5 6 7 Intel Atom(R) processor with Intel(R) SSE4.1 support 4th, 5th, 6th and 7th generation Intel(R) Core processor Intel(R) Xeon(R) processor E5 v3 family (code named Haswell) Intel(R) Xeon(R) processor E5 v4 family (code named Broadwell) Intel(R) Xeon(R) Platinum processor family (code name Skylake) Intel(R) Xeon Phi(TM) product family x200 (code named Knights Landing) Future Intel(R) Xeon Phi(TM) processor (code named Knights Mill) \u3068\u306a\u3063\u3066\u3044\u3066\u3001p2.xlarge\u3068p2.8xlarge\u306f\u5171\u306bBroadwell\u306a\u306e\u3067MKL\u3092\u5229\u7528\u3067\u304d\u308b\u3002 \u4f5c\u696d\u306f\u3059\u3079\u3066Docker\u30b3\u30f3\u30c6\u30ca\u5185\u3067\u884c\u3046 1 2 apt-get update apt-get dist-upgrade Docker\u30b3\u30f3\u30c6\u30ca\u518d\u8d77\u52d5 Intel Math Kernel Library MKL\u3092\u6709\u52b9\u306b\u3059\u308b\u3068\u3001configure\u3067locate\u3092\u4f7f\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u691c\u7d22\u304c\u5b9f\u884c\u3055\u308c\u308b\u3002Docker\u306blocate\u304c\u7121\u3044\u306e\u3067\u3001apt-get install locate\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304c\u3001locate\u306fupdatedb\u3092\u5b9f\u884c\u3057\u3066DB\u306b\u8a18\u9332\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u691c\u7d22\u3059\u308b\u305f\u3081\u3001updatedb\u306e\u5b9f\u884c\u3082\u5fc5\u8981\u306b\u306a\u308b TensorFlow\u306econfigure\u6642\u306bMKLML\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u305f\u3081\u3001Intel\u304b\u3089MKL\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9/\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u306f\u306a\u3044\u3002\u3053\u308c\u305f\u3076\u3093\u5225\u7269\u3002 1 2 apt-get install locate cpio updatedb updatedb\u3067locate\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9DB\u3092\u66f4\u65b0\u3059\u308b bazel\u306f\u30d3\u30eb\u30c9\u3057\u76f4\u3059 1 2 3 4 5 6 7 8 9 10 11 12 ######################################## # bazel-0.5.1 \u30d3\u30eb\u30c9 ######################################## cd /bazel wget --no-check-certificate https://github.com/bazelbuild/bazel/releases/download/0.5.1/bazel-0.5.1-dist.zip unzip bazel-0.5.1-dist.zip -d bazel-0.5.1 cd bazel-0.5.1 time ./compile.sh real 2m31.561s user 19m59.060s sys 2m32.900s cp output/bazel /usr/local/bin/","title":"AWS Ubuntu 16.04 - TensorFlor r1.2 GPU/XLA/MKL Python/C++"},{"location":"develop-ubuntu/r1.2%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#tensorflow-r12","text":"r1.1\u306e\u6642\u306egit\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u3042\u308b\u5834\u5408\u306ffetch/checkout\u3067\u66f4\u65b0\u3059\u308b \u7121\u3051\u308c\u3070git clone\u3067\u30b3\u30fc\u30c9\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068 1 2 3 cd /tensorflow git fetch git checkout r1.2 CPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5834\u5408\u3001 Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you wish to build TensorFlow with CUDA support? [y/N] No CUDA support will be enabled for TensorFlow OpenCL\u30b5\u30dd\u30fc\u30c8\uff1f\u306e\u6b21\u306bCUDA\u30b5\u30dd\u30fc\u30c8\uff1f\u304c\u6765\u308b\u3002CPU\u306a\u306e\u3067CUDA\u4f7f\u308f\u306a\u3044\u3067\u9032\u3080\u3002 \u3068\u3053\u308d\u304c\u3001GPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5834\u5408\u3001 No OpenCL support will be enabled for TensorFlow Do you want to use clang as CUDA compiler? [y/N] nvcc will be used as CUDA compiler OpenCL\u30b5\u30dd\u30fc\u30c8\uff1f\u306e\u6b21\u306b\u3001CUDA\u30b3\u30f3\u30d1\u30a4\u30e9\u306bclang\u4f7f\u3046\uff1f\u3068\u6765\u3066\u3044\u308b\u3002CUDA\u4f7f\u3046\u3053\u3068\u524d\u63d0\u3067\u8a71\u304c\u9032\u3093\u3067\u3044\u308b\u306e\u3067\u3001\u3053\u3053\u3067Y\u3092\u9078\u629e\u3057\u306a\u3044\u3088\u3046\u306b\u6ce8\u610f\uff01clang\u306f\u5341\u5206\u306a\u30c6\u30b9\u30c8\u304c\u3055\u308c\u3066\u3044\u306a\u3044\u3002 1 ./configure Please specify the location of python. [Default is /usr/bin/python]: Found possible Python library paths: /usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages] Using python library path: /usr/local/lib/python2.7/dist-packages Do you wish to build TensorFlow with MKL support? [y/N] Y MKL support will be enabled for TensorFlow Do you wish to download MKL LIB from the web? [Y/n] Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: Do you wish to use jemalloc as the malloc implementation? [Y/n] jemalloc enabled Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] No Google Cloud Platform support will be enabled for TensorFlow Do you wish to build TensorFlow with Hadoop File System support? [y/N] No Hadoop File System support will be enabled for TensorFlow Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] Y XLA JIT support will be enabled for TensorFlow Do you wish to build TensorFlow with VERBS support? [y/N] No VERBS support will be enabled for TensorFlow Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you want to use clang as CUDA compiler? [y/N] nvcc will be used as CUDA compiler Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: Please specify the location where CUDA toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: Please specify the cuDNN version you want to use. [Leave empty to use system default]: Please specify the location where cuDNN library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: Extracting Bazel installation... . INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes. Configuration finished 32\u30b3\u30a2\u306ep2.8xlarge\u306fTensorFlow\u30b3\u30f3\u30d1\u30a4\u30eb\u304c\u65e9\u3044\uff01 1 time bazel build -c opt --output_filter='^//tensorflow' --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package real 20m21.467s user 0m0.044s sys 0m0.144s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # whl\u3092\u751f\u6210 bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg # TensorFlow \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install /tmp/tensorflow_pkg/tensorflow-1.2.0-cp27-cp27mu-linux_x86_64.whl # .so\u3092\u4f5c\u6210 time bazel build -c opt --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so real 0m44.869s user 0m0.004s sys 0m0.052s # /usr/local/lib \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b install -m 0644 bazel-bin/tensorflow/libtensorflow_cc.so /usr/local/lib/ ldconfig # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb time bazel build -c opt //tensorflow/cc:tutorials_example_trainer real 4m12.963s user 0m0.012s sys 0m0.068s # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u5b9f\u884c ./bazel-bin/tensorflow/cc/tutorials_example_trainer","title":"TensorFlow r1.2 \u30b3\u30f3\u30d1\u30a4\u30eb"},{"location":"develop-ubuntu/r1.2%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#header","text":"vi ./tensorflow_header_install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash -eu # -*- coding: utf-8 -*- # http://memo.saitodev.com/home/tensorflow/build/ # tensorflow\u3092\u5229\u7528\u3059\u308bC++\u30bd\u30fc\u30b9\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\u969b\u306b\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30a4\u30f3\u30af\u30eb\u30fc\u30c9\u30d1\u30b9\u306b\u8ffd\u52a0\u3059\u308b\u3002 HEADER_DIR = /usr/local/tensorflow/include if [ ! -e $HEADER_DIR ] ; then mkdir -p $HEADER_DIR fi find tensorflow/core -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/cc -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/c -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find third_party/eigen3 -follow -type f -exec cp --parents {} $HEADER_DIR \\; pushd bazel-genfiles find tensorflow -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/protobuf/src find google -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/eigen_archive find Eigen -follow -type f -exec cp --parents {} $HEADER_DIR \\; find unsupported -follow -type f -exec cp --parents {} $HEADER_DIR \\; popd GPU\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\u306f\u3001bazel build\u30aa\u30d7\u30b7\u30e7\u30f3\u306e --config=cuda \u304c\u3042\u308b\u304b\u3069\u3046\u304b\u3060\u3051\u3002 configure\u3082\u6ce8\u610f\u304c\u5fc5\u8981\u3060\u3051\u3069\u3082\u3002","title":"header \u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/","text":"AWS Ubuntu 16.04 - TensorFlor r1.2.1 CPU/XLA/MKL/CPU\u6700\u9069\u5316 Python/C++ Python\u3068C++\u306e\u4e21\u65b9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059 \u666e\u6bb5\u4f7f\u3044\u306eAWS c4.xlarge\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u51e6\u7406\u6027\u80fd\u3092\u3042\u3052\u308b\u305f\u3081\u306b\u3001TensorFlow\u30d3\u30eb\u30c9\u306b\u6700\u9069\u5316\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u306b\u3002GPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u3082\u540c\u69d8\u306b\u51fa\u6765\u308b\u3051\u308c\u3069\u3082\u3001\u901f\u5ea6\u5411\u4e0a\u306f\u4f53\u611f\u3067\u304d\u306a\u304b\u3063\u305f\u3002\u3057\u304b\u3057\u3001CPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u306f\u9ad8\u901f\u5316\u304c\u51fa\u6765\u305f\u3002 SSD\u7269\u4f53\u8a8d\u8b58\u51e6\u7406 c4.8xlarge 36\u30b3\u30a2 1\u753b\u50cf\u51e6\u7406 CPU\u6700\u9069\u5316\u306a\u3057\uff1atime\\:0.25347185 clock\\:7.83642500 CPU\u6700\u9069\u5316\u3042\u308a\uff1atime\\:0.17184091 clock\\:4.64863300 c4.xlarge 4\u30b3\u30a2 1\u753b\u50cf\u51e6\u7406 CPU\u6700\u9069\u5316\u306a\u3057\uff1atime\\:1.50084400 clock\\:5.85320900 CPU\u6700\u9069\u5316\u3042\u308a\uff1atime\\:0.54308510 clock\\:2.02857800 CPU\u6700\u9069\u5316 \u30d3\u30eb\u30c9\u306b\u4f7f\u3063\u305f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 c4.8xlarge bazel build bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures \\/\\/tensorflow\\/tools\\/pip_package:build_pip_package \u5229\u7528\u53ef\u80fd\u306a\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u63a2\u3057\u65b9 TesnorFlow\u306e\u30b3\u30fc\u30c9\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u5b9f\u884c\u3059\u308b\u969b\u306b\u3067\u308bAVX/SSE4.1\u304c\u4f7f\u3048\u308b\u3068\u3044\u3046\u60c5\u5831\u3092\u5143\u306b\u6c7a\u3081\u3066\u3082\u3044\u3044\u3002CPU\u304c\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u306f 1 cat /proc/cpuinfo flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq monitor est ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt ida \u3092\u898b\u3066\u78ba\u8a8d\u3059\u308b\u3002 \u3053\u3053\u304b\u3089gcc\u3067\u6307\u5b9a\u3067\u304d\u308b\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3059\u308b\u3002 1 gcc -march=native -Q --help=target | grep enable -m64 [enabled] -m80387 [enabled] -m96bit-long-double [enabled] -mabm [enabled] -maes [enabled] -malign-stringops [enabled] -mavx [enabled] -mavx2 [enabled] -mbmi [enabled] -mbmi2 [enabled] -mcx16 [enabled] -mf16c [enabled] -mfancy-math-387 [enabled] -mfentry [enabled] -mfma [enabled] -mfp-ret-in-387 [enabled] -mfsgsbase [enabled] -mfxsr [enabled] -mglibc [enabled] -mhard-float [enabled] -mhle [enabled] -mieee-fp [enabled] -mlong-double-80 [enabled] -mlzcnt [enabled] -mmmx [enabled] -mmovbe [enabled] -mpclmul [enabled] -mpopcnt [enabled] -mpush-args [enabled] -mrdrnd [enabled] -mred-zone [enabled] -mrtm [enabled] -msahf [enabled] -msse [enabled] -msse2 [enabled] -msse3 [enabled] -msse4 [enabled] -msse4.1 [enabled] -msse4.2 [enabled] -mssse3 [enabled] -mstackrealign [enabled] -mtls-direct-seg-refs [enabled] -mxsave [enabled] -mxsaveopt [enabled] r1.2\u304b\u3089\u306e\u66f4\u65b0\u306e\u5834\u5408\u3001header\u30d5\u30a1\u30a4\u30eb\u30b3\u30d4\u30fc\u30b9\u30af\u30ea\u30d7\u30c8\u3067 1 find : File system loop detected ; 'third_party/eigen3/mkl_include/include' is part of the same file system loop as 'third_party/eigen3/mkl_include' . \u3068\u51fa\u308b\u305f\u3081\u3001header\u30d5\u30a1\u30a4\u30eb\u30b3\u30d4\u30fc\u3059\u308b\u969b\u306b\u306f 1 rm /tensorflow/third_party/mkl/mklml_lnx_2018.0.20170425/include/include \u3092\u5b9f\u884c\u3057\u3001\u539f\u56e0\u3068\u306a\u3063\u3066\u3044\u308b\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u524a\u9664\u3059\u308b \u3042\u3068\u306f\u901a\u5e38\u306eTensorFlow\u30d3\u30eb\u30c9 Docker\u306fr.1.1\u306e\u6642\u306eDocker\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 \u4eca\u56de\u306f\u74b0\u5883\u306fr1.1\u53c2\u8003\u306b\u3002 \u66f4\u65b0\u306f\u30d3\u30eb\u30c9\u95a2\u9023\u90e8\u5206\u3060\u3051\u3002 \u4eca\u56de\u30d3\u30eb\u30c9\u306b\u306fc4.8xlarge Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz 36\u30b3\u30a2\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 MKL\u5bfe\u5fdcCPU\u306f\u3001 https://github.com/01org/mkl-dnn Intel MKL-DNN supports Intel(R) 64 architecture processors and is optimized for 1 2 3 4 5 6 7 Intel Atom(R) processor with Intel(R) SSE4.1 support 4th, 5th, 6th and 7th generation Intel(R) Core processor Intel(R) Xeon(R) processor E5 v3 family (code named Haswell) Intel(R) Xeon(R) processor E5 v4 family (code named Broadwell) Intel(R) Xeon(R) Platinum processor family (code name Skylake) Intel(R) Xeon Phi(TM) product family x200 (code named Knights Landing) Future Intel(R) Xeon Phi(TM) processor (code named Knights Mill) \u3068\u306a\u3063\u3066\u3044\u3066\u3001c4.xlarge\u3068c4.8xlarge\u306f\u5171\u306bHaswell\u306a\u306e\u3067MKL\u3092\u5229\u7528\u3067\u304d\u308b\u3002 \u4f5c\u696d\u306f\u3059\u3079\u3066Docker\u30b3\u30f3\u30c6\u30ca\u5185\u3067\u884c\u3046 1 2 apt-get update apt-get dist-upgrade Docker\u30b3\u30f3\u30c6\u30ca\u518d\u8d77\u52d5 Intel Math Kernel Library MKL\u3092\u6709\u52b9\u306b\u3059\u308b\u3068\u3001configure\u3067locate\u3092\u4f7f\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u691c\u7d22\u304c\u5b9f\u884c\u3055\u308c\u308b\u3002Docker\u306blocate\u304c\u7121\u3044\u306e\u3067\u3001apt-get install locate\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304c\u3001locate\u306fupdatedb\u3092\u5b9f\u884c\u3057\u3066DB\u306b\u8a18\u9332\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u691c\u7d22\u3059\u308b\u305f\u3081\u3001updatedb\u306e\u5b9f\u884c\u3082\u5fc5\u8981\u306b\u306a\u308b TensorFlow\u306econfigure\u6642\u306bMKLML\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u305f\u3081\u3001Intel\u304b\u3089MKL\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9/\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u306f\u306a\u3044\u3002\u3053\u308c\u305f\u3076\u3093\u5225\u7269\u3002 1 2 apt-get install locate cpio updatedb updatedb\u3067locate\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9DB\u3092\u66f4\u65b0\u3059\u308b bazel\u306f\u30d3\u30eb\u30c9\u3057\u76f4\u3059 1 2 3 4 5 6 7 8 9 10 ######################################## # bazel - 0 . 5 . 1 \u30d3\u30eb\u30c9 ######################################## cd / bazel wget --no-check-certificate https://github.com/bazelbuild/bazel/releases/download/0.5.1/bazel-0.5.1-dist.zip unzip bazel - 0 . 5 . 1 - dist . zip - d bazel - 0 . 5 . 1 cd bazel - 0 . 5 . 1 time . / compile . sh cp output / bazel / usr / local / bin / TensorFlow r1.2.1 \u30b3\u30f3\u30d1\u30a4\u30eb r1.1\u306e\u6642\u306egit\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u3042\u308b\u5834\u5408\u306ffetch/checkout\u3067\u66f4\u65b0\u3059\u308b \u7121\u3051\u308c\u3070git clone\u3067\u30b3\u30fc\u30c9\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068 1 2 3 4 cd /tensorflow git fetch git checkout r1.2 git pull CPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5834\u5408\u3001 Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you wish to build TensorFlow with CUDA support? [y/N] No CUDA support will be enabled for TensorFlow OpenCL\u30b5\u30dd\u30fc\u30c8\uff1f\u306e\u6b21\u306bCUDA\u30b5\u30dd\u30fc\u30c8\uff1f\u304c\u6765\u308b\u3002CPU\u306a\u306e\u3067CUDA\u4f7f\u308f\u306a\u3044\u3067\u9032\u3080\u3002 1 ./configure Please specify the location of python. [Default is /usr/bin/python]: Found possible Python library paths: /usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages] Using python library path: /usr/local/lib/python2.7/dist-packages Do you wish to build TensorFlow with MKL support? [y/N] Y MKL support will be enabled for TensorFlow Do you wish to download MKL LIB from the web? [Y/n] Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: Do you wish to use jemalloc as the malloc implementation? [Y/n] jemalloc enabled Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] No Google Cloud Platform support will be enabled for TensorFlow Do you wish to build TensorFlow with Hadoop File System support? [y/N] No Hadoop File System support will be enabled for TensorFlow Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] Y XLA JIT support will be enabled for TensorFlow Do you wish to build TensorFlow with VERBS support? [y/N] No VERBS support will be enabled for TensorFlow Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you wish to build TensorFlow with CUDA support? [y/N] No CUDA support will be enabled for TensorFlow ................ INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes. Configuration finished 1 time bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures //tensorflow/tools/pip_package:build_pip_package 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # whl\u3092\u751f\u6210 bazel - bin / tensorflow / tools / pip_package / build_pip_package / tmp / tensorflow_pkg # TensorFlow \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install / tmp / tensorflow_pkg / tensorflow - 1 . 2 . 1 - cp27 - cp27mu - linux_x86_64 . whl # . so\u3092\u4f5c\u6210 time bazel build - c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures //tensorflow:libtensorflow_cc.so # / usr / local / lib \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b install - m 0644 bazel - bin / tensorflow / libtensorflow_cc . so / usr / local / lib / ldconfig # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb time bazel build - c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures //tensorflow/cc:tutorials_example_trainer # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u5b9f\u884c . / bazel - bin / tensorflow / cc / tutorials_example_trainer header \u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b vi ./tensorflow_header_install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash -eu # -*- coding: utf-8 -*- # http://memo.saitodev.com/home/tensorflow/build/ # tensorflow\u3092\u5229\u7528\u3059\u308bC++\u30bd\u30fc\u30b9\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\u969b\u306b\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30a4\u30f3\u30af\u30eb\u30fc\u30c9\u30d1\u30b9\u306b\u8ffd\u52a0\u3059\u308b\u3002 HEADER_DIR = /usr/local/tensorflow/include if [ ! -e $HEADER_DIR ] ; then mkdir -p $HEADER_DIR fi find tensorflow/core -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/cc -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/c -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find third_party/eigen3 -follow -type f -exec cp --parents {} $HEADER_DIR \\; pushd bazel-genfiles find tensorflow -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/protobuf/src find google -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/eigen_archive find Eigen -follow -type f -exec cp --parents {} $HEADER_DIR \\; find unsupported -follow -type f -exec cp --parents {} $HEADER_DIR \\; popd","title":"AWS Ubuntu 16.04 - TensorFlor r1.2.1 CPU/XLA/MKL/CPU\u6700\u9069\u5316 Python/C++"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#aws-ubuntu-1604-tensorflor-r121-cpuxlamklcpu-pythonc","text":"Python\u3068C++\u306e\u4e21\u65b9\u3092\u4f7f\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059 \u666e\u6bb5\u4f7f\u3044\u306eAWS c4.xlarge\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u51e6\u7406\u6027\u80fd\u3092\u3042\u3052\u308b\u305f\u3081\u306b\u3001TensorFlow\u30d3\u30eb\u30c9\u306b\u6700\u9069\u5316\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u306b\u3002GPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u3082\u540c\u69d8\u306b\u51fa\u6765\u308b\u3051\u308c\u3069\u3082\u3001\u901f\u5ea6\u5411\u4e0a\u306f\u4f53\u611f\u3067\u304d\u306a\u304b\u3063\u305f\u3002\u3057\u304b\u3057\u3001CPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3067\u306f\u9ad8\u901f\u5316\u304c\u51fa\u6765\u305f\u3002","title":"AWS Ubuntu 16.04 - TensorFlor r1.2.1 CPU/XLA/MKL/CPU\u6700\u9069\u5316 Python/C++"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#ssd","text":"c4.8xlarge 36\u30b3\u30a2 1\u753b\u50cf\u51e6\u7406 CPU\u6700\u9069\u5316\u306a\u3057\uff1atime\\:0.25347185 clock\\:7.83642500 CPU\u6700\u9069\u5316\u3042\u308a\uff1atime\\:0.17184091 clock\\:4.64863300 c4.xlarge 4\u30b3\u30a2 1\u753b\u50cf\u51e6\u7406 CPU\u6700\u9069\u5316\u306a\u3057\uff1atime\\:1.50084400 clock\\:5.85320900 CPU\u6700\u9069\u5316\u3042\u308a\uff1atime\\:0.54308510 clock\\:2.02857800","title":"SSD\u7269\u4f53\u8a8d\u8b58\u51e6\u7406"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#cpu","text":"\u30d3\u30eb\u30c9\u306b\u4f7f\u3063\u305f\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 c4.8xlarge bazel build bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures \\/\\/tensorflow\\/tools\\/pip_package:build_pip_package","title":"CPU\u6700\u9069\u5316"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#_1","text":"TesnorFlow\u306e\u30b3\u30fc\u30c9\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u3067\u5b9f\u884c\u3059\u308b\u969b\u306b\u3067\u308bAVX/SSE4.1\u304c\u4f7f\u3048\u308b\u3068\u3044\u3046\u60c5\u5831\u3092\u5143\u306b\u6c7a\u3081\u3066\u3082\u3044\u3044\u3002CPU\u304c\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u306f 1 cat /proc/cpuinfo flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq monitor est ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveopt ida \u3092\u898b\u3066\u78ba\u8a8d\u3059\u308b\u3002 \u3053\u3053\u304b\u3089gcc\u3067\u6307\u5b9a\u3067\u304d\u308b\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u6307\u5b9a\u3059\u308b\u3002 1 gcc -march=native -Q --help=target | grep enable -m64 [enabled] -m80387 [enabled] -m96bit-long-double [enabled] -mabm [enabled] -maes [enabled] -malign-stringops [enabled] -mavx [enabled] -mavx2 [enabled] -mbmi [enabled] -mbmi2 [enabled] -mcx16 [enabled] -mf16c [enabled] -mfancy-math-387 [enabled] -mfentry [enabled] -mfma [enabled] -mfp-ret-in-387 [enabled] -mfsgsbase [enabled] -mfxsr [enabled] -mglibc [enabled] -mhard-float [enabled] -mhle [enabled] -mieee-fp [enabled] -mlong-double-80 [enabled] -mlzcnt [enabled] -mmmx [enabled] -mmovbe [enabled] -mpclmul [enabled] -mpopcnt [enabled] -mpush-args [enabled] -mrdrnd [enabled] -mred-zone [enabled] -mrtm [enabled] -msahf [enabled] -msse [enabled] -msse2 [enabled] -msse3 [enabled] -msse4 [enabled] -msse4.1 [enabled] -msse4.2 [enabled] -mssse3 [enabled] -mstackrealign [enabled] -mtls-direct-seg-refs [enabled] -mxsave [enabled] -mxsaveopt [enabled]","title":"\u5229\u7528\u53ef\u80fd\u306a\u30aa\u30d7\u30b7\u30e7\u30f3\u306e\u63a2\u3057\u65b9"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#r12header","text":"1 find : File system loop detected ; 'third_party/eigen3/mkl_include/include' is part of the same file system loop as 'third_party/eigen3/mkl_include' . \u3068\u51fa\u308b\u305f\u3081\u3001header\u30d5\u30a1\u30a4\u30eb\u30b3\u30d4\u30fc\u3059\u308b\u969b\u306b\u306f 1 rm /tensorflow/third_party/mkl/mklml_lnx_2018.0.20170425/include/include \u3092\u5b9f\u884c\u3057\u3001\u539f\u56e0\u3068\u306a\u3063\u3066\u3044\u308b\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u524a\u9664\u3059\u308b","title":"r1.2\u304b\u3089\u306e\u66f4\u65b0\u306e\u5834\u5408\u3001header\u30d5\u30a1\u30a4\u30eb\u30b3\u30d4\u30fc\u30b9\u30af\u30ea\u30d7\u30c8\u3067"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#tensorflow","text":"Docker\u306fr.1.1\u306e\u6642\u306eDocker\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 \u4eca\u56de\u306f\u74b0\u5883\u306fr1.1\u53c2\u8003\u306b\u3002 \u66f4\u65b0\u306f\u30d3\u30eb\u30c9\u95a2\u9023\u90e8\u5206\u3060\u3051\u3002 \u4eca\u56de\u30d3\u30eb\u30c9\u306b\u306fc4.8xlarge Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz 36\u30b3\u30a2\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 MKL\u5bfe\u5fdcCPU\u306f\u3001 https://github.com/01org/mkl-dnn Intel MKL-DNN supports Intel(R) 64 architecture processors and is optimized for 1 2 3 4 5 6 7 Intel Atom(R) processor with Intel(R) SSE4.1 support 4th, 5th, 6th and 7th generation Intel(R) Core processor Intel(R) Xeon(R) processor E5 v3 family (code named Haswell) Intel(R) Xeon(R) processor E5 v4 family (code named Broadwell) Intel(R) Xeon(R) Platinum processor family (code name Skylake) Intel(R) Xeon Phi(TM) product family x200 (code named Knights Landing) Future Intel(R) Xeon Phi(TM) processor (code named Knights Mill) \u3068\u306a\u3063\u3066\u3044\u3066\u3001c4.xlarge\u3068c4.8xlarge\u306f\u5171\u306bHaswell\u306a\u306e\u3067MKL\u3092\u5229\u7528\u3067\u304d\u308b\u3002 \u4f5c\u696d\u306f\u3059\u3079\u3066Docker\u30b3\u30f3\u30c6\u30ca\u5185\u3067\u884c\u3046 1 2 apt-get update apt-get dist-upgrade Docker\u30b3\u30f3\u30c6\u30ca\u518d\u8d77\u52d5 Intel Math Kernel Library MKL\u3092\u6709\u52b9\u306b\u3059\u308b\u3068\u3001configure\u3067locate\u3092\u4f7f\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u691c\u7d22\u304c\u5b9f\u884c\u3055\u308c\u308b\u3002Docker\u306blocate\u304c\u7121\u3044\u306e\u3067\u3001apt-get install locate\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304c\u3001locate\u306fupdatedb\u3092\u5b9f\u884c\u3057\u3066DB\u306b\u8a18\u9332\u3055\u308c\u305f\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\u691c\u7d22\u3059\u308b\u305f\u3081\u3001updatedb\u306e\u5b9f\u884c\u3082\u5fc5\u8981\u306b\u306a\u308b TensorFlow\u306econfigure\u6642\u306bMKLML\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u305f\u3081\u3001Intel\u304b\u3089MKL\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9/\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u304f\u5fc5\u8981\u306f\u306a\u3044\u3002\u3053\u308c\u305f\u3076\u3093\u5225\u7269\u3002 1 2 apt-get install locate cpio updatedb updatedb\u3067locate\u306e\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9DB\u3092\u66f4\u65b0\u3059\u308b bazel\u306f\u30d3\u30eb\u30c9\u3057\u76f4\u3059 1 2 3 4 5 6 7 8 9 10 ######################################## # bazel - 0 . 5 . 1 \u30d3\u30eb\u30c9 ######################################## cd / bazel wget --no-check-certificate https://github.com/bazelbuild/bazel/releases/download/0.5.1/bazel-0.5.1-dist.zip unzip bazel - 0 . 5 . 1 - dist . zip - d bazel - 0 . 5 . 1 cd bazel - 0 . 5 . 1 time . / compile . sh cp output / bazel / usr / local / bin /","title":"\u3042\u3068\u306f\u901a\u5e38\u306eTensorFlow\u30d3\u30eb\u30c9"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#tensorflow-r121","text":"r1.1\u306e\u6642\u306egit\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u3042\u308b\u5834\u5408\u306ffetch/checkout\u3067\u66f4\u65b0\u3059\u308b \u7121\u3051\u308c\u3070git clone\u3067\u30b3\u30fc\u30c9\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068 1 2 3 4 cd /tensorflow git fetch git checkout r1.2 git pull CPU\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u5834\u5408\u3001 Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you wish to build TensorFlow with CUDA support? [y/N] No CUDA support will be enabled for TensorFlow OpenCL\u30b5\u30dd\u30fc\u30c8\uff1f\u306e\u6b21\u306bCUDA\u30b5\u30dd\u30fc\u30c8\uff1f\u304c\u6765\u308b\u3002CPU\u306a\u306e\u3067CUDA\u4f7f\u308f\u306a\u3044\u3067\u9032\u3080\u3002 1 ./configure Please specify the location of python. [Default is /usr/bin/python]: Found possible Python library paths: /usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages] Using python library path: /usr/local/lib/python2.7/dist-packages Do you wish to build TensorFlow with MKL support? [y/N] Y MKL support will be enabled for TensorFlow Do you wish to download MKL LIB from the web? [Y/n] Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: Do you wish to use jemalloc as the malloc implementation? [Y/n] jemalloc enabled Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] No Google Cloud Platform support will be enabled for TensorFlow Do you wish to build TensorFlow with Hadoop File System support? [y/N] No Hadoop File System support will be enabled for TensorFlow Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] Y XLA JIT support will be enabled for TensorFlow Do you wish to build TensorFlow with VERBS support? [y/N] No VERBS support will be enabled for TensorFlow Do you wish to build TensorFlow with OpenCL support? [y/N] No OpenCL support will be enabled for TensorFlow Do you wish to build TensorFlow with CUDA support? [y/N] No CUDA support will be enabled for TensorFlow ................ INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes. Configuration finished 1 time bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures //tensorflow/tools/pip_package:build_pip_package 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # whl\u3092\u751f\u6210 bazel - bin / tensorflow / tools / pip_package / build_pip_package / tmp / tensorflow_pkg # TensorFlow \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb pip install / tmp / tensorflow_pkg / tensorflow - 1 . 2 . 1 - cp27 - cp27mu - linux_x86_64 . whl # . so\u3092\u4f5c\u6210 time bazel build - c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures //tensorflow:libtensorflow_cc.so # / usr / local / lib \u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b install - m 0644 bazel - bin / tensorflow / libtensorflow_cc . so / usr / local / lib / ldconfig # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u30b3\u30f3\u30d1\u30a4\u30eb time bazel build - c opt --copt=-mavx --copt=-mavx2 --copt=-mfxsr --copt=-mxsave --copt=-mxsaveopt --copt=-mbmi --copt=-mbmi2 --copt=-mcx16 --copt=-maes --copt=-mmmx --copt=-mabm --copt=-msse --copt=-msse2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mssse3 --copt=-mf16c --copt=-mfma -k --verbose_failures //tensorflow/cc:tutorials_example_trainer # \u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u5b9f\u884c . / bazel - bin / tensorflow / cc / tutorials_example_trainer","title":"TensorFlow r1.2.1 \u30b3\u30f3\u30d1\u30a4\u30eb"},{"location":"develop-ubuntu/r1.2.1%2Bc%2B%2B/aws-ec2-docker-git-c%2B%2B/#header","text":"vi ./tensorflow_header_install.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash -eu # -*- coding: utf-8 -*- # http://memo.saitodev.com/home/tensorflow/build/ # tensorflow\u3092\u5229\u7528\u3059\u308bC++\u30bd\u30fc\u30b9\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\u969b\u306b\u3001\u3053\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30a4\u30f3\u30af\u30eb\u30fc\u30c9\u30d1\u30b9\u306b\u8ffd\u52a0\u3059\u308b\u3002 HEADER_DIR = /usr/local/tensorflow/include if [ ! -e $HEADER_DIR ] ; then mkdir -p $HEADER_DIR fi find tensorflow/core -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/cc -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find tensorflow/c -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; find third_party/eigen3 -follow -type f -exec cp --parents {} $HEADER_DIR \\; pushd bazel-genfiles find tensorflow -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/protobuf/src find google -follow -type f -name \"*.h\" -exec cp --parents {} $HEADER_DIR \\; popd pushd bazel-tensorflow/external/eigen_archive find Eigen -follow -type f -exec cp --parents {} $HEADER_DIR \\; find unsupported -follow -type f -exec cp --parents {} $HEADER_DIR \\; popd","title":"header \u30d5\u30a1\u30a4\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b"},{"location":"end2end/dev/","text":"\u958b\u767a\u74b0\u5883 TensorFlow\u7b49\u306e\u5b9f\u884c\u74b0\u5883\u306f\u3001Docker\u4e0a\u306b\u3001TensorFlow + Jupyter\u3067Local\u306b\u69cb\u7bc9\u3057\u307e\u3059\u3002 Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://www.docker.com/ Docker Community Edition for Mac https://store.docker.com/editions/community/docker-ce-desktop-mac Docker Community Edition for Windows https://store.docker.com/editions/community/docker-ce-desktop-windows \u3092\u5404\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 TensorFlow Docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5b8c\u4e86\u3057\u305f\u3089\u3001\u30de\u30a6\u30b9\u306eDouble\u30af\u30ea\u30c3\u30af\u3067Docker\u3092\u8d77\u52d5\u3057\u3001Docker\u30b3\u30de\u30f3\u30c9\u3067TensorFlow Docker\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304d\u307e\u3059\u3002 $ docker run -it -p 8888:8888 tensorflow/tensorflow 1 2 3 4 5 [ C 16 : 50 : 37 . 847 NotebookApp ] Copy / paste this URL into your browser when you connect for the first time , to login with a token : http : // localhost : 8888 /? token =######################### localhost:8888\u306b\u63a5\u7d9a\u3002\u4e0a\u306e\u30ea\u30f3\u30af\u3092\u30b3\u30d4\u30fc\u3057\u3066\u63a5\u7d9a\u3059\u308b\u3002","title":"\u958b\u767a\u74b0\u5883"},{"location":"end2end/dev/#_1","text":"TensorFlow\u7b49\u306e\u5b9f\u884c\u74b0\u5883\u306f\u3001Docker\u4e0a\u306b\u3001TensorFlow + Jupyter\u3067Local\u306b\u69cb\u7bc9\u3057\u307e\u3059\u3002","title":"\u958b\u767a\u74b0\u5883"},{"location":"end2end/dev/#docker","text":"https://www.docker.com/ Docker Community Edition for Mac https://store.docker.com/editions/community/docker-ce-desktop-mac Docker Community Edition for Windows https://store.docker.com/editions/community/docker-ce-desktop-windows \u3092\u5404\u74b0\u5883\u306b\u5408\u308f\u305b\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002","title":"Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"end2end/dev/#tensorflow-docker","text":"Docker\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u304c\u5b8c\u4e86\u3057\u305f\u3089\u3001\u30de\u30a6\u30b9\u306eDouble\u30af\u30ea\u30c3\u30af\u3067Docker\u3092\u8d77\u52d5\u3057\u3001Docker\u30b3\u30de\u30f3\u30c9\u3067TensorFlow Docker\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u304d\u307e\u3059\u3002 $ docker run -it -p 8888:8888 tensorflow/tensorflow 1 2 3 4 5 [ C 16 : 50 : 37 . 847 NotebookApp ] Copy / paste this URL into your browser when you connect for the first time , to login with a token : http : // localhost : 8888 /? token =######################### localhost:8888\u306b\u63a5\u7d9a\u3002\u4e0a\u306e\u30ea\u30f3\u30af\u3092\u30b3\u30d4\u30fc\u3057\u3066\u63a5\u7d9a\u3059\u308b\u3002","title":"TensorFlow Docker\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"end2end/dev_mac/","text":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC) pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow \u5b66\u7fd2\u6e08\u307fModel\u306eDownload \u5b9f\u884c\u30bd\u30fc\u30b9 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio . on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio . on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app ) \u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5 AI\u306e\u5b9f\u884c python drive.py model.json","title":"Dev mac"},{"location":"end2end/dev_mac/#localmac","text":"pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow","title":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC)"},{"location":"end2end/dev_mac/#modeldownload","text":"","title":"\u5b66\u7fd2\u6e08\u307fModel\u306eDownload"},{"location":"end2end/dev_mac/#_1","text":"https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio . on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio . on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app )","title":"\u5b9f\u884c\u30bd\u30fc\u30b9"},{"location":"end2end/dev_mac/#_2","text":"","title":"\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5"},{"location":"end2end/dev_mac/#ai","text":"python drive.py model.json","title":"AI\u306e\u5b9f\u884c"},{"location":"end2end/emulator/","text":"Emulator \u4e0b\u8a18\u30ec\u30dd\u30b8\u30c8\u30ea\u3088\u308a\u3001\u5404\u74b0\u5883\u306e\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars Dataset\u306e\u4f5c\u6210 Emulator\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001Training Mode\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d5\u30a9\u30eb\u30c0\u3092\u9078\u629e\u3059\u308b\u753b\u9762\u304c\u3067\u3066\u304d\u307e\u3059\u3002 \u518d\u3073\u3001Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u304c\u59cb\u307e\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u4e2d\u306f\u3001Recornding\u306e\u8a18\u8ff0\u306b\u304b\u308f\u308a\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u3092\u7d42\u308f\u308b\u5834\u5408\u306f\u3001Stop\u3092\u62bc\u3057\u307e\u3059\u3002 \u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8 \u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002 driving_log.csv center,left,right,steering,throttle,brake,speed 1 2 3 4 5 center,left,right,steering,throttle,brake,speed IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, 0 , 0 , 0 , 22.14829 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, 0 , 0 , 0 , 21.87963 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, 0 , 0 , 0 , 1.453011 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, 0 , 0 , 0 , 1.438419 IMG IMG\u30d5\u30a9\u30eb\u30c0\u306b\u306f\u3001\u53f3\u3001\u4e2d\u592e\u3001\u5de6\u306e\u753b\u50cf\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u3002","title":"Emulator"},{"location":"end2end/emulator/#emulator","text":"\u4e0b\u8a18\u30ec\u30dd\u30b8\u30c8\u30ea\u3088\u308a\u3001\u5404\u74b0\u5883\u306e\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3002 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars","title":"Emulator"},{"location":"end2end/emulator/#dataset","text":"Emulator\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u306b\u306f\u3001Training Mode\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002 Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d5\u30a9\u30eb\u30c0\u3092\u9078\u629e\u3059\u308b\u753b\u9762\u304c\u3067\u3066\u304d\u307e\u3059\u3002 \u518d\u3073\u3001Record\u3092\u62bc\u3059\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u304c\u59cb\u307e\u308a\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u4e2d\u306f\u3001Recornding\u306e\u8a18\u8ff0\u306b\u304b\u308f\u308a\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u3092\u7d42\u308f\u308b\u5834\u5408\u306f\u3001Stop\u3092\u62bc\u3057\u307e\u3059\u3002","title":"Dataset\u306e\u4f5c\u6210"},{"location":"end2end/emulator/#_1","text":"\u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30d5\u30a9\u30eb\u30c0\u306b\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002","title":"\u4f5c\u6210\u3055\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8"},{"location":"end2end/emulator/#driving_logcsv","text":"center,left,right,steering,throttle,brake,speed 1 2 3 4 5 center,left,right,steering,throttle,brake,speed IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 287 .jpg, 0 , 0 , 0 , 22.14829 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 30 _ 48 _ 404 .jpg, 0 , 0 , 0 , 21.87963 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 12 _ 937 .jpg, 0 , 0 , 0 , 1.453011 IMG/center_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/left_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, IMG/right_ 2016 _ 12 _ 01 _ 13 _ 31 _ 13 _ 037 .jpg, 0 , 0 , 0 , 1.438419","title":"driving_log.csv"},{"location":"end2end/emulator/#img","text":"IMG\u30d5\u30a9\u30eb\u30c0\u306b\u306f\u3001\u53f3\u3001\u4e2d\u592e\u3001\u5de6\u306e\u753b\u50cf\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u304d\u307e\u3059\u3002","title":"IMG"},{"location":"end2end/link/","text":"\u30ea\u30f3\u30af","title":"\u30ea\u30f3\u30af"},{"location":"end2end/link/#_1","text":"","title":"\u30ea\u30f3\u30af"},{"location":"end2end/model/","text":"Model Notebook\u306e\u4f5c\u6210 Dataset\u306eUpload Dataset\u306e\u89e3\u51cd Model\u306e\u5b9f\u884c https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/model.py \u3088\u308a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 #import modules import numpy as np import keras import csv import cv2 from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from keras.models import * from keras.layers import * from keras.optimizers import Adam import matplotlib.pyplot as plt #define the file direcory features_directory = './' labels_file = './driving_log.csv' #image size after resize rows = 16 cols = 32 #proprocess: change to HSV space and resize def preprocess ( img ): resized = cv2 . resize (( cv2 . cvtColor ( img , cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( cols , rows )) return resized #the function to save model def model_save ( model_json , model_h5 ): json_model = model . to_json () with open ( model_json , \"w\" ) as f : f . write ( json_model ) model . save_weights ( model_h5 ) #load the left center and right camera data, shift (-+)delta for left and right camear def data_loading ( delta ): logs = [] features = [] labels = [] with open ( labels_file , 'rt' ) as f : reader = csv . reader ( f ) for line in reader : logs . append ( line ) log_labels = logs . pop ( 0 ) for i in range ( len ( logs )): for j in range ( 3 ): img_path = logs [ i ][ j ] img_path = features_directory + 'IMG' + ( img_path . split ( 'IMG' )[ 1 ]) . strip () img = plt . imread ( img_path ) features . append ( preprocess ( img )) if j == 0 : labels . append ( float ( logs [ i ][ 3 ])) elif j == 1 : labels . append ( float ( logs [ i ][ 3 ]) + delta ) else : labels . append ( float ( logs [ i ][ 3 ]) - delta ) return features , labels #load the data and transform to numpy array #very important parameter, defining the shift variable for left and righ steering angle delta = 0.2 features , labels = data_loading ( delta ) features = np . array ( features ) . astype ( 'float32' ) labels = np . array ( labels ) . astype ( 'float32' ) print ( features . shape ) #augment the data by horizontal flipping the image features = np . append ( features , features [:,:,:: - 1 ], axis = 0 ) labels = np . append ( labels , - labels , axis = 0 ) # shuffle the data and split to train and validation features , labels = shuffle ( features , labels ) train_features , test_features , train_labels , test_labels = train_test_split ( features , labels , random_state = 0 , test_size = 0.1 ) #reshape the data to feed into the network train_features = train_features . reshape ( train_features . shape [ 0 ], rows , cols , 1 ) test_features = test_features . reshape ( test_features . shape [ 0 ], rows , cols , 1 ) #define the model def steering_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( 16 , 32 , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model #optimize model = steering_model () adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( train_features , train_labels , batch_size = 128 , nb_epoch = 10 , verbose = 1 , validation_data = ( test_features , test_labels )) #save the model architecture and parameters model_json = './model.json' model_h5 = './model.h5' model_save ( model_json , model_h5 ) \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb model.json, model.h5\u304c\u3067\u304d\u308c\u3070\u6210\u529f","title":"Model"},{"location":"end2end/model/#model","text":"","title":"Model"},{"location":"end2end/model/#notebook","text":"","title":"Notebook\u306e\u4f5c\u6210"},{"location":"end2end/model/#datasetupload","text":"","title":"Dataset\u306eUpload"},{"location":"end2end/model/#dataset","text":"","title":"Dataset\u306e\u89e3\u51cd"},{"location":"end2end/model/#model_1","text":"https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/model.py \u3088\u308a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 #import modules import numpy as np import keras import csv import cv2 from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from keras.models import * from keras.layers import * from keras.optimizers import Adam import matplotlib.pyplot as plt #define the file direcory features_directory = './' labels_file = './driving_log.csv' #image size after resize rows = 16 cols = 32 #proprocess: change to HSV space and resize def preprocess ( img ): resized = cv2 . resize (( cv2 . cvtColor ( img , cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( cols , rows )) return resized #the function to save model def model_save ( model_json , model_h5 ): json_model = model . to_json () with open ( model_json , \"w\" ) as f : f . write ( json_model ) model . save_weights ( model_h5 ) #load the left center and right camera data, shift (-+)delta for left and right camear def data_loading ( delta ): logs = [] features = [] labels = [] with open ( labels_file , 'rt' ) as f : reader = csv . reader ( f ) for line in reader : logs . append ( line ) log_labels = logs . pop ( 0 ) for i in range ( len ( logs )): for j in range ( 3 ): img_path = logs [ i ][ j ] img_path = features_directory + 'IMG' + ( img_path . split ( 'IMG' )[ 1 ]) . strip () img = plt . imread ( img_path ) features . append ( preprocess ( img )) if j == 0 : labels . append ( float ( logs [ i ][ 3 ])) elif j == 1 : labels . append ( float ( logs [ i ][ 3 ]) + delta ) else : labels . append ( float ( logs [ i ][ 3 ]) - delta ) return features , labels #load the data and transform to numpy array #very important parameter, defining the shift variable for left and righ steering angle delta = 0.2 features , labels = data_loading ( delta ) features = np . array ( features ) . astype ( 'float32' ) labels = np . array ( labels ) . astype ( 'float32' ) print ( features . shape ) #augment the data by horizontal flipping the image features = np . append ( features , features [:,:,:: - 1 ], axis = 0 ) labels = np . append ( labels , - labels , axis = 0 ) # shuffle the data and split to train and validation features , labels = shuffle ( features , labels ) train_features , test_features , train_labels , test_labels = train_test_split ( features , labels , random_state = 0 , test_size = 0.1 ) #reshape the data to feed into the network train_features = train_features . reshape ( train_features . shape [ 0 ], rows , cols , 1 ) test_features = test_features . reshape ( test_features . shape [ 0 ], rows , cols , 1 ) #define the model def steering_model (): model = Sequential () model . add ( Lambda ( lambda x : x / 127.5 - 1. , input_shape = ( 16 , 32 , 1 ))) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Convolution2D ( 8 , 3 , 3 , init = 'normal' , border_mode = 'valid' )) model . add ( Activation ( 'relu' )) model . add ( MaxPooling2D (( 2 , 2 ), border_mode = 'valid' )) model . add ( Dropout ( 0.2 )) model . add ( Flatten ()) model . add ( Dense ( 50 )) model . add ( Activation ( 'relu' )) model . add ( Dense ( 1 )) model . summary () return model #optimize model = steering_model () adam = Adam ( lr = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-08 , decay = 0.0 ) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) history = model . fit ( train_features , train_labels , batch_size = 128 , nb_epoch = 10 , verbose = 1 , validation_data = ( test_features , test_labels )) #save the model architecture and parameters model_json = './model.json' model_h5 = './model.h5' model_save ( model_json , model_h5 )","title":"Model\u306e\u5b9f\u884c"},{"location":"end2end/model/#_1","text":"model.json, model.h5\u304c\u3067\u304d\u308c\u3070\u6210\u529f","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb"},{"location":"end2end/package/","text":"\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8 notebook\u306e\u4f5c\u6210 Keras\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb !pip install keras OpenCV\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb !apt-get update !apt-get -y install python-opencv","title":"\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8"},{"location":"end2end/package/#_1","text":"","title":"\u5fc5\u8981\u306a\u30d1\u30c3\u30b1\u30fc\u30b8"},{"location":"end2end/package/#notebook","text":"","title":"notebook\u306e\u4f5c\u6210"},{"location":"end2end/package/#keras","text":"!pip install keras","title":"Keras\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"end2end/package/#opencv","text":"!apt-get update !apt-get -y install python-opencv","title":"OpenCV\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"end2end/run/","text":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC) pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow \u5b66\u7fd2\u6e08\u307fModel\u306eDownload \u5b9f\u884c\u30bd\u30fc\u30b9 https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio . on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio . on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app ) \u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5 AI\u306e\u5b9f\u884c python drive.py model.json","title":"Run"},{"location":"end2end/run/#localmac","text":"pip install opencv-python pip install socketio pip install Image pip install keras pip install Flask pip install tensorflow","title":"Local\u74b0\u5883\u306e\u69cb\u7bc9(MAC)"},{"location":"end2end/run/#modeldownload","text":"","title":"\u5b66\u7fd2\u6e08\u307fModel\u306eDownload"},{"location":"end2end/run/#_1","text":"https://github.com/ymshao/End-to-End-Learning-for-Self-Driving-Cars/blob/master/drive.py \u3088\u308a drive.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 import argparse import base64 import json import cv2 import numpy as np import socketio import eventlet import eventlet.wsgi import time from PIL import Image from PIL import ImageOps from flask import Flask , render_template from io import BytesIO from keras.models import model_from_json from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array # Fix error with Keras and TensorFlow import tensorflow as tf tf . python . control_flow_ops = tf sio = socketio . Server () app = Flask ( __name__ ) model = None prev_image_array = None @sio . on ( 'telemetry' ) def telemetry ( sid , data ): # The current steering angle of the car steering_angle = data [ \"steering_angle\" ] # The current throttle of the car throttle = data [ \"throttle\" ] # The current speed of the car speed = data [ \"speed\" ] # The current image from the center camera of the car imgString = data [ \"image\" ] image = Image . open ( BytesIO ( base64 . b64decode ( imgString ))) image_array = np . asarray ( image ) transformed_image_array = image_array [ None , :, :, :] #resize the image transformed_image_array = ( cv2 . resize (( cv2 . cvtColor ( transformed_image_array [ 0 ], cv2 . COLOR_RGB2HSV ))[:,:, 1 ],( 32 , 16 ))) . reshape ( 1 , 16 , 32 , 1 ) # This model currently assumes that the features of the model are just the images. Feel free to change this. steering_angle = float ( model . predict ( transformed_image_array , batch_size = 1 )) # The driving model currently just outputs a constant throttle. Feel free to edit this. throttle = 0.2 #adaptive speed ''' if (float(speed) < 10): throttle = 0.4 else: # When speed is below 20 then increase throttle by speed_factor if ((float(speed)) < 25): speed_factor = 1.35 else: speed_factor = 1.0 if (abs(steering_angle) < 0.1): throttle = 0.3 * speed_factor elif (abs(steering_angle) < 0.5): throttle = 0.2 * speed_factor else: throttle = 0.15 * speed_factor ''' print ( 'Steering angle =' , ' %5.2f ' % ( float ( steering_angle )), 'Throttle =' , ' %.2f ' % ( float ( throttle )), 'Speed =' , ' %.2f ' % ( float ( speed ))) send_control ( steering_angle , throttle ) @sio . on ( 'connect' ) def connect ( sid , environ ): print ( \"connect \" , sid ) send_control ( 0 , 0 ) def send_control ( steering_angle , throttle ): sio . emit ( \"steer\" , data = { 'steering_angle' : steering_angle . __str__ (), 'throttle' : throttle . __str__ () }, skip_sid = True ) if __name__ == '__main__' : parser = argparse . ArgumentParser ( description = 'Remote Driving' ) parser . add_argument ( 'model' , type = str , help = 'Path to model definition json. Model weights should be on the same path.' ) args = parser . parse_args () with open ( args . model , 'r' ) as jfile : # NOTE: if you saved the file by calling json.dump(model.to_json(), ...) # then you will have to call: # # model = model_from_json(json.loads(jfile.read()))\\ # # instead. model = model_from_json ( jfile . read ()) model . compile ( \"adam\" , \"mse\" ) weights_file = args . model . replace ( 'json' , 'h5' ) model . load_weights ( weights_file ) # wrap Flask application with engineio's middleware app = socketio . Middleware ( sio , app ) # deploy as an eventlet WSGI server eventlet . wsgi . server ( eventlet . listen (( '' , 4567 )), app )","title":"\u5b9f\u884c\u30bd\u30fc\u30b9"},{"location":"end2end/run/#_2","text":"","title":"\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u30fc\u306e\u8d77\u52d5"},{"location":"end2end/run/#ai","text":"python drive.py model.json","title":"AI\u306e\u5b9f\u884c"},{"location":"link/link/","text":"https://github.com/nlintz/TensorFlow-Tutorials https://ischlag.github.io/2016/06/04/how-to-use-tensorboard/ http://www.sciweavers.org/free-online-latex-equation-editor https://drive.google.com/file/d/0B04ol8GVySUuWG1jdkozRWVLUjQ/view","title":"Link"},{"location":"local/local/","text":"Local\u74b0\u5883\u306e\u6574\u5099 TensorFlow\u306e\u5b9f\u884c\u306f\u3001CloudML\u3067\u3067\u304d\u308b\u304c\u3001matplotlib\u306f\u3001CloudML\u4e0a\u3067\u306f\u3001\u753b\u50cf\u306b\u51fa\u529b\u306f\u3067\u304d\u3066\u3082CloudShell\u4e0a\u306bWindow\u3092\u51fa\u3057\u3066\u8868\u793a\u3067\u304d\u308b\u4e8b\u304c\u3067\u304d\u306a\u3044\u306e\u3067\u3001matplotlib\u3068numpy\u304c\u6574\u5099\u3055\u308c\u305fPython\u74b0\u5883\u3092Local\u306b\u69cb\u7bc9\u3057\u3066\u304a\u304f\u3002 Python 3.x\u7cfb\u306eANACONDA\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 https://www.continuum.io/downloads 1 $ python --version \u3067\u3001\u3046\u307e\u304f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u305f\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002","title":"Local\u74b0\u5883\u306e\u6574\u5099"},{"location":"local/local/#local","text":"TensorFlow\u306e\u5b9f\u884c\u306f\u3001CloudML\u3067\u3067\u304d\u308b\u304c\u3001matplotlib\u306f\u3001CloudML\u4e0a\u3067\u306f\u3001\u753b\u50cf\u306b\u51fa\u529b\u306f\u3067\u304d\u3066\u3082CloudShell\u4e0a\u306bWindow\u3092\u51fa\u3057\u3066\u8868\u793a\u3067\u304d\u308b\u4e8b\u304c\u3067\u304d\u306a\u3044\u306e\u3067\u3001matplotlib\u3068numpy\u304c\u6574\u5099\u3055\u308c\u305fPython\u74b0\u5883\u3092Local\u306b\u69cb\u7bc9\u3057\u3066\u304a\u304f\u3002 Python 3.x\u7cfb\u306eANACONDA\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\u3002 https://www.continuum.io/downloads 1 $ python --version \u3067\u3001\u3046\u307e\u304f\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u305f\u304b\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002","title":"Local\u74b0\u5883\u306e\u6574\u5099"},{"location":"matplotlib/matplotlib/","text":"Matplotlib Matplotlib\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 $ pip install matplotlib Shell 1 vi ~/.matplotlib/matplotlibrc Txt 1 backend : TkAgg Sample 1 2 3 4 5 6 7 8 9 10 11 import numpy as np import matplotlib.pyplot as plt X = np . random . randn ( 10 , 1 ) Y = np . random . randn ( 10 , 1 ) print X print Y plt . scatter ( X [:, 0 ], Y [:, 0 ], s = 100 , alpha = 0.5 ) plt . show ()","title":"Matplotlib"},{"location":"matplotlib/matplotlib/#matplotlib","text":"Matplotlib\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Shell 1 $ pip install matplotlib Shell 1 vi ~/.matplotlib/matplotlibrc Txt 1 backend : TkAgg","title":"Matplotlib"},{"location":"matplotlib/matplotlib/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 import numpy as np import matplotlib.pyplot as plt X = np . random . randn ( 10 , 1 ) Y = np . random . randn ( 10 , 1 ) print X print Y plt . scatter ( X [:, 0 ], Y [:, 0 ], s = 100 , alpha = 0.5 ) plt . show ()","title":"Sample"},{"location":"matplotlib/matplotlib_line/","text":"\u7dda\u3092\u5f15\u304f Sample 1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt x1 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , - 2 ]) x2 = x1 * 0.3 - 1 plt . figure ( 1 ) plt . plot ( x1 [:, 0 ], x2 [:, 0 ], 'k-' , label = 'Data x1' ) plt . show () \u7d50\u679c","title":"\u7dda\u3092\u5f15\u304f"},{"location":"matplotlib/matplotlib_line/#_1","text":"","title":"\u7dda\u3092\u5f15\u304f"},{"location":"matplotlib/matplotlib_line/#sample","text":"1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt x1 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , - 2 ]) x2 = x1 * 0.3 - 1 plt . figure ( 1 ) plt . plot ( x1 [:, 0 ], x2 [:, 0 ], 'k-' , label = 'Data x1' ) plt . show () \u7d50\u679c","title":"Sample"},{"location":"matplotlib/matplotlib_point/","text":"\u70b9\u3092\u3046\u3064 Sample 1 2 3 4 5 6 7 8 9 10 11 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt x1 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , - 2 ]) x2 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , 2 ]) plt . figure ( 1 ) plt . plot ( x1 [:, 0 ], x1 [:, 1 ], 'ro' , label = 'Data x1' ) plt . plot ( x2 [:, 0 ], x2 [:, 1 ], 'bo' , label = 'Data x2' ) plt . show () \u7d50\u679c","title":"\u70b9\u3092\u3046\u3064"},{"location":"matplotlib/matplotlib_point/#_1","text":"","title":"\u70b9\u3092\u3046\u3064"},{"location":"matplotlib/matplotlib_point/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt x1 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , - 2 ]) x2 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , 2 ]) plt . figure ( 1 ) plt . plot ( x1 [:, 0 ], x1 [:, 1 ], 'ro' , label = 'Data x1' ) plt . plot ( x2 [:, 0 ], x2 [:, 1 ], 'bo' , label = 'Data x2' ) plt . show () \u7d50\u679c","title":"Sample"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/","text":"MNIST DCGAN(Deep Convolution Generative Adversarial Network) DCGAN\u306e\u8ad6\u6587\u306f\u3053\u3061\u3089 DCGAN\u306f2015\u5e74\u9803\u306b\u63d0\u6848\u3055\u308c\u305fGAN\u3092\u62e1\u5f35\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002 GAN\u3068\u540c\u69d8\u306bGenerator\u3068Discriminator\u306e2\u3064\u306e\u30e2\u30c7\u30eb\u3092\u6301\u3064\u304c\u3001\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u69cb\u9020\u304c\u7570\u306a\u308b\u3002 DCGAN\u306e\u5909\u66f4\u70b9 \u6700\u521d\u306b\u63d0\u6848\u3055\u308c\u305fGAN\u3067\u306f * \u5b66\u7fd2\u304c\u4e0d\u5b89\u5b9a\u306a\u3053\u3068 * Generator\u304c\u751f\u6210\u3059\u308b\u30c7\u30fc\u30bf\u306b\u30ce\u30a4\u30ba\u304c\u542b\u307e\u308c\u3066\u3044\u305f\u308a\u3001\u30c7\u30fc\u30bf\u81ea\u4f53\u304c\u610f\u5473\u7684\u306b\u3001\u8996\u899a\u7684\u306b\u7406\u89e3\u304c\u96e3\u3057\u3044\u3082\u306e\u304c\u751f\u6210\u3055\u308c\u308b\u3053\u3068 \u306a\u3069\u3068\u3044\u3063\u305f\u554f\u984c\u304c\u3042\u3052\u3089\u308c\u308b\u3002 DCGAN\u3067\u306f\u6b21\u306e\u3088\u3046\u306a\u5909\u66f4\u70b9\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u4e0a\u8a18\u306e\u554f\u984c\u306e\u6539\u5584\u3092\u3057\u3066\u3044\u308b\u3002 Generator\u3068Discriminator\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3(MLP)\u3067\u306f\u306a\u304f \u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af (CNN)\u3092\u4f7f\u3063\u3066\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3002\u307e\u305fCNN\u3067\u306f\u7573\u307f\u8fbc\u307f\u5c64\u306e\u6b21\u306b\u6700\u5927\u30d7\u30fc\u30ea\u30f3\u30b0(Max Pooling)\u3092\u9069\u7528\u3057\u3066\u3044\u308b\u3082\u306e\u304c\u591a\u3044\u304c\u3001DCGAN\u3067\u306f Max Pooling \u306a\u3069\u3092\u7121\u304f\u3057\u305f \u7573\u307f\u8fbc\u307f\u5c64\u306e\u307f\u304b\u3089\u306a\u308b All Convolution Net \u3068\u3044\u3046CNN\u3092\u63a1\u7528\u3059\u308b\u3002 Alexnet\u306a\u3069\u306eCNN\u3067\u306f\u4f55\u5c64\u3082\u306e\u7573\u307f\u8fbc\u307f\u5c64\u3068\u6700\u5927\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u306e\u5f8c\u3001\u51fa\u529b\u306b\u306f\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u306e\u5024\u3092\u5e73\u5766\u5316(Flatten)\u3057\u3066\u5168\u7d50\u5408\u5c64\u306b\u6e21\u3057\u3066\u8a08\u7b97\u3057\u305f\u3082\u306e\u3092\u51fa\u529b\u3068\u3057\u3066\u3044\u308b\u306e\u304c\u591a\u3044\u3002 DCGAN\u3067\u306fGenerator\u3068Discriminator\u306e \u51fa\u529b\u306b\u5168\u7d50\u5408\u5c64\u3092\u7528\u3044\u306a\u3044 \u3002\u9006\u306b\u5168\u7d50\u5408\u5c64\u3092\u6392\u9664\u3059\u308b\u3068\u3082\u8a00\u3048\u308b\u3002\u305d\u306e\u305f\u3081Generator\u306e\u51fa\u529b\u306f\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u306e\u5024\u306b\u6d3b\u6027\u5316\u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u3082\u306e\u3092\u3001Discriminator\u306e\u51fa\u529b\u306b\u306f\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u306e\u5024\u3092\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u3057\u305f\u3082\u306e\u3092\u51fa\u529b\u3068\u3059\u308b\u3002 \u5b66\u7fd2\u306e\u4e0d\u5b89\u5b9a\u3055\u306e\u5bfe\u7b56\u3068\u3057\u3066\u4e21\u30e2\u30c7\u30eb\u306e\u4e2d\u9593\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306b \u30d0\u30c3\u30c1\u6b63\u898f\u5316(Batch Normalization) \u3092\u9069\u7528\u3059\u308b\u3002 \u3053\u308c\u3089\u306e\u5909\u66f4\u3067\u3001\u4ee5\u524d\u306eGAN\u3088\u308a\u5b66\u7fd2\u304c\u5b89\u5b9a\u3057\u3066\u3001\u304b\u3064\u7cbe\u5bc6\u306a\u3082\u306e\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002 DCGAN\u306e\u69cb\u9020 \u65e2\u306b\u8ff0\u3079\u305f\u306e\u3088\u3046\u306bGenerator\u3068Discriminator\u306fCNN\u3067\u69cb\u7bc9\u3059\u308b\u3002 \u8ad6\u6587\u3067\u306f\u751f\u6210\u30e2\u30c7\u30eb\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u56f3\u304c\u8f09\u3063\u3066\u3044\u308b\u3002 \u3053\u3053\u3067\u306f\u8f09\u3063\u3066\u3044\u306a\u3044\u304c\u3001\u7573\u307f\u8fbc\u307f\u5c64\u306e\u3042\u3068\u306b\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u9069\u7528\u3057\u305f\u3082\u306e\u3092LeakyRelu\u95a2\u6570\u306a\u3069\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3067\u6d3b\u6027\u5316\u3057\u3066\u6b21\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306b\u6e21\u3059\u3002 \u307e\u305fGenerator\u3067\u306e\u7573\u307f\u8fbc\u307f\u306f\u3001 \u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0 (\u8aa4\u3063\u3066\u30c7\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3068\u3082)\u3068\u547c\u3070\u308c\u308b\u7573\u307f\u8fbc\u307f\u3067\u3042\u308b\u3002 Generator\u3067\u306f\u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bRelu\u95a2\u6570\u3001\u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bTanh\u95a2\u6570\u3092\u7528\u3044\u3001Discriminator\u306b\u306f\u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bLeakyRelu\u95a2\u6570\u3001\u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bSigmoid\u95a2\u6570\u3092\u7528\u3044\u308b\u3068\u8ad6\u6587\u306b\u3066\u8aac\u660e\u3055\u308c\u3066\u3044\u308b\u3002 DCGAN\u3067\u306e\u76ee\u7684\u95a2\u6570\u306fGAN\u3068\u540c\u69d8\u3067\u3042\u308b\u306e\u3067\u7701\u7565\u3059\u308b Tensorflow\u3092\u7528\u3044\u305fDCGAN\u306e\u5b9f\u88c5 MNIST\u306e\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u7cbe\u5ea6\u306e\u78ba\u8a8d\u3092\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u8ad6\u6587\u901a\u308a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3067\u306f\u306a\u304f3\u5c64\u3067\u3001\u5404\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u524a\u6e1b\u3057\u3001\u51fa\u529b\u3092 64*64 \u304b\u3089 28*28 \u306b\u5909\u66f4\u3057\u305f\u3082\u306e\u3092\u5b9f\u88c5\u3059\u308b \u307e\u305fDiscriminator\u306e\u7573\u307f\u8fbc\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3060\u3051\u91cd\u307f\u6e1b\u8870\u3092\u9069\u7528\u3059\u308b Generator\u306e\u5b9f\u88c5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def batch_norm ( X , scale , offset , axes , is_train , device_name = '/cpu:0' ): # \u4e88\u6e2c\u306e\u3068\u304d\u306b\u306f\u305d\u306e\u307e\u3093\u307e\u306e\u5024\u3092\u8fd4\u3059 if is_train is False : return X epsilon = 1e-5 with tf . device ( device_name ): mean , variance = tf . nn . moments ( X , axes ) bn = tf . nn . batch_normalization ( X , mean , variance , offset , scale , epsilon ) return bn class Generator (): def __init__ ( self , device_name = '/cpu:0' ): # Generator parameter with tf . device ( device_name ): self . gen_w0 = tf . Variable ( tf . random_normal ( shape = [ 100 , 4 * 4 * 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w0\" ) self . gen_b0 = tf . Variable ( tf . random_normal ( shape = [ 4 * 4 * 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b0\" ) self . gen_w1 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 128 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w1\" ) self . gen_b1 = tf . Variable ( tf . random_normal ( shape = [ 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b1\" ) self . gen_w2 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 64 , 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w2\" ) self . gen_b2 = tf . Variable ( tf . random_normal ( shape = [ 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b2\" ) self . gen_w3 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 1 , 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w3\" ) self . gen_b3 = tf . Variable ( tf . random_normal ( shape = [ 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b3\" ) self . gen_scale_w1 = tf . Variable ( tf . ones ([ 128 ]), name = \"gen_scale_w1\" ) self . gen_offset_w1 = tf . Variable ( tf . zeros ([ 128 ]), name = \"gen_offset_w1\" ) self . gen_scale_w2 = tf . Variable ( tf . ones ([ 64 ]), name = \"gen_scale_w2\" ) self . gen_offset_w2 = tf . Variable ( tf . zeros ([ 64 ]), name = \"gen_offset_w2\" ) self . keep_prob = tf . placeholder ( tf . float32 ) self . batch_size = tf . placeholder ( tf . int32 ) def run ( self , z , is_train , device_name = '/cpu:0' ): with tf . device ( device_name ): h0 = tf . reshape ( tf . nn . relu ( tf . nn . xw_plus_b ( z , self . gen_w0 , self . gen_b0 )),[ - 1 , 4 , 4 , 256 ]) gen_conv1 = tf . nn . conv2d_transpose ( value = h0 , filter = self . gen_w1 , output_shape = [ self . batch_size , 7 , 7 , 128 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . gen_b1 h1 = tf . nn . leaky_relu ( batch_norm ( gen_conv1 , self . gen_scale_w1 , self . gen_offset_w1 , [ 0 , 1 , 2 ], is_train , device_name ), alpha = 0.2 ) gen_conv2 = tf . nn . conv2d_transpose ( value = h1 , filter = self . gen_w2 , output_shape = [ self . batch_size , 14 , 14 , 64 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . gen_b2 h2 = tf . nn . leaky_relu ( batch_norm ( gen_conv2 , self . gen_scale_w2 , self . gen_offset_w2 , [ 0 , 1 , 2 ], is_train , device_name ), alpha = 0.2 ) gen_conv3 = tf . nn . tanh ( tf . nn . conv2d_transpose ( value = h2 , filter = self . gen_w3 , output_shape = [ self . batch_size , 28 , 28 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . gen_b3 ) return gen_conv3 \u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306b\u76f8\u5f53\u3059\u308b\u64cd\u4f5c\u306f tf.nn.conv2d_transpose() \u3067\u884c\u3048\u308b\u3002 \u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066Relu\u95a2\u6570\u3067\u306f\u306a\u304fLeakyRelu\u95a2\u6570\u3092\u9069\u7528\u3059\u308b GAN\u3068\u540c\u69d8\u306b\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u3053\u3068 Discriminator\u306e\u5b9f\u88c5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class Discrimitor (): def __init__ ( self , device_name = '/cpu:0' ): # Discrimitor parameter with tf . device ( device_name ): self . dis_w1 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 1 , 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w1\" ) self . dis_b1 = tf . Variable ( tf . random_normal ( shape = [ 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b1\" ) self . dis_w2 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 64 , 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w2\" ) self . dis_b2 = tf . Variable ( tf . random_normal ( shape = [ 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b2\" ) self . dis_w3 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 128 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w3\" ) self . dis_b3 = tf . Variable ( tf . random_normal ( shape = [ 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b3\" ) self . dis_w4 = tf . Variable ( tf . random_normal ( shape = [ 4 * 4 * 256 , 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w4\" ) self . dis_b4 = tf . Variable ( tf . random_normal ( shape = [ 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b4\" ) def run ( self , x , is_train , device_name = '/cpu:0' ): with tf . device ( device_name ): input_layer = tf . reshape ( x , [ - 1 , 28 , 28 , 1 ]) dis_conv1 = tf . nn . conv2d ( input = input_layer , filter = self . dis_w1 , strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . dis_b1 h1 = tf . nn . leaky_relu ( dis_conv1 , alpha = 0.2 ) dis_conv2 = tf . nn . conv2d ( input = h1 , filter = self . dis_w2 , strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . dis_b2 h2 = tf . nn . leaky_relu ( dis_conv2 , alpha = 0.2 ) dis_conv3 = tf . nn . conv2d ( input = h2 , filter = self . dis_w3 , strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . dis_b3 h3 = tf . nn . leaky_relu ( dis_conv3 , alpha = 0.2 ) h3_flat = tf . reshape ( h3 ,[ - 1 , 4 * 4 * 256 ]) fc = tf . nn . sigmoid ( tf . nn . xw_plus_b ( h3_flat , weights = self . dis_w4 , biases = self . dis_b4 )) return fc Discriminator\u306b\u304a\u3051\u308b\u7573\u307f\u8fbc\u307f\u306f\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u3042\u308b \u8ad6\u6587\u3067\u306f\u51fa\u529b\u3092\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306e\u5024\u3092\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u3057\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u304c\u3001\u3053\u3053\u3067\u306f\u8ad6\u6587\u3067\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u540c\u3058\u3082\u306e\u3067\u306f\u306a\u3044\u305f\u3081\u5168\u7d50\u5408\u5c64\u306b\u6e21\u3057\u305f\u3082\u306e\u3092\u51fa\u529b\u3068\u3059\u308b\u3002 \u307e\u305fGAN\u3067\u306eDiscriminator\u3068\u540c\u3058\u3088\u3046\u306b\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u9069\u7528\u3057\u306a\u3044 DCGAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class DCGAN (): def __init__ ( self , using_gpu ): self . device_name = '/cpu:0' if ( using_gpu ): self . device_name = '/gpu:0' print ( 'using : {} ' . format ( self . device_name )) with tf . device ( self . device_name ): self . G_is_train = tf . placeholder ( tf . bool ) self . D_is_train = tf . placeholder ( tf . bool ) self . input_X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 )) # t0\u306f0\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3057\u3001t1\u306f1\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308b self . label_t0 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) self . label_t1 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # Generator self . generator = Generator ( device_name = self . device_name ) # \u751f\u6210\u30e2\u30c7\u30eb\u306b\u5fc5\u8981\u306a\u30ce\u30a4\u30ba\u306e\u5165\u308c\u7269 self . gen_z = tf . placeholder ( tf . float32 , shape = ( None , 100 )) # Discrimitor self . discrimitor = Discrimitor ( device_name = self . device_name ) # weight decay gen_norm_term = tf . nn . l2_loss ( self . generator . gen_w2 ) + tf . nn . l2_loss ( self . generator . gen_w3 ) gen_lambda_ = 0.001 dis_norm_term = tf . nn . l2_loss ( self . discrimitor . dis_w2 ) + tf . nn . l2_loss ( self . discrimitor . dis_w3 ) dis_lambda_ = 0.001 # \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c input_X = self . discrimitor . run ( self . input_X , is_train = self . D_is_train , device_name = self . device_name ) # \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c generated_X = self . discrimitor . run ( self . generator . run ( z = self . gen_z , is_train = self . G_is_train , device_name = self . device_name ), is_train = self . D_is_train , device_name = self . device_name ) self . dis_entropy_X = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = input_X ) self . dis_entropy_G = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t0 , logits = generated_X ) self . dis_loss = tf . reduce_mean ( self . dis_entropy_X + self . dis_entropy_G ) + dis_norm_term * dis_lambda_ self . gen_entropy = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = generated_X ) self . gen_loss = tf . reduce_mean ( self . gen_entropy ) #+ gen_norm_term * gen_lambda_ # \u6700\u9069\u5316\u3059\u308b\u969b\u306bD\u306a\u3089D\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3001G\u306a\u3089G\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3059\u308b\u3088\u3046\u306b\u3057\u305f\u3044\u306e\u3067\u30e2\u30c7\u30eb\u5225\u306e\u5909\u6570\u3092\u53d6\u5f97\u3059\u308b dis_vars = [ x for x in tf . trainable_variables () if \"dis_\" in x . name ] gen_vars = [ x for x in tf . trainable_variables () if \"gen_\" in x . name ] # \u8b58\u5225\u30e2\u30c7\u30ebD\u306e\u6700\u9069\u5316 self . opt_d = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.1 ) . minimize ( self . dis_loss , var_list = [ dis_vars ]) # \u751f\u6210\u30e2\u30c7\u30ebG\u306e\u6700\u9069\u5316 self . opt_g = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.5 ) . minimize ( self . gen_loss , var_list = [ gen_vars ]) def train ( self , X_train = None , batch_size = 100 , epoch_num = 1000 , imgpath = './mnist_DCGAN_images/' , ckptpath = './mnist_DCGAN_checkpoints/' , log_file = 'mnist_DCGAN_loss_log.csv' , init = False ): if X_train is None : raise TypeError ( \"X_train is None\" ) # \u8a13\u7df4\u9014\u4e2d\u3067\u751f\u6210\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3057\u3066\u4fdd\u5b58\u3057\u305f\u3044\u306e\u3067\u305d\u306e\u4fdd\u5b58\u5148\u306e\u4f5c\u6210 p = Path ( imgpath ) if not ( p . is_dir ()): p . mkdir () # \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u306e\u4fdd\u5b58\u5148 ckpt_p = Path ( ckptpath ) if not ( ckpt_p . is_dir ()): ckpt_p . mkdir () config = tf . ConfigProto () config . gpu_options . allow_growth = True saver = tf . train . Saver () sess = tf . Session () if ( init ): sess . run ( tf . global_variables_initializer ()) print ( 'Initialize' ) ckpt = tf . train . get_checkpoint_state ( str ( ckpt_p . absolute ())) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) step = len ( X_train ) // batch_size #step = mnist.train.num_examples // batch_size # \u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u30df\u30cb\u30d0\u30c3\u30c1 t1_batch = np . ones (( batch_size , 1 ), dtype = np . float32 ) t0_batch = np . zeros (( batch_size , 1 ), dtype = np . float32 ) for epoch in range ( epoch_num ): perm = np . random . permutation ( len ( X_train )) # \uff11\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u304b\u304b\u308b\u6642\u9593\u306e\u8a08\u6e2c start = time . time () for k in range ( step ): #X_batch = mnist.train.next_batch(batch_size)[0] /255. X_batch = X_train [ perm ][ k * batch_size :( k + 1 ) * batch_size ] # Train Discrimitor # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_d , feed_dict = { self . input_X : X_batch , self . D_is_train : True , self . G_is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) if k % 1 == 0 : # Train Generator # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_g , feed_dict = { self . gen_z : noise_z , self . D_is_train : False , self . G_is_train : True , self . generator . keep_prob : 0.5 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch }) # 1epoch\u7d42\u4e86\u6642\u306e\u640d\u5931\u3092\u8868\u793a noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) train_dis_loss = sess . run ( self . dis_loss , feed_dict = { self . input_X : X_batch , self . D_is_train : False , self . G_is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) train_gen_loss = sess . run ( self . gen_loss , feed_dict = { self . gen_z : noise_z , self . D_is_train : False , self . G_is_train : False , self . generator . keep_prob : 1.0 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch }) print ( \"[Train] epoch: %d , dis loss: %f , gen loss : %f Time : %f \" % ( epoch , train_dis_loss , train_gen_loss , time . time () - start )) f = open ( log_file , 'a' ) log_writer = csv . writer ( f , lineterminator = ' \\n ' ) loss_list = [] loss_list . append ( epoch ) loss_list . append ( train_dis_loss ) loss_list . append ( train_gen_loss ) # \u640d\u5931\u306e\u5024\u3092\u66f8\u304d\u8fbc\u3080 log_writer . writerow ( loss_list ) f . close () saver . save ( sess , str ( ckpt_p . absolute ()) + '/DCGAN-mnist' ) # 10epoch\u7d42\u4e86\u6bce\u306b\u751f\u6210\u30e2\u30c7\u30eb\u304b\u30891\u679a\u306e\u753b\u50cf\u3092\u751f\u6210\u3059\u308b if epoch % 2 == 0 : noise_z = np . random . uniform ( - 1 , 1 , size = [ 5 , 100 ]) . astype ( np . float32 ) z_const = tf . constant ( noise_z , dtype = tf . float32 ) gen_imgs = (( sess . run ( self . generator . run ( z_const , is_train = False ), feed_dict = { self . generator . keep_prob : 1.0 , self . generator . batch_size : 5 }) * 0.5 ) + 0.5 ) * 255. for i in range ( 0 , 5 ): Image . fromarray ( gen_imgs [ i ] . reshape ( 28 , 28 )) . convert ( 'RGB' ) . save ( str ( p . absolute ()) + '/generate_img_epoch {0} _ {1} .jpg' . format ( epoch , i )) \u5b66\u7fd2 Generator\u306e\u51fa\u529b\u306b\u6d3b\u6027\u5316\u95a2\u6570Tanh\u3092\u4f7f\u3063\u3066\u3044\u308b\u305f\u3081\u3001 [-1,1] \u306e\u7bc4\u56f2\u306e\u5024\u3092\u51fa\u529b\u304c\u53d6\u308a\u3046\u308b\u3002 \u305d\u306e\u305f\u3081\u8a13\u7df4\u30c7\u30fc\u30bf\u3082 [-1,1] \u306e\u7bc4\u56f2\u306e\u5024\u3092\u53d6\u308b\u3088\u3046\u306b\u524d\u51e6\u7406\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308b\u3002 1 X_train = ( dataset . train . images - 0.5 ) / 0.5 \u5b66\u7fd2\u3092\u884c\u3046 1 2 3 4 5 6 7 8 9 10 11 12 13 import tensorflow as tf device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : raise SystemError ( 'GPU device not found' ) print ( 'Found GPU at: {} ' . format ( device_name )) model = DCGAN ( using_gpu = True ) model . train ( X_train = X_train , batch_size = 100 , epoch_num = 51 , init = True ) 50epoch\u3067\u6b21\u306e\u3088\u3046\u306a\u753b\u50cf\u304c\u751f\u6210\u3055\u308c\u305f \u5b66\u7fd2\u7d4c\u904e Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/DCGAN_mnist.ipynb","title":"MNIST DCGAN(Deep Convolution Generative Adversarial Network)"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#mnist-dcgandeep-convolution-generative-adversarial-network","text":"DCGAN\u306e\u8ad6\u6587\u306f\u3053\u3061\u3089 DCGAN\u306f2015\u5e74\u9803\u306b\u63d0\u6848\u3055\u308c\u305fGAN\u3092\u62e1\u5f35\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002 GAN\u3068\u540c\u69d8\u306bGenerator\u3068Discriminator\u306e2\u3064\u306e\u30e2\u30c7\u30eb\u3092\u6301\u3064\u304c\u3001\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u69cb\u9020\u304c\u7570\u306a\u308b\u3002","title":"MNIST DCGAN(Deep Convolution Generative Adversarial Network)"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#dcgan","text":"\u6700\u521d\u306b\u63d0\u6848\u3055\u308c\u305fGAN\u3067\u306f * \u5b66\u7fd2\u304c\u4e0d\u5b89\u5b9a\u306a\u3053\u3068 * Generator\u304c\u751f\u6210\u3059\u308b\u30c7\u30fc\u30bf\u306b\u30ce\u30a4\u30ba\u304c\u542b\u307e\u308c\u3066\u3044\u305f\u308a\u3001\u30c7\u30fc\u30bf\u81ea\u4f53\u304c\u610f\u5473\u7684\u306b\u3001\u8996\u899a\u7684\u306b\u7406\u89e3\u304c\u96e3\u3057\u3044\u3082\u306e\u304c\u751f\u6210\u3055\u308c\u308b\u3053\u3068 \u306a\u3069\u3068\u3044\u3063\u305f\u554f\u984c\u304c\u3042\u3052\u3089\u308c\u308b\u3002 DCGAN\u3067\u306f\u6b21\u306e\u3088\u3046\u306a\u5909\u66f4\u70b9\u3092\u52a0\u3048\u308b\u3053\u3068\u3067\u4e0a\u8a18\u306e\u554f\u984c\u306e\u6539\u5584\u3092\u3057\u3066\u3044\u308b\u3002 Generator\u3068Discriminator\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3(MLP)\u3067\u306f\u306a\u304f \u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af (CNN)\u3092\u4f7f\u3063\u3066\u69cb\u7bc9\u3059\u308b\u3053\u3068\u3002\u307e\u305fCNN\u3067\u306f\u7573\u307f\u8fbc\u307f\u5c64\u306e\u6b21\u306b\u6700\u5927\u30d7\u30fc\u30ea\u30f3\u30b0(Max Pooling)\u3092\u9069\u7528\u3057\u3066\u3044\u308b\u3082\u306e\u304c\u591a\u3044\u304c\u3001DCGAN\u3067\u306f Max Pooling \u306a\u3069\u3092\u7121\u304f\u3057\u305f \u7573\u307f\u8fbc\u307f\u5c64\u306e\u307f\u304b\u3089\u306a\u308b All Convolution Net \u3068\u3044\u3046CNN\u3092\u63a1\u7528\u3059\u308b\u3002 Alexnet\u306a\u3069\u306eCNN\u3067\u306f\u4f55\u5c64\u3082\u306e\u7573\u307f\u8fbc\u307f\u5c64\u3068\u6700\u5927\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u306e\u5f8c\u3001\u51fa\u529b\u306b\u306f\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u306e\u5024\u3092\u5e73\u5766\u5316(Flatten)\u3057\u3066\u5168\u7d50\u5408\u5c64\u306b\u6e21\u3057\u3066\u8a08\u7b97\u3057\u305f\u3082\u306e\u3092\u51fa\u529b\u3068\u3057\u3066\u3044\u308b\u306e\u304c\u591a\u3044\u3002 DCGAN\u3067\u306fGenerator\u3068Discriminator\u306e \u51fa\u529b\u306b\u5168\u7d50\u5408\u5c64\u3092\u7528\u3044\u306a\u3044 \u3002\u9006\u306b\u5168\u7d50\u5408\u5c64\u3092\u6392\u9664\u3059\u308b\u3068\u3082\u8a00\u3048\u308b\u3002\u305d\u306e\u305f\u3081Generator\u306e\u51fa\u529b\u306f\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u306e\u5024\u306b\u6d3b\u6027\u5316\u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u3082\u306e\u3092\u3001Discriminator\u306e\u51fa\u529b\u306b\u306f\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u306e\u5024\u3092\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u3057\u305f\u3082\u306e\u3092\u51fa\u529b\u3068\u3059\u308b\u3002 \u5b66\u7fd2\u306e\u4e0d\u5b89\u5b9a\u3055\u306e\u5bfe\u7b56\u3068\u3057\u3066\u4e21\u30e2\u30c7\u30eb\u306e\u4e2d\u9593\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306b \u30d0\u30c3\u30c1\u6b63\u898f\u5316(Batch Normalization) \u3092\u9069\u7528\u3059\u308b\u3002 \u3053\u308c\u3089\u306e\u5909\u66f4\u3067\u3001\u4ee5\u524d\u306eGAN\u3088\u308a\u5b66\u7fd2\u304c\u5b89\u5b9a\u3057\u3066\u3001\u304b\u3064\u7cbe\u5bc6\u306a\u3082\u306e\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002","title":"DCGAN\u306e\u5909\u66f4\u70b9"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#dcgan_1","text":"\u65e2\u306b\u8ff0\u3079\u305f\u306e\u3088\u3046\u306bGenerator\u3068Discriminator\u306fCNN\u3067\u69cb\u7bc9\u3059\u308b\u3002 \u8ad6\u6587\u3067\u306f\u751f\u6210\u30e2\u30c7\u30eb\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3068\u3057\u3066\u4ee5\u4e0b\u306e\u56f3\u304c\u8f09\u3063\u3066\u3044\u308b\u3002 \u3053\u3053\u3067\u306f\u8f09\u3063\u3066\u3044\u306a\u3044\u304c\u3001\u7573\u307f\u8fbc\u307f\u5c64\u306e\u3042\u3068\u306b\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u9069\u7528\u3057\u305f\u3082\u306e\u3092LeakyRelu\u95a2\u6570\u306a\u3069\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3067\u6d3b\u6027\u5316\u3057\u3066\u6b21\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306b\u6e21\u3059\u3002 \u307e\u305fGenerator\u3067\u306e\u7573\u307f\u8fbc\u307f\u306f\u3001 \u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0 (\u8aa4\u3063\u3066\u30c7\u30b3\u30f3\u30dc\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3068\u3082)\u3068\u547c\u3070\u308c\u308b\u7573\u307f\u8fbc\u307f\u3067\u3042\u308b\u3002 Generator\u3067\u306f\u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bRelu\u95a2\u6570\u3001\u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bTanh\u95a2\u6570\u3092\u7528\u3044\u3001Discriminator\u306b\u306f\u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bLeakyRelu\u95a2\u6570\u3001\u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306bSigmoid\u95a2\u6570\u3092\u7528\u3044\u308b\u3068\u8ad6\u6587\u306b\u3066\u8aac\u660e\u3055\u308c\u3066\u3044\u308b\u3002 DCGAN\u3067\u306e\u76ee\u7684\u95a2\u6570\u306fGAN\u3068\u540c\u69d8\u3067\u3042\u308b\u306e\u3067\u7701\u7565\u3059\u308b","title":"DCGAN\u306e\u69cb\u9020"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#tensorflowdcgan","text":"MNIST\u306e\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u7cbe\u5ea6\u306e\u78ba\u8a8d\u3092\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u8ad6\u6587\u901a\u308a\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3067\u306f\u306a\u304f3\u5c64\u3067\u3001\u5404\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u3092\u524a\u6e1b\u3057\u3001\u51fa\u529b\u3092 64*64 \u304b\u3089 28*28 \u306b\u5909\u66f4\u3057\u305f\u3082\u306e\u3092\u5b9f\u88c5\u3059\u308b \u307e\u305fDiscriminator\u306e\u7573\u307f\u8fbc\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u3060\u3051\u91cd\u307f\u6e1b\u8870\u3092\u9069\u7528\u3059\u308b","title":"Tensorflow\u3092\u7528\u3044\u305fDCGAN\u306e\u5b9f\u88c5"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#generator","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def batch_norm ( X , scale , offset , axes , is_train , device_name = '/cpu:0' ): # \u4e88\u6e2c\u306e\u3068\u304d\u306b\u306f\u305d\u306e\u307e\u3093\u307e\u306e\u5024\u3092\u8fd4\u3059 if is_train is False : return X epsilon = 1e-5 with tf . device ( device_name ): mean , variance = tf . nn . moments ( X , axes ) bn = tf . nn . batch_normalization ( X , mean , variance , offset , scale , epsilon ) return bn class Generator (): def __init__ ( self , device_name = '/cpu:0' ): # Generator parameter with tf . device ( device_name ): self . gen_w0 = tf . Variable ( tf . random_normal ( shape = [ 100 , 4 * 4 * 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w0\" ) self . gen_b0 = tf . Variable ( tf . random_normal ( shape = [ 4 * 4 * 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b0\" ) self . gen_w1 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 128 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w1\" ) self . gen_b1 = tf . Variable ( tf . random_normal ( shape = [ 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b1\" ) self . gen_w2 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 64 , 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w2\" ) self . gen_b2 = tf . Variable ( tf . random_normal ( shape = [ 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b2\" ) self . gen_w3 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 1 , 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w3\" ) self . gen_b3 = tf . Variable ( tf . random_normal ( shape = [ 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b3\" ) self . gen_scale_w1 = tf . Variable ( tf . ones ([ 128 ]), name = \"gen_scale_w1\" ) self . gen_offset_w1 = tf . Variable ( tf . zeros ([ 128 ]), name = \"gen_offset_w1\" ) self . gen_scale_w2 = tf . Variable ( tf . ones ([ 64 ]), name = \"gen_scale_w2\" ) self . gen_offset_w2 = tf . Variable ( tf . zeros ([ 64 ]), name = \"gen_offset_w2\" ) self . keep_prob = tf . placeholder ( tf . float32 ) self . batch_size = tf . placeholder ( tf . int32 ) def run ( self , z , is_train , device_name = '/cpu:0' ): with tf . device ( device_name ): h0 = tf . reshape ( tf . nn . relu ( tf . nn . xw_plus_b ( z , self . gen_w0 , self . gen_b0 )),[ - 1 , 4 , 4 , 256 ]) gen_conv1 = tf . nn . conv2d_transpose ( value = h0 , filter = self . gen_w1 , output_shape = [ self . batch_size , 7 , 7 , 128 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . gen_b1 h1 = tf . nn . leaky_relu ( batch_norm ( gen_conv1 , self . gen_scale_w1 , self . gen_offset_w1 , [ 0 , 1 , 2 ], is_train , device_name ), alpha = 0.2 ) gen_conv2 = tf . nn . conv2d_transpose ( value = h1 , filter = self . gen_w2 , output_shape = [ self . batch_size , 14 , 14 , 64 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . gen_b2 h2 = tf . nn . leaky_relu ( batch_norm ( gen_conv2 , self . gen_scale_w2 , self . gen_offset_w2 , [ 0 , 1 , 2 ], is_train , device_name ), alpha = 0.2 ) gen_conv3 = tf . nn . tanh ( tf . nn . conv2d_transpose ( value = h2 , filter = self . gen_w3 , output_shape = [ self . batch_size , 28 , 28 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . gen_b3 ) return gen_conv3 \u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306b\u76f8\u5f53\u3059\u308b\u64cd\u4f5c\u306f tf.nn.conv2d_transpose() \u3067\u884c\u3048\u308b\u3002 \u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066Relu\u95a2\u6570\u3067\u306f\u306a\u304fLeakyRelu\u95a2\u6570\u3092\u9069\u7528\u3059\u308b GAN\u3068\u540c\u69d8\u306b\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u3053\u3068","title":"Generator\u306e\u5b9f\u88c5"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#discriminator","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class Discrimitor (): def __init__ ( self , device_name = '/cpu:0' ): # Discrimitor parameter with tf . device ( device_name ): self . dis_w1 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 1 , 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w1\" ) self . dis_b1 = tf . Variable ( tf . random_normal ( shape = [ 64 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b1\" ) self . dis_w2 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 64 , 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w2\" ) self . dis_b2 = tf . Variable ( tf . random_normal ( shape = [ 128 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b2\" ) self . dis_w3 = tf . Variable ( tf . random_normal ( shape = [ 4 , 4 , 128 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w3\" ) self . dis_b3 = tf . Variable ( tf . random_normal ( shape = [ 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b3\" ) self . dis_w4 = tf . Variable ( tf . random_normal ( shape = [ 4 * 4 * 256 , 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w4\" ) self . dis_b4 = tf . Variable ( tf . random_normal ( shape = [ 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b4\" ) def run ( self , x , is_train , device_name = '/cpu:0' ): with tf . device ( device_name ): input_layer = tf . reshape ( x , [ - 1 , 28 , 28 , 1 ]) dis_conv1 = tf . nn . conv2d ( input = input_layer , filter = self . dis_w1 , strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . dis_b1 h1 = tf . nn . leaky_relu ( dis_conv1 , alpha = 0.2 ) dis_conv2 = tf . nn . conv2d ( input = h1 , filter = self . dis_w2 , strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . dis_b2 h2 = tf . nn . leaky_relu ( dis_conv2 , alpha = 0.2 ) dis_conv3 = tf . nn . conv2d ( input = h2 , filter = self . dis_w3 , strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) + self . dis_b3 h3 = tf . nn . leaky_relu ( dis_conv3 , alpha = 0.2 ) h3_flat = tf . reshape ( h3 ,[ - 1 , 4 * 4 * 256 ]) fc = tf . nn . sigmoid ( tf . nn . xw_plus_b ( h3_flat , weights = self . dis_w4 , biases = self . dis_b4 )) return fc Discriminator\u306b\u304a\u3051\u308b\u7573\u307f\u8fbc\u307f\u306f\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3067\u3042\u308b \u8ad6\u6587\u3067\u306f\u51fa\u529b\u3092\u6700\u5f8c\u306e\u7573\u307f\u8fbc\u307f\u5c64\u306e\u5024\u3092\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u3057\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u304c\u3001\u3053\u3053\u3067\u306f\u8ad6\u6587\u3067\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u540c\u3058\u3082\u306e\u3067\u306f\u306a\u3044\u305f\u3081\u5168\u7d50\u5408\u5c64\u306b\u6e21\u3057\u305f\u3082\u306e\u3092\u51fa\u529b\u3068\u3059\u308b\u3002 \u307e\u305fGAN\u3067\u306eDiscriminator\u3068\u540c\u3058\u3088\u3046\u306b\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u9069\u7528\u3057\u306a\u3044","title":"Discriminator\u306e\u5b9f\u88c5"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#dcgan_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class DCGAN (): def __init__ ( self , using_gpu ): self . device_name = '/cpu:0' if ( using_gpu ): self . device_name = '/gpu:0' print ( 'using : {} ' . format ( self . device_name )) with tf . device ( self . device_name ): self . G_is_train = tf . placeholder ( tf . bool ) self . D_is_train = tf . placeholder ( tf . bool ) self . input_X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 )) # t0\u306f0\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3057\u3001t1\u306f1\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308b self . label_t0 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) self . label_t1 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # Generator self . generator = Generator ( device_name = self . device_name ) # \u751f\u6210\u30e2\u30c7\u30eb\u306b\u5fc5\u8981\u306a\u30ce\u30a4\u30ba\u306e\u5165\u308c\u7269 self . gen_z = tf . placeholder ( tf . float32 , shape = ( None , 100 )) # Discrimitor self . discrimitor = Discrimitor ( device_name = self . device_name ) # weight decay gen_norm_term = tf . nn . l2_loss ( self . generator . gen_w2 ) + tf . nn . l2_loss ( self . generator . gen_w3 ) gen_lambda_ = 0.001 dis_norm_term = tf . nn . l2_loss ( self . discrimitor . dis_w2 ) + tf . nn . l2_loss ( self . discrimitor . dis_w3 ) dis_lambda_ = 0.001 # \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c input_X = self . discrimitor . run ( self . input_X , is_train = self . D_is_train , device_name = self . device_name ) # \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c generated_X = self . discrimitor . run ( self . generator . run ( z = self . gen_z , is_train = self . G_is_train , device_name = self . device_name ), is_train = self . D_is_train , device_name = self . device_name ) self . dis_entropy_X = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = input_X ) self . dis_entropy_G = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t0 , logits = generated_X ) self . dis_loss = tf . reduce_mean ( self . dis_entropy_X + self . dis_entropy_G ) + dis_norm_term * dis_lambda_ self . gen_entropy = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = generated_X ) self . gen_loss = tf . reduce_mean ( self . gen_entropy ) #+ gen_norm_term * gen_lambda_ # \u6700\u9069\u5316\u3059\u308b\u969b\u306bD\u306a\u3089D\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3001G\u306a\u3089G\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3059\u308b\u3088\u3046\u306b\u3057\u305f\u3044\u306e\u3067\u30e2\u30c7\u30eb\u5225\u306e\u5909\u6570\u3092\u53d6\u5f97\u3059\u308b dis_vars = [ x for x in tf . trainable_variables () if \"dis_\" in x . name ] gen_vars = [ x for x in tf . trainable_variables () if \"gen_\" in x . name ] # \u8b58\u5225\u30e2\u30c7\u30ebD\u306e\u6700\u9069\u5316 self . opt_d = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.1 ) . minimize ( self . dis_loss , var_list = [ dis_vars ]) # \u751f\u6210\u30e2\u30c7\u30ebG\u306e\u6700\u9069\u5316 self . opt_g = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.5 ) . minimize ( self . gen_loss , var_list = [ gen_vars ]) def train ( self , X_train = None , batch_size = 100 , epoch_num = 1000 , imgpath = './mnist_DCGAN_images/' , ckptpath = './mnist_DCGAN_checkpoints/' , log_file = 'mnist_DCGAN_loss_log.csv' , init = False ): if X_train is None : raise TypeError ( \"X_train is None\" ) # \u8a13\u7df4\u9014\u4e2d\u3067\u751f\u6210\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3057\u3066\u4fdd\u5b58\u3057\u305f\u3044\u306e\u3067\u305d\u306e\u4fdd\u5b58\u5148\u306e\u4f5c\u6210 p = Path ( imgpath ) if not ( p . is_dir ()): p . mkdir () # \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u306e\u4fdd\u5b58\u5148 ckpt_p = Path ( ckptpath ) if not ( ckpt_p . is_dir ()): ckpt_p . mkdir () config = tf . ConfigProto () config . gpu_options . allow_growth = True saver = tf . train . Saver () sess = tf . Session () if ( init ): sess . run ( tf . global_variables_initializer ()) print ( 'Initialize' ) ckpt = tf . train . get_checkpoint_state ( str ( ckpt_p . absolute ())) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) step = len ( X_train ) // batch_size #step = mnist.train.num_examples // batch_size # \u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u30df\u30cb\u30d0\u30c3\u30c1 t1_batch = np . ones (( batch_size , 1 ), dtype = np . float32 ) t0_batch = np . zeros (( batch_size , 1 ), dtype = np . float32 ) for epoch in range ( epoch_num ): perm = np . random . permutation ( len ( X_train )) # \uff11\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u304b\u304b\u308b\u6642\u9593\u306e\u8a08\u6e2c start = time . time () for k in range ( step ): #X_batch = mnist.train.next_batch(batch_size)[0] /255. X_batch = X_train [ perm ][ k * batch_size :( k + 1 ) * batch_size ] # Train Discrimitor # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_d , feed_dict = { self . input_X : X_batch , self . D_is_train : True , self . G_is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) if k % 1 == 0 : # Train Generator # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_g , feed_dict = { self . gen_z : noise_z , self . D_is_train : False , self . G_is_train : True , self . generator . keep_prob : 0.5 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch }) # 1epoch\u7d42\u4e86\u6642\u306e\u640d\u5931\u3092\u8868\u793a noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) train_dis_loss = sess . run ( self . dis_loss , feed_dict = { self . input_X : X_batch , self . D_is_train : False , self . G_is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) train_gen_loss = sess . run ( self . gen_loss , feed_dict = { self . gen_z : noise_z , self . D_is_train : False , self . G_is_train : False , self . generator . keep_prob : 1.0 , self . generator . batch_size : batch_size , self . label_t1 : t1_batch }) print ( \"[Train] epoch: %d , dis loss: %f , gen loss : %f Time : %f \" % ( epoch , train_dis_loss , train_gen_loss , time . time () - start )) f = open ( log_file , 'a' ) log_writer = csv . writer ( f , lineterminator = ' \\n ' ) loss_list = [] loss_list . append ( epoch ) loss_list . append ( train_dis_loss ) loss_list . append ( train_gen_loss ) # \u640d\u5931\u306e\u5024\u3092\u66f8\u304d\u8fbc\u3080 log_writer . writerow ( loss_list ) f . close () saver . save ( sess , str ( ckpt_p . absolute ()) + '/DCGAN-mnist' ) # 10epoch\u7d42\u4e86\u6bce\u306b\u751f\u6210\u30e2\u30c7\u30eb\u304b\u30891\u679a\u306e\u753b\u50cf\u3092\u751f\u6210\u3059\u308b if epoch % 2 == 0 : noise_z = np . random . uniform ( - 1 , 1 , size = [ 5 , 100 ]) . astype ( np . float32 ) z_const = tf . constant ( noise_z , dtype = tf . float32 ) gen_imgs = (( sess . run ( self . generator . run ( z_const , is_train = False ), feed_dict = { self . generator . keep_prob : 1.0 , self . generator . batch_size : 5 }) * 0.5 ) + 0.5 ) * 255. for i in range ( 0 , 5 ): Image . fromarray ( gen_imgs [ i ] . reshape ( 28 , 28 )) . convert ( 'RGB' ) . save ( str ( p . absolute ()) + '/generate_img_epoch {0} _ {1} .jpg' . format ( epoch , i ))","title":"DCGAN"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#_1","text":"Generator\u306e\u51fa\u529b\u306b\u6d3b\u6027\u5316\u95a2\u6570Tanh\u3092\u4f7f\u3063\u3066\u3044\u308b\u305f\u3081\u3001 [-1,1] \u306e\u7bc4\u56f2\u306e\u5024\u3092\u51fa\u529b\u304c\u53d6\u308a\u3046\u308b\u3002 \u305d\u306e\u305f\u3081\u8a13\u7df4\u30c7\u30fc\u30bf\u3082 [-1,1] \u306e\u7bc4\u56f2\u306e\u5024\u3092\u53d6\u308b\u3088\u3046\u306b\u524d\u51e6\u7406\u3092\u884c\u3046\u5fc5\u8981\u304c\u3042\u308b\u3002 1 X_train = ( dataset . train . images - 0.5 ) / 0.5 \u5b66\u7fd2\u3092\u884c\u3046 1 2 3 4 5 6 7 8 9 10 11 12 13 import tensorflow as tf device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : raise SystemError ( 'GPU device not found' ) print ( 'Found GPU at: {} ' . format ( device_name )) model = DCGAN ( using_gpu = True ) model . train ( X_train = X_train , batch_size = 100 , epoch_num = 51 , init = True ) 50epoch\u3067\u6b21\u306e\u3088\u3046\u306a\u753b\u50cf\u304c\u751f\u6210\u3055\u308c\u305f \u5b66\u7fd2\u7d4c\u904e","title":"\u5b66\u7fd2"},{"location":"model_DCGAN/Tensorflow_DCGAN_mnist/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/DCGAN_mnist.ipynb","title":"Notebook"},{"location":"model_GAN/Tensorflow_GAN_mnist/","text":"MNIST GAN(Generative Adversarial Network) GAN\u306e\u8ad6\u6587\u306f\u3053\u3061\u3089 GAN\u306f\u6575\u5bfe\u5b66\u7fd2\u3068\u547c\u3070\u308c\u308b2014\u5e74\u9803\u306b\u63d0\u6848\u3055\u308c\u305f\u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u4e00\u7a2e\u306e\u624b\u6cd5\u3067\u3042\u308b\u3002 GAN\u306f Generator (\u751f\u6210\u30e2\u30c7\u30eb)\u3068 Discriminator (\u8b58\u5225\u30e2\u30c7\u30eb)\u306e2\u3064\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30e2\u30c7\u30eb\u3092\u6301\u3064\u3002 Generator\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u8fd1\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b \u6559\u5e2b\u306a\u3057 \u3067\u5b66\u7fd2\u3059\u308b\u3002\u53cd\u5bfe\u306bDiscriminator\u306f\u4e0e\u3048\u3089\u308c\u305f\u5165\u529b\u304c\u8a13\u7df4\u30c7\u30fc\u30bf(\u672c\u7269)\u304b\u3001Generator\u304c\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf(\u507d\u7269)\u304b\u3092\u8b58\u5225\u3067\u304d\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3059\u308b\u3002\u3053\u306e\u3088\u3046\u306b2\u3064\u306e\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u306e\u95a2\u4fc2\u306f \u30c8\u30ec\u30fc\u30c9\u30aa\u30d5 \u3067\u3042\u308b\u3002 GAN\u306f\u3053\u306e2\u3064\u306e\u30e2\u30c7\u30eb\u3092\u540c\u6642\u306b\u5b66\u7fd2\u3057\u3001\u6700\u7d42\u7684\u306b\u306fGenerator\u304c\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u8fd1\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3057\u3001\u305d\u306e\u6642Discriminator\u306e\u4e88\u6e2c\u3068\u3057\u3066\u306f\u672c\u7269\u3088\u507d\u7269\u306e\u898b\u5206\u3051\u304c\u3064\u304b\u306a\u3044\u3001\u3059\u306a\u308f\u3061\u78ba\u7387\u3068\u3057\u3066\u66d6\u6627\u306a50%\u3068\u3044\u3063\u305f\u5024\u3092\u8fd4\u3059\u3088\u3046\u306b\u306a\u308b\u306e\u304c\u7406\u60f3\u3067\u3042\u308b\u3002 GAN\u306e\u69cb\u9020 GAN\u304c\u6301\u3064\u4e21\u30e2\u30c7\u30eb\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u69cb\u7bc9\u3055\u308c\u308b\u3002 \u3053\u3053\u3067\u306f\u30de\u30eb\u30c1\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3(MLP)\u3092\u7528\u3044\u308b MNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u306e\u753b\u50cf(28*28)\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3068\u3057\u3066Generator\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308b Generator\u306e\u5165\u529b\u306f100\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3067\u3001\u9589\u533a\u9593 [-1,1] \u306e\u4e00\u69d8\u5206\u5e03\u306b\u5f93\u3046\u3082\u306e\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u53d6\u3063\u3066\u304f\u308b\u3002 Discriminator\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308b Discriminator\u306e\u51fa\u529b\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u3063\u3066\u6d3b\u6027\u5316\u3057\u3001 0 <= x <= 1 \u306e\u78ba\u7387(\u507d\u7269\u304b\u672c\u7269\u304b)\u3068\u3057\u3066\u51fa\u529b\u3092\u5f97\u308b\u3002 \u3053\u3053\u3067\u306f\u5024\u304c1\u306b\u8fd1\u3044\u307b\u3069\u672c\u7269\u3067\u3042\u308b\u3068\u3059\u308b\u3002 GAN\u306e\u76ee\u7684\u95a2\u6570 Generator\u306e\u76ee\u7684\u95a2\u6570\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b m \u306f\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u8868\u3057\u3066\u3044\u308b\u3002 \u03b8g \u306f\u5b66\u7fd2\u4fc2\u6570\u3067\u3042\u308b\u3002\u3053\u306e\u5f0f\u304c\u6700\u5c0f\u306e\u5024\u3092\u53d6\u308b\u3088\u3046\u306bGenerator\u304c\u6301\u3064\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u78ba\u7387\u52fe\u914d\u6cd5\u3092\u7528\u3044\u3066\u66f4\u65b0\u3057\u3066\u3044\u304f\u3002\u305d\u306e\u305f\u3081 logD(G(z)) \u306e D(G(z)) \u304c1\u3092\u53d6\u308b\u3088\u3046\u306b\u3059\u308b\u3002\u3059\u306a\u308f\u3061Discriminator\u306bGenerator\u304c\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u305f\u6642\u306b\u672c\u7269\u3068\u591a\u304f\u8b58\u5225\u3055\u308c\u308b\u3088\u3046\u306b\u3059\u308b\u3002 Discriminator\u306e\u76ee\u7684\u95a2\u6570\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b \u03b8d \u306f\u5b66\u7fd2\u4fc2\u6570\u3067\u3042\u308b\u3002\u7b2c\u4e00\u9805\u306e logD(x) \u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u3066\u304a\u308a\u3001\u672c\u7269 = 1 \u3092\u5272\u308a\u5f53\u3066\u308b\u3088\u3046\u306b\u3001\u7b2c\u4e8c\u9805\u306e logD(G(z)) \u306f \u507d\u7269 = 0\u3092\u5272\u308a\u5f53\u3066\u308b\u3088\u3046\u306b\u8b58\u5225\u3059\u308b\u3002\u3053\u306e\u5f0f\u304c\u6700\u5c0f\u306e\u5024\u3092\u3068\u308b\u3088\u3046\u306bDiscriminator\u304c\u6301\u3064\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u78ba\u7387\u52fe\u914d\u6cd5\u3092\u7528\u3044\u3066\u66f4\u65b0\u3057\u3066\u3044\u304f\u3002 Tensorflow\u3092\u7528\u3044\u305fGAN\u306e\u5b9f\u88c5 Generator\u306e\u5b9f\u88c5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Generator (): def __init__ ( self , device_name = '/cpu:0' ): # Generator parameter with tf . device ( device_name ): self . gen_w1 = tf . Variable ( tf . truncated_normal ( shape = [ 100 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w1\" ) self . gen_b1 = tf . Variable ( tf . truncated_normal ( shape = [ 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b1\" ) self . gen_w2 = tf . Variable ( tf . truncated_normal ([ 256 , 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w2\" ) self . gen_b2 = tf . Variable ( tf . truncated_normal ( shape = [ 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b2\" ) self . gen_w3 = tf . Variable ( tf . truncated_normal ([ 512 , 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w3\" ) self . gen_b3 = tf . Variable ( tf . truncated_normal ( shape = [ 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b3\" ) self . gen_w4 = tf . Variable ( tf . truncated_normal ([ 1024 , 28 * 28 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w4\" ) self . gen_b4 = tf . Variable ( tf . truncated_normal ( shape = [ 28 * 28 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b4\" ) self . gen_scale_w1 = tf . Variable ( tf . ones ([ 256 ]), name = \"gen_scale_w1\" ) self . gen_offset_w1 = tf . Variable ( tf . zeros ([ 256 ]), name = \"gen_offset_w1\" ) self . gen_scale_w2 = tf . Variable ( tf . ones ([ 512 ]), name = \"gen_scale_w2\" ) self . gen_offset_w2 = tf . Variable ( tf . zeros ([ 512 ]), name = \"gen_offset_w2\" ) self . gen_scale_w3 = tf . Variable ( tf . ones ([ 1024 ]), name = \"gen_scale_w3\" ) self . gen_offset_w3 = tf . Variable ( tf . zeros ([ 1024 ]), name = \"gen_offset_w3\" ) self . keep_prob = tf . placeholder ( tf . float32 ) def run ( self , z , is_train , device_name = '/cpu:0' ): with tf . device ( device_name ): h1 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( z , self . gen_w1 , self . gen_b1 ), alpha = 0.2 ) h1 = batch_norm ( h1 , self . gen_scale_w1 , self . gen_offset_w1 ,[ 0 ], is_train , device_name ) h2 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h1 , self . gen_w2 , self . gen_b2 ), alpha = 0.2 ) h2 = batch_norm ( h2 , self . gen_scale_w2 , self . gen_offset_w2 ,[ 0 ], is_train , device_name ) h3 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h2 , self . gen_w3 , self . gen_b3 ), alpha = 0.2 ) h3 = batch_norm ( h3 , self . gen_scale_w3 , self . gen_offset_w3 ,[ 0 ], is_train , device_name ) h3_drop = tf . nn . dropout ( h3 , self . keep_prob ) fc = tf . nn . sigmoid ( tf . nn . xw_plus_b ( h3_drop , self . gen_w4 , self . gen_b4 )) run() \u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u3002\u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066 leaky_relu \u95a2\u6570\u3092\u9069\u7528\u3059\u308b\u3002\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066 alpha=0.2 \u3092\u63a1\u7528\u3059\u308b\u3002\u307e\u305f \u30d0\u30c3\u30c1\u6b63\u898f\u5316 (Batch normalization)\u3082\u9069\u7528\u3059\u308b\u3002 \u6ce8\u610f\u70b9\u3068\u3057\u3066\u3001Generator\u306e\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b Discriminator\u3068\u533a\u5225\u304c\u3064\u304f\u3088\u3046\u306b name=\"gen_~\" \u3068\u3044\u3063\u305f\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u3053\u3068\u3002(\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\u6642\u306b\u5fc5\u8981) Discriminator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Discrimitor (): def __init__ ( self , device_name = '/cpu:0' ): # Discrimitor parameter with tf . device ( device_name ): self . dis_w1 = tf . Variable ( tf . truncated_normal ([ 28 * 28 , 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w1\" ) self . dis_b1 = tf . Variable ( tf . truncated_normal ([ 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b1\" ) self . dis_w2 = tf . Variable ( tf . truncated_normal ([ 1024 , 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w2\" ) self . dis_b2 = tf . Variable ( tf . truncated_normal ([ 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b2\" ) self . dis_w3 = tf . Variable ( tf . truncated_normal ([ 512 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w3\" ) self . dis_b3 = tf . Variable ( tf . truncated_normal ([ 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b3\" ) self . dis_w4 = tf . Variable ( tf . truncated_normal ([ 256 , 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w4\" ) self . dis_b4 = tf . Variable ( tf . truncated_normal ([ 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b4\" ) def run ( self , x , device_name = '/cpu:0' ): with tf . device ( device_name ): h1 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( x , self . dis_w1 , self . dis_b1 ), alpha = 0.2 ) h2 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h1 , self . dis_w2 , self . dis_b2 ), alpha = 0.2 ) h3 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h2 , self . dis_w3 , self . dis_b3 ), alpha = 0.2 ) fc = tf . nn . sigmoid ( tf . nn . xw_plus_b ( h3 , self . dis_w4 , self . dis_b4 )) return fc Discrimitor\u306b\u306f\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u9069\u7528\u3057\u306a\u3044\u3002 \u3053\u3061\u3089\u3082Generator\u3068\u533a\u5225\u304c\u3064\u304f\u3088\u3046\u306b\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u3053\u3068\u3002 GAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class GAN (): def __init__ ( self , using_gpu ): self . device_name = '/cpu:0' if ( using_gpu ): self . device_name = '/gpu:0' print ( 'using : {} ' . format ( self . device_name )) with tf . device ( self . device_name ): # Generator\u306eBatchnorm\u306b\u5fc5\u8981 self . is_train = tf . placeholder ( tf . bool ) # t0\u306f0\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3057\u3001t1\u306f1\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308b self . label_t0 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) self . label_t1 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # Generator self . generator = Generator ( device_name = self . device_name ) # \u751f\u6210\u30e2\u30c7\u30eb\u306b\u5fc5\u8981\u306a\u30ce\u30a4\u30ba\u306e\u5165\u308c\u7269 self . gen_z = tf . placeholder ( tf . float32 , shape = ( None , 100 )) # Discrimitor self . discrimitor = Discrimitor ( device_name = self . device_name ) # Discriminator\u306e\u5165\u529b\u306e\u5165\u308c\u7269 self . input_X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 )) # \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c\u7d50\u679c input_X = self . discrimitor . run ( self . input_X , device_name = self . device_name ) # \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c\u7d50\u679c generated_X = self . discrimitor . run ( self . generator . run ( z = self . gen_z , is_train = self . is_train , device_name = self . device_name )) self . dis_entropy_X = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = input_X ) self . dis_entropy_G = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t0 , logits = generated_X ) # Discriminator\u306e\u76ee\u7684\u95a2\u6570 self . dis_loss = tf . reduce_mean ( self . dis_entropy_X + self . dis_entropy_G ) self . gen_entropy = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = generated_X ) # Generator\u306e\u76ee\u7684\u95a2\u6570 self . gen_loss = tf . reduce_mean ( self . gen_entropy ) # \u6700\u9069\u5316\u3059\u308b\u969b\u306bD\u306a\u3089D\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3001G\u306a\u3089G\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3059\u308b\u3088\u3046\u306b\u3057\u305f\u3044\u306e\u3067\u30e2\u30c7\u30eb\u5225\u306e\u5909\u6570\u3092\u53d6\u5f97\u3059\u308b dis_vars = [ x for x in tf . trainable_variables () if \"dis_\" in x . name ] gen_vars = [ x for x in tf . trainable_variables () if \"gen_\" in x . name ] # \u8b58\u5225\u30e2\u30c7\u30ebD\u306e\u6700\u9069\u5316 self . opt_d = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.1 ) . minimize ( self . dis_loss , var_list = [ dis_vars ]) # \u751f\u6210\u30e2\u30c7\u30ebG\u306e\u6700\u9069\u5316 self . opt_g = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.5 ) . minimize ( self . gen_loss , var_list = [ gen_vars ]) \u3053\u3053\u3067 1 2 dis_vars = [ x for x in tf . trainable_variables () if \"dis_\" in x . name ] gen_vars = [ x for x in tf . trainable_variables () if \"gen_\" in x . name ] \u3068\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u306e\u306f\u3001Generator\u3067\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\u306b\u304a\u3044\u3066Discriminator\u3092\u7d4c\u7531\u3057\u3066\u304a\u308a self.opt_g \u306e\u5f15\u6570 var_list=[gen_vars] \u3068\u3044\u3063\u305f\u66f4\u65b0\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068Generator\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u307f\u3092\u66f4\u65b0\u3057\u305f\u3044\u306e\u306b Discriminator\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u307e\u3067\u66f4\u65b0\u3057\u3066\u3057\u307e\u3046 \u304b\u3089\u3067\u3042\u308b\u3002 \u3053\u306e\u3068\u304dDiscriminator\u306fGenerator\u304b\u3089\u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u672c\u7269\u3068\u8b58\u5225\u3059\u308b\u3088\u3046\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3057\u3066\u3057\u307e\u3046\u306e\u3067\u5b66\u7fd2\u304c\u826f\u3044\u65b9\u5411\u306b\u9032\u307e\u306a\u304f\u306a\u3063\u3066\u3057\u307e\u3046\u3002 Discrimitor\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u306b\u304a\u3044\u3066\u3082Generator\u3092\u7d4c\u7531\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u540c\u69d8\u306e\u3053\u3068\u304c\u8d77\u3053\u308b\u3002\u305d\u306e\u305f\u3081\u524d\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u533a\u5225\u304c\u3064\u304f\u3088\u3046\u306b\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308b\u3002 \u6b21\u306b GAN \u306e\u30af\u30e9\u30b9\u306b\u6b21\u306e\u95a2\u6570\u3092\u8ffd\u52a0\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def train ( self , X_train = None , batch_size = 100 , epoch_num = 1000 , imgpath = './mnist_GAN_images/' , ckptpath = './mnist_GAN_checkpoints/' , log_file = 'mnist_GAN_loss_log.csv' , init = False ): if X_train is None : raise TypeError ( \"X_train is None\" ) # \u8a13\u7df4\u9014\u4e2d\u3067\u751f\u6210\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3057\u3066\u4fdd\u5b58\u3057\u305f\u3044\u306e\u3067\u305d\u306e\u4fdd\u5b58\u5148\u306e\u4f5c\u6210 p = Path ( imgpath ) if not ( p . is_dir ()): p . mkdir () # \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u306e\u4fdd\u5b58\u5148 ckpt_p = Path ( ckptpath ) if not ( ckpt_p . is_dir ()): ckpt_p . mkdir () config = tf . ConfigProto () config . gpu_options . allow_growth = True saver = tf . train . Saver () sess = tf . Session () if ( init ): sess . run ( tf . global_variables_initializer ()) print ( 'Initialize' ) ckpt = tf . train . get_checkpoint_state ( str ( ckpt_p . absolute ())) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) step = len ( X_train ) // batch_size # \u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u30df\u30cb\u30d0\u30c3\u30c1 t1_batch = np . ones (( batch_size , 1 ), dtype = np . float32 ) t0_batch = np . zeros (( batch_size , 1 ), dtype = np . float32 ) for epoch in range ( epoch_num ): #\u5404\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3059\u308b perm = np . random . permutation ( len ( X_train )) # \uff11\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u304b\u304b\u308b\u6642\u9593\u306e\u8a08\u6e2c start = time . time () for k in range ( step ): X_batch = X_train [ perm ][ k * batch_size :( k + 1 ) * batch_size ] # Train Discrimitor # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_d , feed_dict = { self . input_X : X_batch , self . is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) if k % 1 == 0 : # Train Generator # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_g , feed_dict = { self . gen_z : noise_z , self . is_train : True , self . generator . keep_prob : 0.5 , self . label_t1 : t1_batch }) # 1epoch\u7d42\u4e86\u6642\u306e\u640d\u5931\u3092\u8868\u793a noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) train_dis_loss = sess . run ( self . dis_loss , feed_dict = { self . input_X : X_batch , self . is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) train_gen_loss = sess . run ( self . gen_loss , feed_dict = { self . gen_z : noise_z , self . is_train : False , self . generator . keep_prob : 1.0 , self . label_t1 : t1_batch }) print ( \"[Train] epoch: %d , dis loss: %f , gen loss : %f Time : %f \" % ( epoch , train_dis_loss , train_gen_loss , time . time () - start )) saver . save ( sess , str ( ckpt_p . absolute ()) + '/GAN-mnist' ) # loss\u306e\u8a18\u9332 f = open ( log_file , 'a' ) log_writer = csv . writer ( f , lineterminator = ' \\n ' ) loss_list = [] loss_list . append ( epoch ) loss_list . append ( train_dis_loss ) loss_list . append ( train_gen_loss ) # \u640d\u5931\u306e\u5024\u3092\u66f8\u304d\u8fbc\u3080 log_writer . writerow ( loss_list ) f . close () # 10epoch\u7d42\u4e86\u6bce\u306b\u751f\u6210\u30e2\u30c7\u30eb\u304b\u30895\u679a\u306e\u753b\u50cf\u3092\u751f\u6210\u3059\u308b if epoch % 10 == 0 : noise_z = np . random . uniform ( - 1 , 1 , size = [ 5 , 100 ]) . astype ( np . float32 ) z_const = tf . constant ( noise_z , dtype = tf . float32 ) gen_imgs = sess . run ( self . generator . run ( z_const , is_train = False ), feed_dict = { self . generator . keep_prob : 1.0 }) * 255. for i , img in enumerate ( gen_imgs ): Image . fromarray ( img . reshape ( 28 , 28 )) . convert ( 'RGB' ) . save ( str ( p . absolute ()) + '/generate_img_epoch {0} _ {1} .jpg' . format ( epoch , i ) ) GPU\u3092\u7528\u3044\u3066\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306f\u8a13\u7df4\u3059\u308b\u524d\u306b 1 2 3 4 5 6 7 8 import tensorflow as tf device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : raise SystemError ( 'GPU device not found' ) print ( 'Found GPU at: {} ' . format ( device_name )) model = GAN ( using_gpu = True ) \u3068\u3059\u308b\u3053\u3068\u3002\u4f7f\u7528\u3057\u306a\u3044\u5834\u5408\u306f model = GAN(using_gpu=False) \u306e\u307f\u3067\u554f\u984c\u306a\u3044 \u5b66\u7fd2 1 2 3 4 5 6 7 8 9 10 X_train = np . r_ [ dataset . train . images ] model = GAN ( using_gpu = True ) model . train ( X_train = X_train , batch_size = 100 , epoch_num = 71 , init = True , ckptpath = './mnist_GAN_checkpoints_adam_gpu/' , imgpath = './mnist_GAN_images_gpu/' ) batch_size \u3084 epoch_num \u306f\u4efb\u610f\u5909\u66f4 MNIST\u306e\u5168\u8a13\u7df4\u30c7\u30fc\u30bf(55000\u4ef6)\u3092\u7528\u3044\u306670epoch\u307e\u3067\u306e\u5b66\u7fd2\u3067\u751f\u6210\u3055\u308c\u305f\u7d50\u679c\u7269 Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/GAN_mnist.ipynb","title":"MNIST GAN(Generative Adversarial Network)"},{"location":"model_GAN/Tensorflow_GAN_mnist/#mnist-gangenerative-adversarial-network","text":"GAN\u306e\u8ad6\u6587\u306f\u3053\u3061\u3089 GAN\u306f\u6575\u5bfe\u5b66\u7fd2\u3068\u547c\u3070\u308c\u308b2014\u5e74\u9803\u306b\u63d0\u6848\u3055\u308c\u305f\u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0\u306e\u4e00\u7a2e\u306e\u624b\u6cd5\u3067\u3042\u308b\u3002 GAN\u306f Generator (\u751f\u6210\u30e2\u30c7\u30eb)\u3068 Discriminator (\u8b58\u5225\u30e2\u30c7\u30eb)\u306e2\u3064\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30e2\u30c7\u30eb\u3092\u6301\u3064\u3002 Generator\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u8fd1\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b \u6559\u5e2b\u306a\u3057 \u3067\u5b66\u7fd2\u3059\u308b\u3002\u53cd\u5bfe\u306bDiscriminator\u306f\u4e0e\u3048\u3089\u308c\u305f\u5165\u529b\u304c\u8a13\u7df4\u30c7\u30fc\u30bf(\u672c\u7269)\u304b\u3001Generator\u304c\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf(\u507d\u7269)\u304b\u3092\u8b58\u5225\u3067\u304d\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3059\u308b\u3002\u3053\u306e\u3088\u3046\u306b2\u3064\u306e\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u306e\u95a2\u4fc2\u306f \u30c8\u30ec\u30fc\u30c9\u30aa\u30d5 \u3067\u3042\u308b\u3002 GAN\u306f\u3053\u306e2\u3064\u306e\u30e2\u30c7\u30eb\u3092\u540c\u6642\u306b\u5b66\u7fd2\u3057\u3001\u6700\u7d42\u7684\u306b\u306fGenerator\u304c\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u8fd1\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3057\u3001\u305d\u306e\u6642Discriminator\u306e\u4e88\u6e2c\u3068\u3057\u3066\u306f\u672c\u7269\u3088\u507d\u7269\u306e\u898b\u5206\u3051\u304c\u3064\u304b\u306a\u3044\u3001\u3059\u306a\u308f\u3061\u78ba\u7387\u3068\u3057\u3066\u66d6\u6627\u306a50%\u3068\u3044\u3063\u305f\u5024\u3092\u8fd4\u3059\u3088\u3046\u306b\u306a\u308b\u306e\u304c\u7406\u60f3\u3067\u3042\u308b\u3002","title":"MNIST GAN(Generative Adversarial Network)"},{"location":"model_GAN/Tensorflow_GAN_mnist/#gan","text":"GAN\u304c\u6301\u3064\u4e21\u30e2\u30c7\u30eb\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u69cb\u7bc9\u3055\u308c\u308b\u3002 \u3053\u3053\u3067\u306f\u30de\u30eb\u30c1\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3(MLP)\u3092\u7528\u3044\u308b MNIST\u306e\u624b\u66f8\u304d\u6570\u5b57\u306e\u753b\u50cf(28*28)\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3068\u3057\u3066Generator\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308b Generator\u306e\u5165\u529b\u306f100\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u3067\u3001\u9589\u533a\u9593 [-1,1] \u306e\u4e00\u69d8\u5206\u5e03\u306b\u5f93\u3046\u3082\u306e\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u53d6\u3063\u3066\u304f\u308b\u3002 Discriminator\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308b Discriminator\u306e\u51fa\u529b\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u3063\u3066\u6d3b\u6027\u5316\u3057\u3001 0 <= x <= 1 \u306e\u78ba\u7387(\u507d\u7269\u304b\u672c\u7269\u304b)\u3068\u3057\u3066\u51fa\u529b\u3092\u5f97\u308b\u3002 \u3053\u3053\u3067\u306f\u5024\u304c1\u306b\u8fd1\u3044\u307b\u3069\u672c\u7269\u3067\u3042\u308b\u3068\u3059\u308b\u3002","title":"GAN\u306e\u69cb\u9020"},{"location":"model_GAN/Tensorflow_GAN_mnist/#gan_1","text":"Generator\u306e\u76ee\u7684\u95a2\u6570\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b m \u306f\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u8868\u3057\u3066\u3044\u308b\u3002 \u03b8g \u306f\u5b66\u7fd2\u4fc2\u6570\u3067\u3042\u308b\u3002\u3053\u306e\u5f0f\u304c\u6700\u5c0f\u306e\u5024\u3092\u53d6\u308b\u3088\u3046\u306bGenerator\u304c\u6301\u3064\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u78ba\u7387\u52fe\u914d\u6cd5\u3092\u7528\u3044\u3066\u66f4\u65b0\u3057\u3066\u3044\u304f\u3002\u305d\u306e\u305f\u3081 logD(G(z)) \u306e D(G(z)) \u304c1\u3092\u53d6\u308b\u3088\u3046\u306b\u3059\u308b\u3002\u3059\u306a\u308f\u3061Discriminator\u306bGenerator\u304c\u751f\u6210\u3057\u305f\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u305f\u6642\u306b\u672c\u7269\u3068\u591a\u304f\u8b58\u5225\u3055\u308c\u308b\u3088\u3046\u306b\u3059\u308b\u3002 Discriminator\u306e\u76ee\u7684\u95a2\u6570\u306f\u6b21\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3055\u308c\u308b \u03b8d \u306f\u5b66\u7fd2\u4fc2\u6570\u3067\u3042\u308b\u3002\u7b2c\u4e00\u9805\u306e logD(x) \u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u6e21\u3057\u3066\u304a\u308a\u3001\u672c\u7269 = 1 \u3092\u5272\u308a\u5f53\u3066\u308b\u3088\u3046\u306b\u3001\u7b2c\u4e8c\u9805\u306e logD(G(z)) \u306f \u507d\u7269 = 0\u3092\u5272\u308a\u5f53\u3066\u308b\u3088\u3046\u306b\u8b58\u5225\u3059\u308b\u3002\u3053\u306e\u5f0f\u304c\u6700\u5c0f\u306e\u5024\u3092\u3068\u308b\u3088\u3046\u306bDiscriminator\u304c\u6301\u3064\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u78ba\u7387\u52fe\u914d\u6cd5\u3092\u7528\u3044\u3066\u66f4\u65b0\u3057\u3066\u3044\u304f\u3002","title":"GAN\u306e\u76ee\u7684\u95a2\u6570"},{"location":"model_GAN/Tensorflow_GAN_mnist/#tensorflowgan","text":"","title":"Tensorflow\u3092\u7528\u3044\u305fGAN\u306e\u5b9f\u88c5"},{"location":"model_GAN/Tensorflow_GAN_mnist/#generator","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Generator (): def __init__ ( self , device_name = '/cpu:0' ): # Generator parameter with tf . device ( device_name ): self . gen_w1 = tf . Variable ( tf . truncated_normal ( shape = [ 100 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w1\" ) self . gen_b1 = tf . Variable ( tf . truncated_normal ( shape = [ 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b1\" ) self . gen_w2 = tf . Variable ( tf . truncated_normal ([ 256 , 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w2\" ) self . gen_b2 = tf . Variable ( tf . truncated_normal ( shape = [ 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b2\" ) self . gen_w3 = tf . Variable ( tf . truncated_normal ([ 512 , 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w3\" ) self . gen_b3 = tf . Variable ( tf . truncated_normal ( shape = [ 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b3\" ) self . gen_w4 = tf . Variable ( tf . truncated_normal ([ 1024 , 28 * 28 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_w4\" ) self . gen_b4 = tf . Variable ( tf . truncated_normal ( shape = [ 28 * 28 ], stddev = 0.02 , dtype = tf . float32 ), name = \"gen_b4\" ) self . gen_scale_w1 = tf . Variable ( tf . ones ([ 256 ]), name = \"gen_scale_w1\" ) self . gen_offset_w1 = tf . Variable ( tf . zeros ([ 256 ]), name = \"gen_offset_w1\" ) self . gen_scale_w2 = tf . Variable ( tf . ones ([ 512 ]), name = \"gen_scale_w2\" ) self . gen_offset_w2 = tf . Variable ( tf . zeros ([ 512 ]), name = \"gen_offset_w2\" ) self . gen_scale_w3 = tf . Variable ( tf . ones ([ 1024 ]), name = \"gen_scale_w3\" ) self . gen_offset_w3 = tf . Variable ( tf . zeros ([ 1024 ]), name = \"gen_offset_w3\" ) self . keep_prob = tf . placeholder ( tf . float32 ) def run ( self , z , is_train , device_name = '/cpu:0' ): with tf . device ( device_name ): h1 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( z , self . gen_w1 , self . gen_b1 ), alpha = 0.2 ) h1 = batch_norm ( h1 , self . gen_scale_w1 , self . gen_offset_w1 ,[ 0 ], is_train , device_name ) h2 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h1 , self . gen_w2 , self . gen_b2 ), alpha = 0.2 ) h2 = batch_norm ( h2 , self . gen_scale_w2 , self . gen_offset_w2 ,[ 0 ], is_train , device_name ) h3 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h2 , self . gen_w3 , self . gen_b3 ), alpha = 0.2 ) h3 = batch_norm ( h3 , self . gen_scale_w3 , self . gen_offset_w3 ,[ 0 ], is_train , device_name ) h3_drop = tf . nn . dropout ( h3 , self . keep_prob ) fc = tf . nn . sigmoid ( tf . nn . xw_plus_b ( h3_drop , self . gen_w4 , self . gen_b4 )) run() \u3067\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u3002\u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066 leaky_relu \u95a2\u6570\u3092\u9069\u7528\u3059\u308b\u3002\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066 alpha=0.2 \u3092\u63a1\u7528\u3059\u308b\u3002\u307e\u305f \u30d0\u30c3\u30c1\u6b63\u898f\u5316 (Batch normalization)\u3082\u9069\u7528\u3059\u308b\u3002 \u6ce8\u610f\u70b9\u3068\u3057\u3066\u3001Generator\u306e\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b Discriminator\u3068\u533a\u5225\u304c\u3064\u304f\u3088\u3046\u306b name=\"gen_~\" \u3068\u3044\u3063\u305f\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u3053\u3068\u3002(\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\u6642\u306b\u5fc5\u8981)","title":"Generator\u306e\u5b9f\u88c5"},{"location":"model_GAN/Tensorflow_GAN_mnist/#discriminator","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Discrimitor (): def __init__ ( self , device_name = '/cpu:0' ): # Discrimitor parameter with tf . device ( device_name ): self . dis_w1 = tf . Variable ( tf . truncated_normal ([ 28 * 28 , 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w1\" ) self . dis_b1 = tf . Variable ( tf . truncated_normal ([ 1024 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b1\" ) self . dis_w2 = tf . Variable ( tf . truncated_normal ([ 1024 , 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w2\" ) self . dis_b2 = tf . Variable ( tf . truncated_normal ([ 512 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b2\" ) self . dis_w3 = tf . Variable ( tf . truncated_normal ([ 512 , 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w3\" ) self . dis_b3 = tf . Variable ( tf . truncated_normal ([ 256 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b3\" ) self . dis_w4 = tf . Variable ( tf . truncated_normal ([ 256 , 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_w4\" ) self . dis_b4 = tf . Variable ( tf . truncated_normal ([ 1 ], stddev = 0.02 , dtype = tf . float32 ), name = \"dis_b4\" ) def run ( self , x , device_name = '/cpu:0' ): with tf . device ( device_name ): h1 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( x , self . dis_w1 , self . dis_b1 ), alpha = 0.2 ) h2 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h1 , self . dis_w2 , self . dis_b2 ), alpha = 0.2 ) h3 = tf . nn . leaky_relu ( tf . nn . xw_plus_b ( h2 , self . dis_w3 , self . dis_b3 ), alpha = 0.2 ) fc = tf . nn . sigmoid ( tf . nn . xw_plus_b ( h3 , self . dis_w4 , self . dis_b4 )) return fc Discrimitor\u306b\u306f\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u9069\u7528\u3057\u306a\u3044\u3002 \u3053\u3061\u3089\u3082Generator\u3068\u533a\u5225\u304c\u3064\u304f\u3088\u3046\u306b\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u3053\u3068\u3002","title":"Discriminator"},{"location":"model_GAN/Tensorflow_GAN_mnist/#gan_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class GAN (): def __init__ ( self , using_gpu ): self . device_name = '/cpu:0' if ( using_gpu ): self . device_name = '/gpu:0' print ( 'using : {} ' . format ( self . device_name )) with tf . device ( self . device_name ): # Generator\u306eBatchnorm\u306b\u5fc5\u8981 self . is_train = tf . placeholder ( tf . bool ) # t0\u306f0\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3057\u3001t1\u306f1\u306e\u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308b self . label_t0 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) self . label_t1 = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # Generator self . generator = Generator ( device_name = self . device_name ) # \u751f\u6210\u30e2\u30c7\u30eb\u306b\u5fc5\u8981\u306a\u30ce\u30a4\u30ba\u306e\u5165\u308c\u7269 self . gen_z = tf . placeholder ( tf . float32 , shape = ( None , 100 )) # Discrimitor self . discrimitor = Discrimitor ( device_name = self . device_name ) # Discriminator\u306e\u5165\u529b\u306e\u5165\u308c\u7269 self . input_X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 )) # \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c\u7d50\u679c input_X = self . discrimitor . run ( self . input_X , device_name = self . device_name ) # \u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u8b58\u5225\u4e88\u6e2c\u7d50\u679c generated_X = self . discrimitor . run ( self . generator . run ( z = self . gen_z , is_train = self . is_train , device_name = self . device_name )) self . dis_entropy_X = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = input_X ) self . dis_entropy_G = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t0 , logits = generated_X ) # Discriminator\u306e\u76ee\u7684\u95a2\u6570 self . dis_loss = tf . reduce_mean ( self . dis_entropy_X + self . dis_entropy_G ) self . gen_entropy = tf . nn . sigmoid_cross_entropy_with_logits ( labels = self . label_t1 , logits = generated_X ) # Generator\u306e\u76ee\u7684\u95a2\u6570 self . gen_loss = tf . reduce_mean ( self . gen_entropy ) # \u6700\u9069\u5316\u3059\u308b\u969b\u306bD\u306a\u3089D\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3001G\u306a\u3089G\u306e\u307f\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3059\u308b\u3088\u3046\u306b\u3057\u305f\u3044\u306e\u3067\u30e2\u30c7\u30eb\u5225\u306e\u5909\u6570\u3092\u53d6\u5f97\u3059\u308b dis_vars = [ x for x in tf . trainable_variables () if \"dis_\" in x . name ] gen_vars = [ x for x in tf . trainable_variables () if \"gen_\" in x . name ] # \u8b58\u5225\u30e2\u30c7\u30ebD\u306e\u6700\u9069\u5316 self . opt_d = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.1 ) . minimize ( self . dis_loss , var_list = [ dis_vars ]) # \u751f\u6210\u30e2\u30c7\u30ebG\u306e\u6700\u9069\u5316 self . opt_g = tf . train . AdamOptimizer ( 0.0002 , beta1 = 0.5 ) . minimize ( self . gen_loss , var_list = [ gen_vars ]) \u3053\u3053\u3067 1 2 dis_vars = [ x for x in tf . trainable_variables () if \"dis_\" in x . name ] gen_vars = [ x for x in tf . trainable_variables () if \"gen_\" in x . name ] \u3068\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u306e\u306f\u3001Generator\u3067\u306f\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\u306b\u304a\u3044\u3066Discriminator\u3092\u7d4c\u7531\u3057\u3066\u304a\u308a self.opt_g \u306e\u5f15\u6570 var_list=[gen_vars] \u3068\u3044\u3063\u305f\u66f4\u65b0\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068Generator\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u307f\u3092\u66f4\u65b0\u3057\u305f\u3044\u306e\u306b Discriminator\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u307e\u3067\u66f4\u65b0\u3057\u3066\u3057\u307e\u3046 \u304b\u3089\u3067\u3042\u308b\u3002 \u3053\u306e\u3068\u304dDiscriminator\u306fGenerator\u304b\u3089\u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u672c\u7269\u3068\u8b58\u5225\u3059\u308b\u3088\u3046\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3057\u3066\u3057\u307e\u3046\u306e\u3067\u5b66\u7fd2\u304c\u826f\u3044\u65b9\u5411\u306b\u9032\u307e\u306a\u304f\u306a\u3063\u3066\u3057\u307e\u3046\u3002 Discrimitor\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u306b\u304a\u3044\u3066\u3082Generator\u3092\u7d4c\u7531\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u540c\u69d8\u306e\u3053\u3068\u304c\u8d77\u3053\u308b\u3002\u305d\u306e\u305f\u3081\u524d\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u533a\u5225\u304c\u3064\u304f\u3088\u3046\u306b\u540d\u524d\u3092\u3064\u3051\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308b\u3002 \u6b21\u306b GAN \u306e\u30af\u30e9\u30b9\u306b\u6b21\u306e\u95a2\u6570\u3092\u8ffd\u52a0\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def train ( self , X_train = None , batch_size = 100 , epoch_num = 1000 , imgpath = './mnist_GAN_images/' , ckptpath = './mnist_GAN_checkpoints/' , log_file = 'mnist_GAN_loss_log.csv' , init = False ): if X_train is None : raise TypeError ( \"X_train is None\" ) # \u8a13\u7df4\u9014\u4e2d\u3067\u751f\u6210\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\u3057\u3066\u4fdd\u5b58\u3057\u305f\u3044\u306e\u3067\u305d\u306e\u4fdd\u5b58\u5148\u306e\u4f5c\u6210 p = Path ( imgpath ) if not ( p . is_dir ()): p . mkdir () # \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u306e\u4fdd\u5b58\u5148 ckpt_p = Path ( ckptpath ) if not ( ckpt_p . is_dir ()): ckpt_p . mkdir () config = tf . ConfigProto () config . gpu_options . allow_growth = True saver = tf . train . Saver () sess = tf . Session () if ( init ): sess . run ( tf . global_variables_initializer ()) print ( 'Initialize' ) ckpt = tf . train . get_checkpoint_state ( str ( ckpt_p . absolute ())) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) step = len ( X_train ) // batch_size # \u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u30df\u30cb\u30d0\u30c3\u30c1 t1_batch = np . ones (( batch_size , 1 ), dtype = np . float32 ) t0_batch = np . zeros (( batch_size , 1 ), dtype = np . float32 ) for epoch in range ( epoch_num ): #\u5404\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\u3059\u308b perm = np . random . permutation ( len ( X_train )) # \uff11\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u304b\u304b\u308b\u6642\u9593\u306e\u8a08\u6e2c start = time . time () for k in range ( step ): X_batch = X_train [ perm ][ k * batch_size :( k + 1 ) * batch_size ] # Train Discrimitor # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_d , feed_dict = { self . input_X : X_batch , self . is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) if k % 1 == 0 : # Train Generator # \u30ce\u30a4\u30ba\u4e8b\u524d\u5206\u5e03\u304b\u3089\u30ce\u30a4\u30ba\u3092\u30df\u30cb\u30d0\u30c3\u30c1\u5206\u53d6\u5f97 noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) sess . run ( self . opt_g , feed_dict = { self . gen_z : noise_z , self . is_train : True , self . generator . keep_prob : 0.5 , self . label_t1 : t1_batch }) # 1epoch\u7d42\u4e86\u6642\u306e\u640d\u5931\u3092\u8868\u793a noise_z = np . random . uniform ( - 1 , 1 , size = [ batch_size , 100 ]) . astype ( np . float32 ) train_dis_loss = sess . run ( self . dis_loss , feed_dict = { self . input_X : X_batch , self . is_train : False , self . gen_z : noise_z , self . generator . keep_prob : 1.0 , self . label_t1 : t1_batch , self . label_t0 : t0_batch }) train_gen_loss = sess . run ( self . gen_loss , feed_dict = { self . gen_z : noise_z , self . is_train : False , self . generator . keep_prob : 1.0 , self . label_t1 : t1_batch }) print ( \"[Train] epoch: %d , dis loss: %f , gen loss : %f Time : %f \" % ( epoch , train_dis_loss , train_gen_loss , time . time () - start )) saver . save ( sess , str ( ckpt_p . absolute ()) + '/GAN-mnist' ) # loss\u306e\u8a18\u9332 f = open ( log_file , 'a' ) log_writer = csv . writer ( f , lineterminator = ' \\n ' ) loss_list = [] loss_list . append ( epoch ) loss_list . append ( train_dis_loss ) loss_list . append ( train_gen_loss ) # \u640d\u5931\u306e\u5024\u3092\u66f8\u304d\u8fbc\u3080 log_writer . writerow ( loss_list ) f . close () # 10epoch\u7d42\u4e86\u6bce\u306b\u751f\u6210\u30e2\u30c7\u30eb\u304b\u30895\u679a\u306e\u753b\u50cf\u3092\u751f\u6210\u3059\u308b if epoch % 10 == 0 : noise_z = np . random . uniform ( - 1 , 1 , size = [ 5 , 100 ]) . astype ( np . float32 ) z_const = tf . constant ( noise_z , dtype = tf . float32 ) gen_imgs = sess . run ( self . generator . run ( z_const , is_train = False ), feed_dict = { self . generator . keep_prob : 1.0 }) * 255. for i , img in enumerate ( gen_imgs ): Image . fromarray ( img . reshape ( 28 , 28 )) . convert ( 'RGB' ) . save ( str ( p . absolute ()) + '/generate_img_epoch {0} _ {1} .jpg' . format ( epoch , i ) ) GPU\u3092\u7528\u3044\u3066\u5b66\u7fd2\u3092\u884c\u3046\u969b\u306f\u8a13\u7df4\u3059\u308b\u524d\u306b 1 2 3 4 5 6 7 8 import tensorflow as tf device_name = tf . test . gpu_device_name () if device_name != '/device:GPU:0' : raise SystemError ( 'GPU device not found' ) print ( 'Found GPU at: {} ' . format ( device_name )) model = GAN ( using_gpu = True ) \u3068\u3059\u308b\u3053\u3068\u3002\u4f7f\u7528\u3057\u306a\u3044\u5834\u5408\u306f model = GAN(using_gpu=False) \u306e\u307f\u3067\u554f\u984c\u306a\u3044","title":"GAN"},{"location":"model_GAN/Tensorflow_GAN_mnist/#_1","text":"1 2 3 4 5 6 7 8 9 10 X_train = np . r_ [ dataset . train . images ] model = GAN ( using_gpu = True ) model . train ( X_train = X_train , batch_size = 100 , epoch_num = 71 , init = True , ckptpath = './mnist_GAN_checkpoints_adam_gpu/' , imgpath = './mnist_GAN_images_gpu/' ) batch_size \u3084 epoch_num \u306f\u4efb\u610f\u5909\u66f4 MNIST\u306e\u5168\u8a13\u7df4\u30c7\u30fc\u30bf(55000\u4ef6)\u3092\u7528\u3044\u306670epoch\u307e\u3067\u306e\u5b66\u7fd2\u3067\u751f\u6210\u3055\u308c\u305f\u7d50\u679c\u7269","title":"\u5b66\u7fd2"},{"location":"model_GAN/Tensorflow_GAN_mnist/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/GAN_mnist.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_argmin/","text":"\u6700\u5c0f\u5024\u30fb\u6700\u5927\u5024 tf.argmin(input, dimension, name=None) \u6700\u5c0f\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059 input : Tensor dimension : \u6b21\u5143 \u30d9\u30af\u30c8\u30eb\u306e\u5834\u5408\u30010 \u884c\u5217\u306e\u5834\u5408\u30010:\u5217\u30011:\u884c tf.argmax(input, dimension, name=None) \u6700\u5927\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059 input : Tensor dimension : \u6b21\u5143 Sample \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/sequence_comparison_and_indexing#argmin https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/sequence_comparison_and_indexing#argmax Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/argminmax.ipynb","title":"\u6700\u5c0f\u5024\u30fb\u6700\u5927\u5024"},{"location":"model_basic/tensorflow_argmin/#_1","text":"tf.argmin(input, dimension, name=None) \u6700\u5c0f\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059 input : Tensor dimension : \u6b21\u5143 \u30d9\u30af\u30c8\u30eb\u306e\u5834\u5408\u30010 \u884c\u5217\u306e\u5834\u5408\u30010:\u5217\u30011:\u884c tf.argmax(input, dimension, name=None) \u6700\u5927\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059 input : Tensor dimension : \u6b21\u5143","title":"\u6700\u5c0f\u5024\u30fb\u6700\u5927\u5024"},{"location":"model_basic/tensorflow_argmin/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_argmin/#_2","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/sequence_comparison_and_indexing#argmin https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/sequence_comparison_and_indexing#argmax","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_argmin/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/argminmax.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_boolean/","text":"Boolean tensor tf.where(input, name=None) True\u3068\u306a\u3063\u3066\u3044\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059 Sample \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/sequence_comparison_and_indexing#where Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/boolean.ipynb","title":"Boolean tensor"},{"location":"model_basic/tensorflow_boolean/#boolean-tensor","text":"tf.where(input, name=None) True\u3068\u306a\u3063\u3066\u3044\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059","title":"Boolean tensor"},{"location":"model_basic/tensorflow_boolean/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_boolean/#_1","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/sequence_comparison_and_indexing#where","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_boolean/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/boolean.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_cast/","text":"\u30ad\u30e3\u30b9\u30c8 Tensor\u306e\u5024\u3092\u5909\u63db\u3059\u308b \u95a2\u6570\u540d \u8aac\u660e tf.cast(x, dtype) \u30ad\u30e3\u30b9\u30c8\u3059\u308b tf.string_to_number(string_tensor) \u6587\u5b57\u5217\u3092 tf.float32 \u306b\u30ad\u30e3\u30b9\u30c8\u3059\u308b tf.to_float(x) tf.float32 \u306b\u30ad\u30e3\u30b9\u30c8\u3059\u308b tf.to_int32(x) tf.int32 \u306b\u30ad\u30e3\u30b9\u30c8\u3059\u308b Sample \u53c2\u8003 https://www.tensorflow.org/api_docs/python/array_ops/casting Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/cast.ipynb","title":"\u30ad\u30e3\u30b9\u30c8"},{"location":"model_basic/tensorflow_cast/#_1","text":"Tensor\u306e\u5024\u3092\u5909\u63db\u3059\u308b \u95a2\u6570\u540d \u8aac\u660e tf.cast(x, dtype) \u30ad\u30e3\u30b9\u30c8\u3059\u308b tf.string_to_number(string_tensor) \u6587\u5b57\u5217\u3092 tf.float32 \u306b\u30ad\u30e3\u30b9\u30c8\u3059\u308b tf.to_float(x) tf.float32 \u306b\u30ad\u30e3\u30b9\u30c8\u3059\u308b tf.to_int32(x) tf.int32 \u306b\u30ad\u30e3\u30b9\u30c8\u3059\u308b","title":"\u30ad\u30e3\u30b9\u30c8"},{"location":"model_basic/tensorflow_cast/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_cast/#_2","text":"https://www.tensorflow.org/api_docs/python/array_ops/casting","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_cast/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/cast.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_clip_by_value/","text":"Tensor\u306e\u30af\u30ea\u30c3\u30d4\u30f3\u30b0 Tensor\u3092\u6307\u5b9a\u3057\u305f\u7bc4\u56f2\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u3059\u308b tf.clip_by_value(t, clip_value_min, clip_value_max, name=None) t : A Tensor. clip_value_min : \u7bc4\u56f2\u306e\u6700\u5c0f\u5024\u3068\u306a\u308b\u30b9\u30ab\u30e9\u30fc\u5024 clip_value_max : \u7bc4\u56f2\u306e\u6700\u5927\u5024\u3068\u306a\u308b\u30b9\u30ab\u30e9\u30fc\u5024 Sample \u53c2\u8003 https://www.tensorflow.org/api_docs/python/train/gradient_clipping#clip_by_value Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/clip.ipynb","title":"Tensor\u306e\u30af\u30ea\u30c3\u30d4\u30f3\u30b0"},{"location":"model_basic/tensorflow_clip_by_value/#tensor","text":"Tensor\u3092\u6307\u5b9a\u3057\u305f\u7bc4\u56f2\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u3059\u308b tf.clip_by_value(t, clip_value_min, clip_value_max, name=None) t : A Tensor. clip_value_min : \u7bc4\u56f2\u306e\u6700\u5c0f\u5024\u3068\u306a\u308b\u30b9\u30ab\u30e9\u30fc\u5024 clip_value_max : \u7bc4\u56f2\u306e\u6700\u5927\u5024\u3068\u306a\u308b\u30b9\u30ab\u30e9\u30fc\u5024","title":"Tensor\u306e\u30af\u30ea\u30c3\u30d4\u30f3\u30b0"},{"location":"model_basic/tensorflow_clip_by_value/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_clip_by_value/#_1","text":"https://www.tensorflow.org/api_docs/python/train/gradient_clipping#clip_by_value","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_clip_by_value/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/clip.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_comparison/","text":"Tensor\u306e\u6bd4\u8f03 Sample \u53c2\u8003 https://www.tensorflow.org/api_docs/python/control_flow_ops/comparison_operators Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/comparison.ipynb","title":"Tensor\u306e\u6bd4\u8f03"},{"location":"model_basic/tensorflow_comparison/#tensor","text":"","title":"Tensor\u306e\u6bd4\u8f03"},{"location":"model_basic/tensorflow_comparison/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_comparison/#_1","text":"https://www.tensorflow.org/api_docs/python/control_flow_ops/comparison_operators","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_comparison/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/comparison.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_data_augment/","text":"\u30c7\u30fc\u30bf\u62e1\u5f35 \u5b66\u7fd2\u306e\u524d\u306b\u30c7\u30fc\u30bf\u306e\u6570\u3092\u6c34\u5897\u3057\u3059\u308b\u524d\u51e6\u7406\u306e\u3053\u3068\u3092\u30c7\u30fc\u30bf\u62e1\u5f35(data augument)\u3068\u547c\u3076\u3002\u753b\u50cf\u30c7\u30fc\u30bf\u306e\u5834\u5408\u3001\u30c7\u30fc\u30bf\u62e1\u5f35\u306b\u306f\u753b\u50cf\u306e\u5de6\u53f3\u53cd\u8ee2\u3084\u30b3\u30f3\u30c8\u30e9\u30b9\u30c8\u5909\u5316\u3001\u660e\u308b\u3055\u5909\u5316\u7b49\u304c\u884c\u308f\u308c\u308b\u3002 TensorFlow\u306b\u306f\u30c7\u30fc\u30bf\u62e1\u5f35\u3092\u884c\u3046\u305f\u3081\u306e\u95a2\u6570\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b(\u4ee5\u4e0b\u3092\u53c2\u7167)\u3002 https://www.tensorflow.org/api_guides/python/image \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 \u30c7\u30fc\u30bf\u62e1\u5f35\u3092\u884c\u3046\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u6210\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 #!/usr/bin/env python # coding:utf-8 \"\"\" filename: data_augument.py \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf def random_crop ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u753b\u50cf\u3092\u5207\u308a\u53d6\u308b\"\"\" _proc = tf . random_crop ( img , [ 100 , 100 , 3 ]) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) # \u753b\u50cf\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u8fd4\u3059 def random_flip_left_right ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u5de6\u53f3\u53cd\u8ee2\u3059\u308b\"\"\" _proc = tf . image . random_flip_left_right ( img ) # \u5de6\u53f3\u53cd\u8ee2 img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) # \u753b\u50cf\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u8fd4\u3059 def random_flip_up_down ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u4e0a\u4e0b\u53cd\u8ee2\u3059\u308b\"\"\" _proc = tf . image . random_flip_up_down ( img ) # \u4e0a\u4e0b\u53cd\u8ee2 img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) # \u753b\u50cf\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u8fd4\u3059 def random_brightness ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u8f1d\u5ea6\u3092\u5909\u66f4\u3059\u308b\"\"\" _proc = tf . image . random_brightness ( img , max_delta = 63 ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def random_contrast ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u30b3\u30f3\u30c8\u30e9\u30b9\u30c8\u3092\u5909\u66f4\u3059\u308b\"\"\" _proc = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def image_standardization ( img ): \"\"\"\u753b\u50cf\u3092\u767d\u8272\u5316(\u6b63\u898f\u5316)\u3059\u308b\"\"\" _proc = tf . image . per_image_standardization ( img ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def data_augment ( img ): \"\"\"\u4e0a\u8a18\u306e\u524d\u51e6\u7406\u3092\u9023\u7d50\u3059\u308b\"\"\" _proc = tf . random_crop ( img , [ 100 , 100 , 3 ]) _proc = tf . image . random_flip_left_right ( _proc ) _proc = tf . image . random_flip_up_down ( _proc ) _proc = tf . image . random_brightness ( _proc , max_delta = 63 ) _proc = tf . image . random_contrast ( _proc , lower = 0.2 , upper = 1.8 ) _proc = tf . image . per_image_standardization ( _proc ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def to_img ( contents , name ): \"\"\"PNG\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u66f8\u304d\u51fa\u3059\"\"\" with open ( name , 'wb' ) as f : f . write ( contents ) f . close () filenames = [ \"./Lenna.png\" ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . WholeFileReader ( name = \"wholefile_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) image = tf . image . decode_png ( value , name = \"image\" ) image_as_float = tf . cast ( image , tf . float32 , name = \"image_as_float\" ) / 255. # \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u7fa4 random_crop_op = random_crop ( image_as_float ) random_flip_left_right_op = random_flip_left_right ( image_as_float ) random_flip_up_down_op = random_flip_up_down ( image_as_float ) random_brightness_op = random_brightness ( image_as_float ) random_contrast_op = random_contrast ( image_as_float ) standardization_op = image_standardization ( image_as_float ) data_augment_op = data_augment ( image_as_float ) sess = tf . Session () coord = tf . train . Coordinator () threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) # \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b _crop , _left_right , _up_down , _brightness , _contrast , _std = sess . run ([ random_crop_op , random_flip_left_right_op , random_flip_up_down_op , random_brightness_op , random_contrast_op , standardization_op ]) to_img ( _crop , \"crop.png\" ) to_img ( _left_right , \"left_right.png\" ) to_img ( _up_down , \"up_down.png\" ) to_img ( _brightness , \"brightness.png\" ) to_img ( _contrast , \"contrast.png\" ) to_img ( _std , \"std.png\" ) for i in range ( 3 ): ret = sess . run ( data_augment_op ) to_img ( ret , \"data_aug_ %i .png\" % ( i + 1 )) coord . request_stop () coord . join ( threads ) sess . close () \u5b9f\u884c\u7d50\u679c 1 $ python data_augument.py \u5143\u753b\u50cf : crop.png : left_right.png : up_down.png : brightness.png : contrast.png : std.png : data_aug_1.png : data_aug_2.png : data_aug_3.png :","title":"\u30c7\u30fc\u30bf\u62e1\u5f35"},{"location":"model_basic/tensorflow_data_augment/#_1","text":"\u5b66\u7fd2\u306e\u524d\u306b\u30c7\u30fc\u30bf\u306e\u6570\u3092\u6c34\u5897\u3057\u3059\u308b\u524d\u51e6\u7406\u306e\u3053\u3068\u3092\u30c7\u30fc\u30bf\u62e1\u5f35(data augument)\u3068\u547c\u3076\u3002\u753b\u50cf\u30c7\u30fc\u30bf\u306e\u5834\u5408\u3001\u30c7\u30fc\u30bf\u62e1\u5f35\u306b\u306f\u753b\u50cf\u306e\u5de6\u53f3\u53cd\u8ee2\u3084\u30b3\u30f3\u30c8\u30e9\u30b9\u30c8\u5909\u5316\u3001\u660e\u308b\u3055\u5909\u5316\u7b49\u304c\u884c\u308f\u308c\u308b\u3002 TensorFlow\u306b\u306f\u30c7\u30fc\u30bf\u62e1\u5f35\u3092\u884c\u3046\u305f\u3081\u306e\u95a2\u6570\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b(\u4ee5\u4e0b\u3092\u53c2\u7167)\u3002 https://www.tensorflow.org/api_guides/python/image","title":"\u30c7\u30fc\u30bf\u62e1\u5f35"},{"location":"model_basic/tensorflow_data_augment/#_2","text":"\u30c7\u30fc\u30bf\u62e1\u5f35\u3092\u884c\u3046\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u6210\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 #!/usr/bin/env python # coding:utf-8 \"\"\" filename: data_augument.py \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf def random_crop ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u753b\u50cf\u3092\u5207\u308a\u53d6\u308b\"\"\" _proc = tf . random_crop ( img , [ 100 , 100 , 3 ]) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) # \u753b\u50cf\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u8fd4\u3059 def random_flip_left_right ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u5de6\u53f3\u53cd\u8ee2\u3059\u308b\"\"\" _proc = tf . image . random_flip_left_right ( img ) # \u5de6\u53f3\u53cd\u8ee2 img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) # \u753b\u50cf\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u8fd4\u3059 def random_flip_up_down ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u4e0a\u4e0b\u53cd\u8ee2\u3059\u308b\"\"\" _proc = tf . image . random_flip_up_down ( img ) # \u4e0a\u4e0b\u53cd\u8ee2 img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) # \u753b\u50cf\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u8fd4\u3059 def random_brightness ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u8f1d\u5ea6\u3092\u5909\u66f4\u3059\u308b\"\"\" _proc = tf . image . random_brightness ( img , max_delta = 63 ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def random_contrast ( img ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u306b\u30b3\u30f3\u30c8\u30e9\u30b9\u30c8\u3092\u5909\u66f4\u3059\u308b\"\"\" _proc = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def image_standardization ( img ): \"\"\"\u753b\u50cf\u3092\u767d\u8272\u5316(\u6b63\u898f\u5316)\u3059\u308b\"\"\" _proc = tf . image . per_image_standardization ( img ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def data_augment ( img ): \"\"\"\u4e0a\u8a18\u306e\u524d\u51e6\u7406\u3092\u9023\u7d50\u3059\u308b\"\"\" _proc = tf . random_crop ( img , [ 100 , 100 , 3 ]) _proc = tf . image . random_flip_left_right ( _proc ) _proc = tf . image . random_flip_up_down ( _proc ) _proc = tf . image . random_brightness ( _proc , max_delta = 63 ) _proc = tf . image . random_contrast ( _proc , lower = 0.2 , upper = 1.8 ) _proc = tf . image . per_image_standardization ( _proc ) img_as_int8 = tf . cast ( _proc * 255.0 , tf . uint8 ) return tf . image . encode_png ( img_as_int8 ) def to_img ( contents , name ): \"\"\"PNG\u306e\u30d0\u30a4\u30ca\u30ea\u3092\u66f8\u304d\u51fa\u3059\"\"\" with open ( name , 'wb' ) as f : f . write ( contents ) f . close () filenames = [ \"./Lenna.png\" ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . WholeFileReader ( name = \"wholefile_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) image = tf . image . decode_png ( value , name = \"image\" ) image_as_float = tf . cast ( image , tf . float32 , name = \"image_as_float\" ) / 255. # \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u7fa4 random_crop_op = random_crop ( image_as_float ) random_flip_left_right_op = random_flip_left_right ( image_as_float ) random_flip_up_down_op = random_flip_up_down ( image_as_float ) random_brightness_op = random_brightness ( image_as_float ) random_contrast_op = random_contrast ( image_as_float ) standardization_op = image_standardization ( image_as_float ) data_augment_op = data_augment ( image_as_float ) sess = tf . Session () coord = tf . train . Coordinator () threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) # \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3092\u5b9f\u884c\u3059\u308b _crop , _left_right , _up_down , _brightness , _contrast , _std = sess . run ([ random_crop_op , random_flip_left_right_op , random_flip_up_down_op , random_brightness_op , random_contrast_op , standardization_op ]) to_img ( _crop , \"crop.png\" ) to_img ( _left_right , \"left_right.png\" ) to_img ( _up_down , \"up_down.png\" ) to_img ( _brightness , \"brightness.png\" ) to_img ( _contrast , \"contrast.png\" ) to_img ( _std , \"std.png\" ) for i in range ( 3 ): ret = sess . run ( data_augment_op ) to_img ( ret , \"data_aug_ %i .png\" % ( i + 1 )) coord . request_stop () coord . join ( threads ) sess . close ()","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"model_basic/tensorflow_data_augment/#_3","text":"1 $ python data_augument.py \u5143\u753b\u50cf : crop.png : left_right.png : up_down.png : brightness.png : contrast.png : std.png : data_aug_1.png : data_aug_2.png : data_aug_3.png :","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"model_basic/tensorflow_flags/","text":"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u306e\u51e6\u7406 tf.app.flags \u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e Google commandline flags \u306e\u30e9\u30c3\u30d1\u30fc\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d1\u30b9\u6307\u5b9a\u3084\u30c7\u30d0\u30c3\u30b0\u7b49\u306e\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u306b\u7528\u3044\u3089\u308c\u308b\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # coding:utf-8 \"\"\" file name : tensorflow_flags.py \"\"\" import tensorflow as tf FLAGS = tf . app . flags . FLAGS # \u6587\u5b57\u5217\u306e\u5b9a\u7fa9 tf . app . flags . DEFINE_string ( \"name\" , None , \"\u540d\u524d\" ) # \u6574\u6570\u306e\u5b9a\u7fa9 tf . app . flags . DEFINE_integer ( \"int_value\" , 5 , \"\u6574\u6570\u5024\" ) # \u6d6e\u52d5\u5c0f\u6570\u70b9\u6570\u306e\u5b9a\u7fa9 tf . app . flags . DEFINE_integer ( \"float_value\" , 3.14 , \"\u6d6e\u52d5\u5c0f\u6570\u70b9\u6570\" ) # \u30d6\u30fc\u30eb\u5024 tf . app . flags . DEFINE_bool ( \"bool_value\" , False , \"\u30d6\u30fc\u30eb\u5024\" ) if __name__ == \"__main__\" : print FLAGS . name print FLAGS . int_value print FLAGS . float_value print FLAGS . bool_value \u5b9f\u884c\u7d50\u679c : python tensorflow_flags.py : 1 2 3 4 None 5 3.14 False python tensorflow_flags.py -h : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 usage : tensorflow_flags . py [ -h ] [ --name NAME ] [ --int_value INT_VALUE ] [ --float_value FLOAT_VALUE ] [ --bool_value [BOOL_VALUE ] ] [ --nobool_value ] optional arguments : - h , --help show this help message and exit --name NAME \u540d\u524d --int_value INT_VALUE \u6574\u6570\u5024 --float_value FLOAT_VALUE \u6d6e\u52d5\u5c0f\u6570\u70b9\u6570 --bool_value [BOOL_VALUE] \u30d6\u30fc\u30eb\u5024 --nobool_value python tensorflow_flags.py --name feynman --int_value 7 --bool_value : 1 2 3 4 feynman 7 3.14 True \u53c2\u8003 python-gflags","title":"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u306e\u51e6\u7406"},{"location":"model_basic/tensorflow_flags/#_1","text":"tf.app.flags \u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e Google commandline flags \u306e\u30e9\u30c3\u30d1\u30fc\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d1\u30b9\u6307\u5b9a\u3084\u30c7\u30d0\u30c3\u30b0\u7b49\u306e\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u306b\u7528\u3044\u3089\u308c\u308b\u3002","title":"\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u306e\u51e6\u7406"},{"location":"model_basic/tensorflow_flags/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # coding:utf-8 \"\"\" file name : tensorflow_flags.py \"\"\" import tensorflow as tf FLAGS = tf . app . flags . FLAGS # \u6587\u5b57\u5217\u306e\u5b9a\u7fa9 tf . app . flags . DEFINE_string ( \"name\" , None , \"\u540d\u524d\" ) # \u6574\u6570\u306e\u5b9a\u7fa9 tf . app . flags . DEFINE_integer ( \"int_value\" , 5 , \"\u6574\u6570\u5024\" ) # \u6d6e\u52d5\u5c0f\u6570\u70b9\u6570\u306e\u5b9a\u7fa9 tf . app . flags . DEFINE_integer ( \"float_value\" , 3.14 , \"\u6d6e\u52d5\u5c0f\u6570\u70b9\u6570\" ) # \u30d6\u30fc\u30eb\u5024 tf . app . flags . DEFINE_bool ( \"bool_value\" , False , \"\u30d6\u30fc\u30eb\u5024\" ) if __name__ == \"__main__\" : print FLAGS . name print FLAGS . int_value print FLAGS . float_value print FLAGS . bool_value \u5b9f\u884c\u7d50\u679c : python tensorflow_flags.py : 1 2 3 4 None 5 3.14 False python tensorflow_flags.py -h : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 usage : tensorflow_flags . py [ -h ] [ --name NAME ] [ --int_value INT_VALUE ] [ --float_value FLOAT_VALUE ] [ --bool_value [BOOL_VALUE ] ] [ --nobool_value ] optional arguments : - h , --help show this help message and exit --name NAME \u540d\u524d --int_value INT_VALUE \u6574\u6570\u5024 --float_value FLOAT_VALUE \u6d6e\u52d5\u5c0f\u6570\u70b9\u6570 --bool_value [BOOL_VALUE] \u30d6\u30fc\u30eb\u5024 --nobool_value python tensorflow_flags.py --name feynman --int_value 7 --bool_value : 1 2 3 4 feynman 7 3.14 True","title":"Sample"},{"location":"model_basic/tensorflow_flags/#_2","text":"python-gflags","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_hello/","text":"HelloWorld Hello World Hello World Version\u3092\u8868\u793a\u3059\u308b Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/HelloWorld.ipynb","title":"HelloWorld"},{"location":"model_basic/tensorflow_hello/#helloworld","text":"","title":"HelloWorld"},{"location":"model_basic/tensorflow_hello/#hello-world","text":"Hello World Version\u3092\u8868\u793a\u3059\u308b","title":"Hello World"},{"location":"model_basic/tensorflow_hello/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/HelloWorld.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_l2_norm/","text":"L2\u6b63\u5247\u5316 Sample \u53c2\u8003 https://www.tensorflow.org/api_docs/python/nn/losses#l2_loss Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/l2_normal.ipynb","title":"L2\u6b63\u5247\u5316"},{"location":"model_basic/tensorflow_l2_norm/#l2","text":"","title":"L2\u6b63\u5247\u5316"},{"location":"model_basic/tensorflow_l2_norm/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_l2_norm/#_1","text":"https://www.tensorflow.org/api_docs/python/nn/losses#l2_loss","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_l2_norm/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/l2_normal.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_model/","text":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f Sample \u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3002 \u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u30e2\u30c7\u30eb\u3092\u8868\u793a\u3002 \u30c6\u30ad\u30b9\u30c8\u5f62\u5f0f\u306e\u30e2\u30c7\u30eb\u3092\u8868\u793a\u3002 \u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u3068\u5404\u5909\u6570\u306e\u8868\u793a\u3002 \u65b0\u3057\u3044\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u7acb\u3061\u4e0a\u3052\u3001\u8aad\u307f\u8fbc\u3093\u3060\u30e2\u30c7\u30eb\u3067\u8a08\u7b97\u3002 Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/model.ipynb","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f"},{"location":"model_basic/tensorflow_model/#_1","text":"","title":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f"},{"location":"model_basic/tensorflow_model/#sample","text":"\u30e2\u30c7\u30eb\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3002 \u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u30e2\u30c7\u30eb\u3092\u8868\u793a\u3002 \u30c6\u30ad\u30b9\u30c8\u5f62\u5f0f\u306e\u30e2\u30c7\u30eb\u3092\u8868\u793a\u3002 \u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u3068\u5404\u5909\u6570\u306e\u8868\u793a\u3002 \u65b0\u3057\u3044\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u7acb\u3061\u4e0a\u3052\u3001\u8aad\u307f\u8fbc\u3093\u3060\u30e2\u30c7\u30eb\u3067\u8a08\u7b97\u3002","title":"Sample"},{"location":"model_basic/tensorflow_model/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/model.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_normal/","text":"\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306eTensor\u3092\u4f5c\u308b \u6b63\u898f\u5206\u5e03\u306b\u3088\u308a\u4e71\u6570\u3092\u751f\u6210\u3059\u308b\u3002 tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) \u5909\u6570 \u6982\u8981 shape Tensor\u306e\u30b5\u30a4\u30ba mean \u5e73\u5747 stdev \u6a19\u6e96\u504f\u5dee dtype \u5024\u306e\u578b seed \u30b7\u30fc\u30c9 name \u64cd\u4f5c\u540d \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f mean=0.0, stddev=1.0 \u3068\u306a\u3063\u3066\u304a\u308a\u3001\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u306a\u3063\u3066\u3044\u308b\u3002 Sample \u6b63\u898f\u5206\u5e03 1\u4e07\u4ef6\u306b\u5897\u3084\u3057\u3001\u6b63\u898f\u5206\u5e03\u304c1.0\u306b\u8fd1\u3065\u304f\u304b\u3092\u78ba\u8a8d\u3000 stddev\u306b10\u6307\u5b9a\u3057\u3001\u5e73\u5747\u5024\u304c10\u306b\u8fd1\u3065\u304f\u4e8b\u3092\u78ba\u8a8d \u53c2\u8003 \u5207\u65ad\u6b63\u898f\u5206\u5e03\u306e\u89e3\u8aac https://www.tensorflow.org/versions/r1.0/api_docs/python/constant_op.html#random_normal \u5207\u65ad\u6b63\u898f\u5206\u5e03 \u6a19\u6e96\u504f\u5dee\u306e2\u500d\u306e\u9593\u306b\u53ce\u307e\u308b\u3088\u3046\u306a\u4e71\u6570\u3092\u751f\u6210\u3059\u308b tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) shape : Tensor\u306e\u5f62\u5f0f mean : \u6b63\u898f\u5206\u5e03\u306e\u5e73\u5747\u3002\u30c7\u30d5\u30a9\u30eb\u30c8 0.0 stddev : \u6b63\u898f\u5206\u5e03\u306e\u6a19\u6e96\u504f\u5dee\u3002\u30c7\u30d5\u30a9\u30eb\u30c8 1.0 dtype : \u5024\u306e\u578b Sample \u5207\u65ad\u6b63\u898f\u5206\u5e03 \u53c2\u8003 \u5207\u65ad\u6b63\u898f\u5206\u5e03\u306e\u89e3\u8aac \u4e71\u6570\u306e\u30b7\u30fc\u30c9 \u5b66\u7fd2\u7d50\u679c\u304a\u3088\u3073\u8a08\u7b97\u7d50\u679c\u306b\u518d\u73fe\u6027\u3092\u6301\u305f\u305b\u308b\u305f\u3081\u306b\u4f7f\u3046\u3002 tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) shape Tensor\u306e\u30b5\u30a4\u30ba mean \u5e73\u5747 stddev \u6a19\u6e96\u504f\u5dee dtype \u5024\u306e\u578b \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f mean=0.0, stddev=1.0 \u3068\u306a\u3063\u3066\u304a\u308a\u3001\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u306a\u3063\u3066\u3044\u308b\u3002 Sample \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/constant_op.html#random_normal Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/normal.ipynb","title":"\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306eTensor\u3092\u4f5c\u308b"},{"location":"model_basic/tensorflow_normal/#tensor","text":"\u6b63\u898f\u5206\u5e03\u306b\u3088\u308a\u4e71\u6570\u3092\u751f\u6210\u3059\u308b\u3002 tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) \u5909\u6570 \u6982\u8981 shape Tensor\u306e\u30b5\u30a4\u30ba mean \u5e73\u5747 stdev \u6a19\u6e96\u504f\u5dee dtype \u5024\u306e\u578b seed \u30b7\u30fc\u30c9 name \u64cd\u4f5c\u540d \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f mean=0.0, stddev=1.0 \u3068\u306a\u3063\u3066\u304a\u308a\u3001\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u306a\u3063\u3066\u3044\u308b\u3002","title":"\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306eTensor\u3092\u4f5c\u308b"},{"location":"model_basic/tensorflow_normal/#sample","text":"\u6b63\u898f\u5206\u5e03 1\u4e07\u4ef6\u306b\u5897\u3084\u3057\u3001\u6b63\u898f\u5206\u5e03\u304c1.0\u306b\u8fd1\u3065\u304f\u304b\u3092\u78ba\u8a8d\u3000 stddev\u306b10\u6307\u5b9a\u3057\u3001\u5e73\u5747\u5024\u304c10\u306b\u8fd1\u3065\u304f\u4e8b\u3092\u78ba\u8a8d","title":"Sample"},{"location":"model_basic/tensorflow_normal/#_1","text":"\u5207\u65ad\u6b63\u898f\u5206\u5e03\u306e\u89e3\u8aac https://www.tensorflow.org/versions/r1.0/api_docs/python/constant_op.html#random_normal","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_normal/#_2","text":"\u6a19\u6e96\u504f\u5dee\u306e2\u500d\u306e\u9593\u306b\u53ce\u307e\u308b\u3088\u3046\u306a\u4e71\u6570\u3092\u751f\u6210\u3059\u308b tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) shape : Tensor\u306e\u5f62\u5f0f mean : \u6b63\u898f\u5206\u5e03\u306e\u5e73\u5747\u3002\u30c7\u30d5\u30a9\u30eb\u30c8 0.0 stddev : \u6b63\u898f\u5206\u5e03\u306e\u6a19\u6e96\u504f\u5dee\u3002\u30c7\u30d5\u30a9\u30eb\u30c8 1.0 dtype : \u5024\u306e\u578b","title":"\u5207\u65ad\u6b63\u898f\u5206\u5e03"},{"location":"model_basic/tensorflow_normal/#sample_1","text":"\u5207\u65ad\u6b63\u898f\u5206\u5e03","title":"Sample"},{"location":"model_basic/tensorflow_normal/#_3","text":"\u5207\u65ad\u6b63\u898f\u5206\u5e03\u306e\u89e3\u8aac","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_normal/#_4","text":"\u5b66\u7fd2\u7d50\u679c\u304a\u3088\u3073\u8a08\u7b97\u7d50\u679c\u306b\u518d\u73fe\u6027\u3092\u6301\u305f\u305b\u308b\u305f\u3081\u306b\u4f7f\u3046\u3002 tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) shape Tensor\u306e\u30b5\u30a4\u30ba mean \u5e73\u5747 stddev \u6a19\u6e96\u504f\u5dee dtype \u5024\u306e\u578b \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f mean=0.0, stddev=1.0 \u3068\u306a\u3063\u3066\u304a\u308a\u3001\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u306a\u3063\u3066\u3044\u308b\u3002","title":"\u4e71\u6570\u306e\u30b7\u30fc\u30c9"},{"location":"model_basic/tensorflow_normal/#sample_2","text":"","title":"Sample"},{"location":"model_basic/tensorflow_normal/#_5","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/constant_op.html#random_normal","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_normal/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/normal.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_operator/","text":"Tensor(\u884c\u5217)\u306e\u548c tf.add tf.add(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat1 + mat2 = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#add Tensor(\u884c\u5217)\u306e\u5dee tf.subtract tf.subtract(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat2 - mat1 = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#sub Tensor(\u884c\u5217)\u306e\u7a4d tf.matmul tf.matmul(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat1 x mat2 = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#matmul Tensor(\u884c\u5217)\u306e\u8981\u7d20\u306e\u7a4d tf.multiply tf.multiply(mat_a, mat_b, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample mat_a x mat_b = result_mat \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/arithmetic_operators#multiply Notebooks https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/operator.ipynb","title":"Tensor(\u884c\u5217)\u306e\u548c"},{"location":"model_basic/tensorflow_operator/#tensor","text":"","title":"Tensor(\u884c\u5217)\u306e\u548c"},{"location":"model_basic/tensorflow_operator/#tfadd","text":"tf.add(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.add"},{"location":"model_basic/tensorflow_operator/#sample","text":"mat1 + mat2 = result_mat","title":"Sample"},{"location":"model_basic/tensorflow_operator/#_1","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#add","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_operator/#tensor_1","text":"","title":"Tensor(\u884c\u5217)\u306e\u5dee"},{"location":"model_basic/tensorflow_operator/#tfsubtract","text":"tf.subtract(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.subtract"},{"location":"model_basic/tensorflow_operator/#sample_1","text":"mat2 - mat1 = result_mat","title":"Sample"},{"location":"model_basic/tensorflow_operator/#_2","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#sub","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_operator/#tensor_2","text":"","title":"Tensor(\u884c\u5217)\u306e\u7a4d"},{"location":"model_basic/tensorflow_operator/#tfmatmul","text":"tf.matmul(x, y, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.matmul"},{"location":"model_basic/tensorflow_operator/#sample_2","text":"mat1 x mat2 = result_mat","title":"Sample"},{"location":"model_basic/tensorflow_operator/#_3","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#matmul","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_operator/#tensor_3","text":"","title":"Tensor(\u884c\u5217)\u306e\u8981\u7d20\u306e\u7a4d"},{"location":"model_basic/tensorflow_operator/#tfmultiply","text":"tf.multiply(mat_a, mat_b, name=None) \u5909\u6570 \u6982\u8981 x half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b y half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128\u306e\u578b\u306e\u5024\u3092\u5f15\u6570\u3067\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"tf.multiply"},{"location":"model_basic/tensorflow_operator/#sample_3","text":"mat_a x mat_b = result_mat","title":"Sample"},{"location":"model_basic/tensorflow_operator/#_4","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops/arithmetic_operators#multiply","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_operator/#notebooks","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/operator.ipynb","title":"Notebooks"},{"location":"model_basic/tensorflow_pack/","text":"Tensor\u306e\u96c6\u7d04 Sample \u53c2\u8003 https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/pack.ipynb","title":"Tensor\u306e\u96c6\u7d04"},{"location":"model_basic/tensorflow_pack/#tensor","text":"","title":"Tensor\u306e\u96c6\u7d04"},{"location":"model_basic/tensorflow_pack/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_pack/#_1","text":"https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_pack/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/pack.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_placeholder/","text":"Constant Constant\u306f\u3001\u5b9a\u6570\u3067\u3042\u308b\u3002 tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False) \u5f15\u6570\u540d \u6982\u8981 value dtype\u3067\u6307\u5b9a\u3057\u305fOutput\u306e\u5b9a\u6570\u306e\u5024 dtype Tensor\u306e\u578b shape Tensor\u306e\u5f62\u72b6, \u7121\u6307\u5b9a\u306e\u5834\u5408\u306f\u4efb\u610f\u306e\u5f62\u72b6\u306eTensor\u3092\u6e21\u305b\u308b name \u540d\u524d(Const) verify_shape \u5024\u306e\u5f62\u72b6\u306eVarification(\u691c\u8a3c)\u3092\u3059\u308b\u304b Sample Variable Variable\u306f\u3001\u5909\u6570\u3067\u3042\u308b\u3002 tf.Variable. init (initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None) \u5f15\u6570\u540d \u6982\u8981 initial_value \u521d\u671f\u5024 trainable collections validate_shape caching_device name \u540d\u524d(Const) variable_def dtype Tensor\u306e\u578b expected_shape import_scope Sample Placeholder tf.placeholder(dtype, shape=None, name=None) \u5f15\u6570\u540d \u6982\u8981 dtype Tensor\u306e\u578b shape Tensor\u306e\u5f62\u72b6, \u7121\u6307\u5b9a\u306e\u5834\u5408\u306f\u4efb\u610f\u306e\u5f62\u72b6\u306eTensor\u3092\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d Sample Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/placeholder.ipynb \u53c2\u8003 Placeholder","title":"Constant"},{"location":"model_basic/tensorflow_placeholder/#constant","text":"Constant\u306f\u3001\u5b9a\u6570\u3067\u3042\u308b\u3002 tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False) \u5f15\u6570\u540d \u6982\u8981 value dtype\u3067\u6307\u5b9a\u3057\u305fOutput\u306e\u5b9a\u6570\u306e\u5024 dtype Tensor\u306e\u578b shape Tensor\u306e\u5f62\u72b6, \u7121\u6307\u5b9a\u306e\u5834\u5408\u306f\u4efb\u610f\u306e\u5f62\u72b6\u306eTensor\u3092\u6e21\u305b\u308b name \u540d\u524d(Const) verify_shape \u5024\u306e\u5f62\u72b6\u306eVarification(\u691c\u8a3c)\u3092\u3059\u308b\u304b","title":"Constant"},{"location":"model_basic/tensorflow_placeholder/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_placeholder/#variable","text":"Variable\u306f\u3001\u5909\u6570\u3067\u3042\u308b\u3002 tf.Variable. init (initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None) \u5f15\u6570\u540d \u6982\u8981 initial_value \u521d\u671f\u5024 trainable collections validate_shape caching_device name \u540d\u524d(Const) variable_def dtype Tensor\u306e\u578b expected_shape import_scope","title":"Variable"},{"location":"model_basic/tensorflow_placeholder/#sample_1","text":"","title":"Sample"},{"location":"model_basic/tensorflow_placeholder/#placeholder","text":"tf.placeholder(dtype, shape=None, name=None) \u5f15\u6570\u540d \u6982\u8981 dtype Tensor\u306e\u578b shape Tensor\u306e\u5f62\u72b6, \u7121\u6307\u5b9a\u306e\u5834\u5408\u306f\u4efb\u610f\u306e\u5f62\u72b6\u306eTensor\u3092\u6e21\u305b\u308b name \u64cd\u4f5c\u306e\u540d\u524d","title":"Placeholder"},{"location":"model_basic/tensorflow_placeholder/#sample_2","text":"","title":"Sample"},{"location":"model_basic/tensorflow_placeholder/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/placeholder.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_placeholder/#_1","text":"Placeholder","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_read_csv/","text":"CSV\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 TensorFlow\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u30c7\u30fc\u30bf\u3092\u6295\u5165\u3059\u308b\u4e3b\u306a\u65b9\u6cd5\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3002 \u5404\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3059\u308b\u5ea6\u306b\u72ec\u81ea\u306ePython\u30b3\u30fc\u30c9\u3067\u30c7\u30fc\u30bf\u3092\u4e0e\u3048\u308b \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 \u5c0f\u3055\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5834\u5408\u3001 tf.constant \u3068 tf.Variable \u306b\u30c7\u30fc\u30bf\u3092\u6301\u305f\u305b\u308b \u53c2\u8003 \u672c\u8a18\u4e8b\u3067\u306f 2. \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 \u3092TensorFlow\u5185\u3067\u6271\u3046\u65b9\u6cd5\u3092\u7d39\u4ecb\u3059\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 CSV\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb( hoge.csv \u3001 foo.csv \u3001 bar.csv )\u304b\u3089\u5404\u5217\u306e\u5024\u3092\u8aad\u307f\u8fbc\u3080\u3002 \u69cb\u7bc9\u3055\u308c\u308bTensorFlow\u30b0\u30e9\u30d5\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/usr/bin/env python # coding:utf-8 \"\"\" file name: read_csv.py Reading CSV format data \"\"\" from __future__ import division from __future__ import print_function from __future__ import absolute_import import tensorflow as tf ### CSV\u5f62\u5f0f\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u51e6\u7406\u3092\u30b0\u30e9\u30d5\u3068\u3057\u3066\u69cb\u7bc9\u3059\u308b # \u8aad\u307f\u8fbc\u3080\u30d5\u30a1\u30a4\u30eb\u306e\u30ea\u30b9\u30c8 filenames = [ \"./hoge.csv\" , \"./foo.csv\" , \"./bar.csv\" ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) # \u30c6\u30ad\u30b9\u30c8\u30921\u884c\u8aad\u307f\u8fbc\u3080 reader = tf . TextLineReader ( name = \"textline_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) record_defaults = [[ 1 ], [ 1 ], [ 1 ], [ 1 ], [ 1 ]] # \u578b\u306e\u4ee3\u8868\u5024 # \u6587\u5b57\u5217\"1,1,1,1,1\"\u30925\u3064\u306e\u6574\u6570\u5024\u3068\u3057\u3066\u30d1\u30fc\u30b9\u3059\u308b col1 , col2 , col3 , col4 , col5 = tf . decode_csv ( value , record_defaults = record_defaults , name = \"decode_csv\" ) # \u6700\u521d\u306e4\u3064\u306e\u6574\u6570\u5024\u3092\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3001\u6700\u5f8c\u306e\u5024\u3092\u30e9\u30d9\u30eb\u3068\u3057\u3066\u307e\u3068\u3081\u308b features = tf . stack ([ col1 , col2 , col3 , col4 ], name = \"features\" ) label = tf . stack ([ col5 ], name = \"label\" ) sess = tf . Session () coord = tf . train . Coordinator () # \u30b9\u30ec\u30c3\u30c9\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9 # `sess.run(...)`\u3092\u5b9f\u884c\u3059\u308b\u524d\u306b`tf.train.start_queue_runners(...)`\u3092\u5b9f\u884c\u3059\u308b threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) for _ in range ( 20 ): # \u7279\u5fb4\u91cf\u3068\u30e9\u30d9\u30eb\u3092\u53d6\u308a\u51fa\u3059 _features_ , _label = sess . run ([ features , label ]) print ( _features_ , _label ) # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u8981\u6c42\u3059\u308b coord . request_stop () # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u5f85\u3061\u5408\u308f\u305b\u308b coord . join ( threads ) sess . close () \u5b9f\u884c\u7d50\u679c hoge.csv \u3001 foo.csv \u3001 bar.csv \u3092\u4f5c\u6210 1 2 3 $ python -c 'import sys;import numpy as np;np.savetxt(\"%s.csv\"%sys.argv[1],np.random.randint(0,100,(5,5)),delimiter=\",\",fmt=\"%i\")' hoge $ python -c 'import sys;import numpy as np;np.savetxt(\"%s.csv\"%sys.argv[1],np.random.randint(0,100,(5,5)),delimiter=\",\",fmt=\"%i\")' foo $ python -c 'import sys;import numpy as np;np.savetxt(\"%s.csv\"%sys.argv[1],np.random.randint(0,100,(5,5)),delimiter=\",\",fmt=\"%i\")' bar python read_csv.py : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [54 60 97 9] [93] [71 16 40 3] [69] [59 76 78 80] [31] [28 75 53 53] [63] [50 93 86 38] [58] [51 58 67 63] [27] [72 10 99 84] [85] [35 4 18 66] [98] [83 71 90 63] [36] [54 12 52 11] [73] [80 71 97 96] [45] [11 0 3 25] [51] [ 8 6 81 57] [93] [86 81 83 53] [62] [60 59 4 98] [76] [80 71 97 96] [45] [11 0 3 25] [51] [ 8 6 81 57] [93] [86 81 83 53] [62] [60 59 4 98] [76] \u5b9f\u884c\u74b0\u5883 Python 3.6.0 TensorFlow 1.0.0 \u53c2\u8003 https://www.tensorflow.org/programmers_guide/reading_data https://www.tensorflow.org/api_docs/python/tf/train/start_queue_runners https://www.tensorflow.org/api_docs/python/tf/decode_csv https://www.tensorflow.org/versions/r0.11/api_docs/python/train/coordinator_and_queuerunner","title":"CSV\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080"},{"location":"model_basic/tensorflow_read_csv/#csv","text":"TensorFlow\u30d7\u30ed\u30b0\u30e9\u30e0\u306b\u30c7\u30fc\u30bf\u3092\u6295\u5165\u3059\u308b\u4e3b\u306a\u65b9\u6cd5\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3002 \u5404\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3059\u308b\u5ea6\u306b\u72ec\u81ea\u306ePython\u30b3\u30fc\u30c9\u3067\u30c7\u30fc\u30bf\u3092\u4e0e\u3048\u308b \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 \u5c0f\u3055\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5834\u5408\u3001 tf.constant \u3068 tf.Variable \u306b\u30c7\u30fc\u30bf\u3092\u6301\u305f\u305b\u308b \u53c2\u8003 \u672c\u8a18\u4e8b\u3067\u306f 2. \u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 \u3092TensorFlow\u5185\u3067\u6271\u3046\u65b9\u6cd5\u3092\u7d39\u4ecb\u3059\u308b\u3002","title":"CSV\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080"},{"location":"model_basic/tensorflow_read_csv/#_1","text":"CSV\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb( hoge.csv \u3001 foo.csv \u3001 bar.csv )\u304b\u3089\u5404\u5217\u306e\u5024\u3092\u8aad\u307f\u8fbc\u3080\u3002 \u69cb\u7bc9\u3055\u308c\u308bTensorFlow\u30b0\u30e9\u30d5\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/usr/bin/env python # coding:utf-8 \"\"\" file name: read_csv.py Reading CSV format data \"\"\" from __future__ import division from __future__ import print_function from __future__ import absolute_import import tensorflow as tf ### CSV\u5f62\u5f0f\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u51e6\u7406\u3092\u30b0\u30e9\u30d5\u3068\u3057\u3066\u69cb\u7bc9\u3059\u308b # \u8aad\u307f\u8fbc\u3080\u30d5\u30a1\u30a4\u30eb\u306e\u30ea\u30b9\u30c8 filenames = [ \"./hoge.csv\" , \"./foo.csv\" , \"./bar.csv\" ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) # \u30c6\u30ad\u30b9\u30c8\u30921\u884c\u8aad\u307f\u8fbc\u3080 reader = tf . TextLineReader ( name = \"textline_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) record_defaults = [[ 1 ], [ 1 ], [ 1 ], [ 1 ], [ 1 ]] # \u578b\u306e\u4ee3\u8868\u5024 # \u6587\u5b57\u5217\"1,1,1,1,1\"\u30925\u3064\u306e\u6574\u6570\u5024\u3068\u3057\u3066\u30d1\u30fc\u30b9\u3059\u308b col1 , col2 , col3 , col4 , col5 = tf . decode_csv ( value , record_defaults = record_defaults , name = \"decode_csv\" ) # \u6700\u521d\u306e4\u3064\u306e\u6574\u6570\u5024\u3092\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3001\u6700\u5f8c\u306e\u5024\u3092\u30e9\u30d9\u30eb\u3068\u3057\u3066\u307e\u3068\u3081\u308b features = tf . stack ([ col1 , col2 , col3 , col4 ], name = \"features\" ) label = tf . stack ([ col5 ], name = \"label\" ) sess = tf . Session () coord = tf . train . Coordinator () # \u30b9\u30ec\u30c3\u30c9\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9 # `sess.run(...)`\u3092\u5b9f\u884c\u3059\u308b\u524d\u306b`tf.train.start_queue_runners(...)`\u3092\u5b9f\u884c\u3059\u308b threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) for _ in range ( 20 ): # \u7279\u5fb4\u91cf\u3068\u30e9\u30d9\u30eb\u3092\u53d6\u308a\u51fa\u3059 _features_ , _label = sess . run ([ features , label ]) print ( _features_ , _label ) # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u8981\u6c42\u3059\u308b coord . request_stop () # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u5f85\u3061\u5408\u308f\u305b\u308b coord . join ( threads ) sess . close ()","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"model_basic/tensorflow_read_csv/#_2","text":"hoge.csv \u3001 foo.csv \u3001 bar.csv \u3092\u4f5c\u6210 1 2 3 $ python -c 'import sys;import numpy as np;np.savetxt(\"%s.csv\"%sys.argv[1],np.random.randint(0,100,(5,5)),delimiter=\",\",fmt=\"%i\")' hoge $ python -c 'import sys;import numpy as np;np.savetxt(\"%s.csv\"%sys.argv[1],np.random.randint(0,100,(5,5)),delimiter=\",\",fmt=\"%i\")' foo $ python -c 'import sys;import numpy as np;np.savetxt(\"%s.csv\"%sys.argv[1],np.random.randint(0,100,(5,5)),delimiter=\",\",fmt=\"%i\")' bar python read_csv.py : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [54 60 97 9] [93] [71 16 40 3] [69] [59 76 78 80] [31] [28 75 53 53] [63] [50 93 86 38] [58] [51 58 67 63] [27] [72 10 99 84] [85] [35 4 18 66] [98] [83 71 90 63] [36] [54 12 52 11] [73] [80 71 97 96] [45] [11 0 3 25] [51] [ 8 6 81 57] [93] [86 81 83 53] [62] [60 59 4 98] [76] [80 71 97 96] [45] [11 0 3 25] [51] [ 8 6 81 57] [93] [86 81 83 53] [62] [60 59 4 98] [76]","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"model_basic/tensorflow_read_csv/#_3","text":"Python 3.6.0 TensorFlow 1.0.0","title":"\u5b9f\u884c\u74b0\u5883"},{"location":"model_basic/tensorflow_read_csv/#_4","text":"https://www.tensorflow.org/programmers_guide/reading_data https://www.tensorflow.org/api_docs/python/tf/train/start_queue_runners https://www.tensorflow.org/api_docs/python/tf/decode_csv https://www.tensorflow.org/versions/r0.11/api_docs/python/train/coordinator_and_queuerunner","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_read_img/","text":"\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080 \u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6574\u6570\u5024(uint8)\u306e\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u306b\u306f\u6b21\u306e\u95a2\u6570\u3092\u7528\u3044\u308b\u3002 tf.image.decode_gif tf.image.decode_jpeg tf.image.decode_png \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 \u69cb\u7bc9\u3055\u308c\u308bTensorFlow\u30b0\u30e9\u30d5\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #!/usr/bin/env python # coding:utf-8 \"\"\" filename: read_img.py Read PNG format images with TensorFlow \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function from glob import glob import tensorflow as tf filenames = glob ( './*.png' ) # ['./0.png', './1.png', ... ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . WholeFileReader ( name = \"wholefile_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) image = tf . image . decode_png ( value , name = \"image\" ) # jpg->tf.image.decode_jpeg(...) image_as_float = tf . divide ( tf . cast ( image , tf . float32 ), 255. , name = \"image_as_float\" ) sess = tf . Session () coord = tf . train . Coordinator () # \u30b9\u30ec\u30c3\u30c9\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9 # `sess.run(...)`\u3092\u5b9f\u884c\u3059\u308b\u524d\u306b`tf.train.start_queue_runners(...)`\u3092\u5b9f\u884c\u3059\u308b threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) for _ in range ( 10 ): _key , _image = sess . run ([ key , image ]) print ( _key , _image ) # \u30d5\u30a1\u30a4\u30eb\u540d, \u753b\u50cf\u306eRGB(\u6574\u6570\u5024) _image_as_float = sess . run ( image_as_float ) print ( _image_as_float ) # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u8981\u6c42\u3059\u308b coord . request_stop () # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u5f85\u3061\u5408\u308f\u305b\u308b coord . join ( threads ) sess . close () \u5b9f\u884c\u7d50\u679c \u30c0\u30df\u30fc\u753b\u50cf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b 1 2 3 $ for i in { 0 ..10 } ; do curl \"https://placehold.jp/50x50.png?text= $i \" -o \" $i .png\" ; sleep 1 ; done ; $ ls *.png 0 .png 1 .png 10 .png 2 .png 3 .png 4 .png 5 .png 6 .png 7 .png 8 .png 9 .png python read_img.py : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 b './9.png' [[[ 204 ] [ 204 ] [ 204 ] ..., [ 204 ] [ 204 ] [ 204 ]] [[ 204 ] [ 204 ] [ 204 ] ..., ( \u7565 ) [[ 0 . 80000001 ] [ 0 . 80000001 ] [ 0 . 80000001 ] ..., [ 0 . 80000001 ] [ 0 . 80000001 ] [ 0 . 80000001 ]]] \u53c2\u8003 https://www.tensorflow.org/api_docs/python/tf/WholeFileReader","title":"\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080"},{"location":"model_basic/tensorflow_read_img/#_1","text":"\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6574\u6570\u5024(uint8)\u306e\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u306b\u306f\u6b21\u306e\u95a2\u6570\u3092\u7528\u3044\u308b\u3002 tf.image.decode_gif tf.image.decode_jpeg tf.image.decode_png","title":"\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080"},{"location":"model_basic/tensorflow_read_img/#_2","text":"\u69cb\u7bc9\u3055\u308c\u308bTensorFlow\u30b0\u30e9\u30d5\uff1a 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #!/usr/bin/env python # coding:utf-8 \"\"\" filename: read_img.py Read PNG format images with TensorFlow \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function from glob import glob import tensorflow as tf filenames = glob ( './*.png' ) # ['./0.png', './1.png', ... ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . WholeFileReader ( name = \"wholefile_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) image = tf . image . decode_png ( value , name = \"image\" ) # jpg->tf.image.decode_jpeg(...) image_as_float = tf . divide ( tf . cast ( image , tf . float32 ), 255. , name = \"image_as_float\" ) sess = tf . Session () coord = tf . train . Coordinator () # \u30b9\u30ec\u30c3\u30c9\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9 # `sess.run(...)`\u3092\u5b9f\u884c\u3059\u308b\u524d\u306b`tf.train.start_queue_runners(...)`\u3092\u5b9f\u884c\u3059\u308b threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) for _ in range ( 10 ): _key , _image = sess . run ([ key , image ]) print ( _key , _image ) # \u30d5\u30a1\u30a4\u30eb\u540d, \u753b\u50cf\u306eRGB(\u6574\u6570\u5024) _image_as_float = sess . run ( image_as_float ) print ( _image_as_float ) # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u8981\u6c42\u3059\u308b coord . request_stop () # \u30b9\u30ec\u30c3\u30c9\u306e\u505c\u6b62\u3092\u5f85\u3061\u5408\u308f\u305b\u308b coord . join ( threads ) sess . close ()","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"model_basic/tensorflow_read_img/#_3","text":"\u30c0\u30df\u30fc\u753b\u50cf\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b 1 2 3 $ for i in { 0 ..10 } ; do curl \"https://placehold.jp/50x50.png?text= $i \" -o \" $i .png\" ; sleep 1 ; done ; $ ls *.png 0 .png 1 .png 10 .png 2 .png 3 .png 4 .png 5 .png 6 .png 7 .png 8 .png 9 .png python read_img.py : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 b './9.png' [[[ 204 ] [ 204 ] [ 204 ] ..., [ 204 ] [ 204 ] [ 204 ]] [[ 204 ] [ 204 ] [ 204 ] ..., ( \u7565 ) [[ 0 . 80000001 ] [ 0 . 80000001 ] [ 0 . 80000001 ] ..., [ 0 . 80000001 ] [ 0 . 80000001 ] [ 0 . 80000001 ]]]","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"model_basic/tensorflow_read_img/#_4","text":"https://www.tensorflow.org/api_docs/python/tf/WholeFileReader","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_read_tfrecords/","text":"TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 \u30d0\u30a4\u30ca\u30ea\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u3042\u308b TFRecords \u5f62\u5f0f\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 tf.parse_single_example(serialized, features, name=None, example_names=None) : \u6587\u5b57\u5217\u306eTensor\u304b\u3089\u8f9e\u66f8\u578b\u306e\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b serialized : \u6587\u5b57\u5217\u306eTensor features : FixedLenFeature\u3092\u6301\u3064\u8f9e\u66f8 name : \u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u540d \u6271\u3046TFRecords\u5f62\u5f0f\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\uff1a \u30ad\u30fc \u5185\u5bb9 width \u753b\u50cf\u306e\u5e45 height \u753b\u50cf\u306e\u9ad8\u3055 channels \u753b\u50cf\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570 label \u753b\u50cf\u306e\u30e9\u30d9\u30eb image \u753b\u50cf\u306e\u30d0\u30a4\u30c8\u30c7\u30fc\u30bf TensorFlow\u306e\u30b0\u30e9\u30d5 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #!/usr/bin/env python # coding:utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function import os from glob import glob from random import randint import tensorflow as tf WIDTH = 50 HEIGHT = 50 CHANNELS = 3 filenames = [ \"./dataset.tfrecords\" ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . TFRecordReader ( name = \"tfrecord_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) # \u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u3080 features = tf . parse_single_example ( value , features = { 'image' : tf . FixedLenFeature ([], tf . string ), 'label' : tf . FixedLenFeature ([], tf . int64 ), 'width' : tf . FixedLenFeature ([], tf . int64 ), 'height' : tf . FixedLenFeature ([], tf . int64 ), 'channels' : tf . FixedLenFeature ([], tf . int64 )}, name = \"parse_single_example\" ) image = tf . image . decode_png ( features [ 'image' ], name = \"decode_png\" ) # PNG\u3092\u30c7\u30b3\u30fc\u30c9\u3059\u308b image . set_shape ([ WIDTH , HEIGHT , CHANNELS ]) # shape\u3092\u8a2d\u5b9a\u3059\u308b sess = tf . Session () coord = tf . train . Coordinator () threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) for _ in range ( 5 ): _features , _image = sess . run ([ features , image ]) print ( _features [ 'label' ], _features [ 'width' ], _features [ 'height' ], _features [ 'channels' ], ) print ( _image ) coord . request_stop () coord . join ( threads ) sess . close () \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 0 50 50 3 [[[204] [204] [204] ..., [204] [204] [204]] (\u7565) ..., [204] [204] [204]]] \u5b9f\u884c\u74b0\u5883 Python 3.6.0 TensorFlow 1.0.0 \u53c2\u8003 https://www.tensorflow.org/api_docs/python/tf/TFRecordReader https://www.tensorflow.org/api_docs/python/tf/parse_single_example","title":"TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080"},{"location":"model_basic/tensorflow_read_tfrecords/#tfrecords","text":"\u30d0\u30a4\u30ca\u30ea\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u3042\u308b TFRecords \u5f62\u5f0f\u304b\u3089\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\u3002","title":"TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080"},{"location":"model_basic/tensorflow_read_tfrecords/#_1","text":"tf.parse_single_example(serialized, features, name=None, example_names=None) : \u6587\u5b57\u5217\u306eTensor\u304b\u3089\u8f9e\u66f8\u578b\u306e\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b serialized : \u6587\u5b57\u5217\u306eTensor features : FixedLenFeature\u3092\u6301\u3064\u8f9e\u66f8 name : \u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u540d \u6271\u3046TFRecords\u5f62\u5f0f\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\uff1a \u30ad\u30fc \u5185\u5bb9 width \u753b\u50cf\u306e\u5e45 height \u753b\u50cf\u306e\u9ad8\u3055 channels \u753b\u50cf\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570 label \u753b\u50cf\u306e\u30e9\u30d9\u30eb image \u753b\u50cf\u306e\u30d0\u30a4\u30c8\u30c7\u30fc\u30bf TensorFlow\u306e\u30b0\u30e9\u30d5 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 #!/usr/bin/env python # coding:utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function import os from glob import glob from random import randint import tensorflow as tf WIDTH = 50 HEIGHT = 50 CHANNELS = 3 filenames = [ \"./dataset.tfrecords\" ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . TFRecordReader ( name = \"tfrecord_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) # \u7279\u5fb4\u91cf\u3092\u8aad\u307f\u8fbc\u3080 features = tf . parse_single_example ( value , features = { 'image' : tf . FixedLenFeature ([], tf . string ), 'label' : tf . FixedLenFeature ([], tf . int64 ), 'width' : tf . FixedLenFeature ([], tf . int64 ), 'height' : tf . FixedLenFeature ([], tf . int64 ), 'channels' : tf . FixedLenFeature ([], tf . int64 )}, name = \"parse_single_example\" ) image = tf . image . decode_png ( features [ 'image' ], name = \"decode_png\" ) # PNG\u3092\u30c7\u30b3\u30fc\u30c9\u3059\u308b image . set_shape ([ WIDTH , HEIGHT , CHANNELS ]) # shape\u3092\u8a2d\u5b9a\u3059\u308b sess = tf . Session () coord = tf . train . Coordinator () threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) for _ in range ( 5 ): _features , _image = sess . run ([ features , image ]) print ( _features [ 'label' ], _features [ 'width' ], _features [ 'height' ], _features [ 'channels' ], ) print ( _image ) coord . request_stop () coord . join ( threads ) sess . close ()","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"model_basic/tensorflow_read_tfrecords/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 0 50 50 3 [[[204] [204] [204] ..., [204] [204] [204]] (\u7565) ..., [204] [204] [204]]]","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"model_basic/tensorflow_read_tfrecords/#_3","text":"Python 3.6.0 TensorFlow 1.0.0","title":"\u5b9f\u884c\u74b0\u5883"},{"location":"model_basic/tensorflow_read_tfrecords/#_4","text":"https://www.tensorflow.org/api_docs/python/tf/TFRecordReader https://www.tensorflow.org/api_docs/python/tf/parse_single_example","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_session/","text":"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f \u30bb\u30c3\u30b7\u30e7\u30f3\u5185\u306e Variables \u3092\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u3080\u3002\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u6642\u306b\u30d1\u30e9\u30e1\u30bf\u306e\u5024\u3092\u4fdd\u5b58\u3057\u3066\u304a\u304d\u305f\u3044\u5834\u5408\u306b\u5229\u7528\u3059\u308b\u3002 Sample \u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u305b\u308b 1 2 3 4 $ ls rand* rand-0.data-00000-of-00001 rand-1.data-00000-of-00001 rand-2.data-00000-of-00001 rand-0.index rand-1.index rand-2.index rand-0.meta rand-1.meta rand-2.meta \u30ce\u30fc\u30c8 \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u8aad\u307f\u8fbc\u307f\u6642\u306b\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\"\u30d5\u30a1\u30a4\u30eb\u540d\"\u3068\u3059\u308b\u3068\u5931\u6557\u3059\u308b\u53ef\u80fd\u6027\u3042\u308a\u3002\"./\u30d5\u30a1\u30a4\u30eb\u540d\"\u3068\u3057\u305f\u3068\u3053\u308d\u6210\u529f\u3057\u305f\u3002 \u5b9f\u884c\u74b0\u5883: Python 2.7.12 :: Anaconda 4.1.1 (x86_64) TensorFlow 0.12.0-rc0 \u985e\u4f3c\u306e\u4e0d\u5177\u5408: https://github.com/tensorflow/tensorflow/issues/571 Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/session.ipynb","title":"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f"},{"location":"model_basic/tensorflow_session/#_1","text":"\u30bb\u30c3\u30b7\u30e7\u30f3\u5185\u306e Variables \u3092\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u3080\u3002\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u6642\u306b\u30d1\u30e9\u30e1\u30bf\u306e\u5024\u3092\u4fdd\u5b58\u3057\u3066\u304a\u304d\u305f\u3044\u5834\u5408\u306b\u5229\u7528\u3059\u308b\u3002","title":"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f"},{"location":"model_basic/tensorflow_session/#sample","text":"\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u305b\u308b 1 2 3 4 $ ls rand* rand-0.data-00000-of-00001 rand-1.data-00000-of-00001 rand-2.data-00000-of-00001 rand-0.index rand-1.index rand-2.index rand-0.meta rand-1.meta rand-2.meta","title":"Sample"},{"location":"model_basic/tensorflow_session/#_2","text":"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u8aad\u307f\u8fbc\u307f\u6642\u306b\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u3092\"\u30d5\u30a1\u30a4\u30eb\u540d\"\u3068\u3059\u308b\u3068\u5931\u6557\u3059\u308b\u53ef\u80fd\u6027\u3042\u308a\u3002\"./\u30d5\u30a1\u30a4\u30eb\u540d\"\u3068\u3057\u305f\u3068\u3053\u308d\u6210\u529f\u3057\u305f\u3002 \u5b9f\u884c\u74b0\u5883: Python 2.7.12 :: Anaconda 4.1.1 (x86_64) TensorFlow 0.12.0-rc0 \u985e\u4f3c\u306e\u4e0d\u5177\u5408: https://github.com/tensorflow/tensorflow/issues/571","title":"\u30ce\u30fc\u30c8"},{"location":"model_basic/tensorflow_session/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/session.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_sum/","text":"Tensor\u306e\u7dcf\u548c \u5909\u6570 \u6982\u8981 input_tensor \u5f15\u6570\u3068\u3057\u3066\u6e21\u3059Tensor aixs \u51e6\u7406\u5bfe\u8c61\u3068\u3059\u308b\u6b21\u5143, 0 \u5217, 1 \u884c keep_dims true\u306e\u5834\u5408\u3001\u9577\u30551\u306e\u7e2e\u5c0f\u3055\u308c\u305f\u6b21\u5143\u3092\u4fdd\u6301\u3002 name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#reduce_sum Tensor\u306e\u7dcf\u7a4d tf.reduce_prod(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None) \u5909\u6570 \u6982\u8981 input_tensor \u5f15\u6570\u3068\u3057\u3066\u6e21\u3059Tensor aixs \u51e6\u7406\u5bfe\u8c61\u3068\u3059\u308b\u6b21\u5143, 0 \u5217, 1 \u884c keep_dims true\u306e\u5834\u5408\u3001\u9577\u30551\u306e\u7e2e\u5c0f\u3055\u308c\u305f\u6b21\u5143\u3092\u4fdd\u6301\u3002 name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f) Sample \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#reduce_prod Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/sumprod.ipynb","title":"Tensor\u306e\u7dcf\u548c"},{"location":"model_basic/tensorflow_sum/#tensor","text":"\u5909\u6570 \u6982\u8981 input_tensor \u5f15\u6570\u3068\u3057\u3066\u6e21\u3059Tensor aixs \u51e6\u7406\u5bfe\u8c61\u3068\u3059\u308b\u6b21\u5143, 0 \u5217, 1 \u884c keep_dims true\u306e\u5834\u5408\u3001\u9577\u30551\u306e\u7e2e\u5c0f\u3055\u308c\u305f\u6b21\u5143\u3092\u4fdd\u6301\u3002 name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"Tensor\u306e\u7dcf\u548c"},{"location":"model_basic/tensorflow_sum/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_sum/#_1","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#reduce_sum","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_sum/#tensor_1","text":"tf.reduce_prod(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None) \u5909\u6570 \u6982\u8981 input_tensor \u5f15\u6570\u3068\u3057\u3066\u6e21\u3059Tensor aixs \u51e6\u7406\u5bfe\u8c61\u3068\u3059\u308b\u6b21\u5143, 0 \u5217, 1 \u884c keep_dims true\u306e\u5834\u5408\u3001\u9577\u30551\u306e\u7e2e\u5c0f\u3055\u308c\u305f\u6b21\u5143\u3092\u4fdd\u6301\u3002 name \u64cd\u4f5c\u306e\u540d\u524d(\u4efb\u610f)","title":"Tensor\u306e\u7dcf\u7a4d"},{"location":"model_basic/tensorflow_sum/#sample_1","text":"","title":"Sample"},{"location":"model_basic/tensorflow_sum/#_2","text":"https://www.tensorflow.org/versions/r1.0/api_docs/python/math_ops.html#reduce_prod","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_sum/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/sumprod.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_type/","text":"Tensor\u306e\u578b\u30fb\u6b21\u5143\u6570\u30fb\u30e9\u30f3\u30af\u30fb\u30b5\u30a4\u30ba Sample \u53c2\u8003 https://www.tensorflow.org/versions/r1.0/resources/dims_types.html Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/type.ipynb","title":"Tensor\u306e\u578b\u30fb\u6b21\u5143\u6570\u30fb\u30e9\u30f3\u30af\u30fb\u30b5\u30a4\u30ba"},{"location":"model_basic/tensorflow_type/#tensor","text":"","title":"Tensor\u306e\u578b\u30fb\u6b21\u5143\u6570\u30fb\u30e9\u30f3\u30af\u30fb\u30b5\u30a4\u30ba"},{"location":"model_basic/tensorflow_type/#sample","text":"","title":"Sample"},{"location":"model_basic/tensorflow_type/#_1","text":"https://www.tensorflow.org/versions/r1.0/resources/dims_types.html","title":"\u53c2\u8003"},{"location":"model_basic/tensorflow_type/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/type.ipynb","title":"Notebook"},{"location":"model_basic/tensorflow_write_tfrecords/","text":"TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u66f8\u304d\u8fbc\u3080 TFRecords \u306f\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30d5\u30a1\u30a4\u30eb\u3067\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u306e\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u306b\u5411\u3044\u305f\u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\u3067\u3042\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 PNG\u5f62\u5f0f\u306e\u753b\u50cf\u3068\u305d\u306e\u30e9\u30d9\u30eb\u3092TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u66f8\u304d\u51fa\u3059\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u6210\u3059\u308b\u3002 \u6271\u3046TFRecords\u5f62\u5f0f\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\uff1a \u30ad\u30fc \u5185\u5bb9 width \u753b\u50cf\u306e\u5e45 height \u753b\u50cf\u306e\u9ad8\u3055 channels \u753b\u50cf\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570 label \u753b\u50cf\u306e\u30e9\u30d9\u30eb image \u753b\u50cf\u306e\u30d0\u30a4\u30c8\u30c7\u30fc\u30bf TensorFlow\u306e\u30b0\u30e9\u30d5 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #!/usr/bin/env python # coding:utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function from glob import glob from random import randint import tensorflow as tf # \u753b\u50cf\u306e\u30b5\u30a4\u30ba WIDTH = 50 HEIGHT = 50 CHANNELS = 3 def _bytes_feature ( value ): \"\"\"\u30d0\u30a4\u30ca\u30ea\u3068\u3057\u3066\u66f8\u304d\u8fbc\u3080\"\"\" return tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ value ])) def _int64_feature ( value ): \"\"\"\u6574\u6570\u5024(int64)\u3068\u3057\u3066\u66f8\u304d\u8fbc\u3080\"\"\" return tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ value ])) filenames = glob ( './*.png' ) # ['./0.png', './1.png', ... ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . WholeFileReader ( name = \"wholefile_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) sess = tf . Session () coord = tf . train . Coordinator () threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) filename = 'dataset.tfrecords' # \u66f8\u304d\u8fbc\u3080\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d print ( 'Writing' , filename ) writer = tf . python_io . TFRecordWriter ( filename ) for _ in range ( 10 ): _value = sess . run ( value ) # \u66f8\u304d\u8fbc\u3080\u30c7\u30fc\u30bf\u306e\u30ad\u30fc\u3068\u5185\u5bb9\u3092\u6307\u5b9a\u3059\u308b example = tf . train . Example ( features = tf . train . Features ( feature = { 'width' : _int64_feature ( HEIGHT ), 'height' : _int64_feature ( WIDTH ), 'channels' : _int64_feature ( CHANNELS ), 'label' : _int64_feature ( randint ( 0 , 1 )), 'image' : _bytes_feature ( _value )})) writer . write ( example . SerializeToString ()) # \u66f8\u304d\u8fbc\u3080 writer . close () # \u66f8\u304d\u8fbc\u307f\u7d42\u4e86 coord . request_stop () coord . join ( threads ) sess . close () \u5b9f\u884c\u7d50\u679c 1 Writing dataset.tfrecords \u5b9f\u884c\u7d50\u679c Python 3.6.0 TensorFlow 1.0.0 \u53c2\u8003 https://www.tensorflow.org/api_docs/python/tf/TFRecordReader","title":"TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u66f8\u304d\u8fbc\u3080"},{"location":"model_basic/tensorflow_write_tfrecords/#tfrecords","text":"TFRecords \u306f\u30d0\u30a4\u30ca\u30ea\u5f62\u5f0f\u306e\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30d5\u30a1\u30a4\u30eb\u3067\u3001\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u306e\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u306b\u5411\u3044\u305f\u30d5\u30a1\u30a4\u30eb\u5f62\u5f0f\u3067\u3042\u308b\u3002","title":"TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u66f8\u304d\u8fbc\u3080"},{"location":"model_basic/tensorflow_write_tfrecords/#_1","text":"PNG\u5f62\u5f0f\u306e\u753b\u50cf\u3068\u305d\u306e\u30e9\u30d9\u30eb\u3092TFRecords\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u66f8\u304d\u51fa\u3059\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u6210\u3059\u308b\u3002 \u6271\u3046TFRecords\u5f62\u5f0f\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\uff1a \u30ad\u30fc \u5185\u5bb9 width \u753b\u50cf\u306e\u5e45 height \u753b\u50cf\u306e\u9ad8\u3055 channels \u753b\u50cf\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570 label \u753b\u50cf\u306e\u30e9\u30d9\u30eb image \u753b\u50cf\u306e\u30d0\u30a4\u30c8\u30c7\u30fc\u30bf TensorFlow\u306e\u30b0\u30e9\u30d5 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #!/usr/bin/env python # coding:utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function from glob import glob from random import randint import tensorflow as tf # \u753b\u50cf\u306e\u30b5\u30a4\u30ba WIDTH = 50 HEIGHT = 50 CHANNELS = 3 def _bytes_feature ( value ): \"\"\"\u30d0\u30a4\u30ca\u30ea\u3068\u3057\u3066\u66f8\u304d\u8fbc\u3080\"\"\" return tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ value ])) def _int64_feature ( value ): \"\"\"\u6574\u6570\u5024(int64)\u3068\u3057\u3066\u66f8\u304d\u8fbc\u3080\"\"\" return tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ value ])) filenames = glob ( './*.png' ) # ['./0.png', './1.png', ... ] filename_queue = tf . train . string_input_producer ( filenames , name = \"input_producer\" ) reader = tf . WholeFileReader ( name = \"wholefile_reader\" ) key , value = reader . read ( filename_queue , name = \"reader_read\" ) sess = tf . Session () coord = tf . train . Coordinator () threads = tf . train . start_queue_runners ( coord = coord , sess = sess ) filename = 'dataset.tfrecords' # \u66f8\u304d\u8fbc\u3080\u5148\u306e\u30d5\u30a1\u30a4\u30eb\u540d print ( 'Writing' , filename ) writer = tf . python_io . TFRecordWriter ( filename ) for _ in range ( 10 ): _value = sess . run ( value ) # \u66f8\u304d\u8fbc\u3080\u30c7\u30fc\u30bf\u306e\u30ad\u30fc\u3068\u5185\u5bb9\u3092\u6307\u5b9a\u3059\u308b example = tf . train . Example ( features = tf . train . Features ( feature = { 'width' : _int64_feature ( HEIGHT ), 'height' : _int64_feature ( WIDTH ), 'channels' : _int64_feature ( CHANNELS ), 'label' : _int64_feature ( randint ( 0 , 1 )), 'image' : _bytes_feature ( _value )})) writer . write ( example . SerializeToString ()) # \u66f8\u304d\u8fbc\u3080 writer . close () # \u66f8\u304d\u8fbc\u307f\u7d42\u4e86 coord . request_stop () coord . join ( threads ) sess . close ()","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"model_basic/tensorflow_write_tfrecords/#_2","text":"1 Writing dataset.tfrecords","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"model_basic/tensorflow_write_tfrecords/#_3","text":"Python 3.6.0 TensorFlow 1.0.0","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"model_basic/tensorflow_write_tfrecords/#_4","text":"https://www.tensorflow.org/api_docs/python/tf/TFRecordReader","title":"\u53c2\u8003"},{"location":"model_linear/tensorflow_linear01/","text":"\u7dda\u5f62\u56de\u5e30 \u7dda\u5f62\u306a\u5024\u306e\u6559\u5e2b\u30c7\u30fc\u30bf\u304b\u3089\u3001TensorFlow\u306ew(\u30a6\u30a7\u30a4\u30c8)\u3068b(\u30d0\u30a4\u30a2\u30b9)\u306e\u53ce\u675f\u3092\u89b3\u5bdf\u3059\u308b\u3002 \u307e\u305a\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u306f\u3001(x,y)\u5ea7\u6a19\u306f\u3001 y = x * W + b \u3067\u5b9a\u7fa9\u3059\u308b\u3002 1 2 3 4 b_train = -1 w_train = 0.7 X_train = np.linspace(0, 1.0, 100) y_train = x_train * w_train + b_train \u3053\u306e\u7dda\u5f62\u306a\u5024\u306e\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u540c\u69d8\u306eW\u3068b\u306bTensorFlow\u3067\u53ce\u675f\u3059\u308b\u7d50\u679c\u3092\u89b3\u5bdf\u3059\u308b\u3002\u3064\u307e\u308aTensorFlow\u3092\u7528\u3044\u3066\u3001w\u3068b\u3092\u5c0e\u304d\u51fa\u3059\u3002 \u6559\u5e2b\u30c7\u30fc\u30bf \u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u3001matplotlib\u3067\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3057\u3066\u307f\u308b\u3002 Coding \u305d\u308c\u3067\u306f\u3001\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u3067\u3001\u53ce\u675f\u3092\u78ba\u8a8d\u3057\u3066\u3044\u304f\u3002 \u5909\u6570\u306e\u5b9a\u7fa9 TensorFlow\u306e\u4e2d\u3067\u6271\u3046\u5909\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3002X\u306btrain_x, y\u306btrain_y\u3092\u4ee3\u5165\u3057\u3001w, b\u3092\u5c0e\u304d\u51fa\u3059\u3002 1 2 3 4 x = tf . placeholder ( tf . float32 , name = \"input\" ) y = tf . placeholder ( tf . float32 , name = \"output\" ) w = tf . Variable ( np . random . randn (), name = \"weight\" ) b = tf . Variable ( np . random . randn (), name = \"bias\" ) \u6d3b\u6027\u5316\u95a2\u6570 y_pred = X * W + b 1 y_pred = tf . add ( tf . mul ( x , w ), b ) \u640d\u5931\u95a2\u6570 \u6d3b\u6027\u5316\u95a2\u6570\u3067\u5b9a\u7fa9\u3057\u305fy_pred\u3068\u3001y(\u3053\u3053\u306b\u306fy_train\u3092\u4ee3\u5165)\u306e\u5dee\u30922\u4e57\u3057\u305f(tf.pow(y_pred - y, 2))\u5e73\u5747(tf.reduce_mean)\u3092\u6c42\u3081\u3066\u3044\u308b\u30022\u4e57\u3057\u3066\u3044\u308b\u7406\u7531\u304c\u3001\u5dee\u304c+\u306b\u306a\u308b\u304b-\u306b\u306a\u308b\u304b\u4e88\u60f3\u3067\u304d\u306a\u3044\u305f\u3081\u30012\u4e57\u3057\u3001\u3059\u3079\u3066\u5dee\u3092+\u306b\u3057\u3066\u3001\u305d\u306e\u5e73\u5747\u5024\u3092\u51fa\u3057\u3066\u3044\u308b\u3002\u3053\u306e 1 loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 )) Optimizer \u5b66\u7fd2\u306fOptimizer\u306b\u640d\u5931\u95a2\u6570\u3092\u30bb\u30c3\u30c8\u3059\u308b\u4e8b\u3067\u304a\u3053\u306a\u3046\u3002 1 2 learning_rate = 0.5 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) TensorFlow\u3067\u5229\u7528\u3067\u304d\u308bOptimizer\u306f\u3001\u4e0b\u8a18\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u5b58\u5728\u3057\u3066\u3044\u308b\u3002 Optimizer \u65e5\u672c\u8a9e\u540d tf.train.GradientDescentOptimizer \u52fe\u914d\u964d\u4e0b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdadeltaOptimizer Adadelta\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdagradOptimizer Adagrad\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdagradDAOptimizer Adagrad\u53cc\u5bfe\u5e73\u5747\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.MomentumOptimizer Momentum(\u6163\u6027\u9805)\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdamOptimizer Adam\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.FtrlOptimizer FTRL(Follow The Reaularized Leader)\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.ProximalGradientDescentOptimizer \u8fd1\u4f4d\u52fe\u914d\u964d\u4e0b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.ProximalAdagradOptimizer \u8fd1\u4f4dAdagrad\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.RMSPropOptimizer RMSprop\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 \u30bb\u30c3\u30b7\u30e7\u30f3 1 2 # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : \u5909\u6570\u306e\u521d\u671f\u5316 1 2 3 # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . initialize_all_variables () sess . run ( init_op ) \u5b66\u7fd2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): sess . run ( train_op , feed_dict = { x : x_train , y : y_train }) # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : loss_output = sess . run ( loss , feed_dict = { x : x_train , y : y_train }) w_output = sess . run ( w ) b_output = sess . run ( b ) print \"Step %i , cost %f , weight %f , bias %f \" % ( step , loss_output , w_output , b_output ) \u7d50\u679c Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/linear_regression.ipynb","title":"\u7dda\u5f62\u56de\u5e30"},{"location":"model_linear/tensorflow_linear01/#_1","text":"\u7dda\u5f62\u306a\u5024\u306e\u6559\u5e2b\u30c7\u30fc\u30bf\u304b\u3089\u3001TensorFlow\u306ew(\u30a6\u30a7\u30a4\u30c8)\u3068b(\u30d0\u30a4\u30a2\u30b9)\u306e\u53ce\u675f\u3092\u89b3\u5bdf\u3059\u308b\u3002 \u307e\u305a\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u306f\u3001(x,y)\u5ea7\u6a19\u306f\u3001 y = x * W + b \u3067\u5b9a\u7fa9\u3059\u308b\u3002 1 2 3 4 b_train = -1 w_train = 0.7 X_train = np.linspace(0, 1.0, 100) y_train = x_train * w_train + b_train \u3053\u306e\u7dda\u5f62\u306a\u5024\u306e\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u7528\u3044\u3066\u3001\u540c\u69d8\u306eW\u3068b\u306bTensorFlow\u3067\u53ce\u675f\u3059\u308b\u7d50\u679c\u3092\u89b3\u5bdf\u3059\u308b\u3002\u3064\u307e\u308aTensorFlow\u3092\u7528\u3044\u3066\u3001w\u3068b\u3092\u5c0e\u304d\u51fa\u3059\u3002","title":"\u7dda\u5f62\u56de\u5e30"},{"location":"model_linear/tensorflow_linear01/#_2","text":"\u6559\u5e2b\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u3001matplotlib\u3067\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3057\u3066\u307f\u308b\u3002","title":"\u6559\u5e2b\u30c7\u30fc\u30bf"},{"location":"model_linear/tensorflow_linear01/#coding","text":"\u305d\u308c\u3067\u306f\u3001\u4e0b\u8a18\u306e\u30b3\u30fc\u30c9\u3067\u3001\u53ce\u675f\u3092\u78ba\u8a8d\u3057\u3066\u3044\u304f\u3002","title":"Coding"},{"location":"model_linear/tensorflow_linear01/#_3","text":"TensorFlow\u306e\u4e2d\u3067\u6271\u3046\u5909\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3002X\u306btrain_x, y\u306btrain_y\u3092\u4ee3\u5165\u3057\u3001w, b\u3092\u5c0e\u304d\u51fa\u3059\u3002 1 2 3 4 x = tf . placeholder ( tf . float32 , name = \"input\" ) y = tf . placeholder ( tf . float32 , name = \"output\" ) w = tf . Variable ( np . random . randn (), name = \"weight\" ) b = tf . Variable ( np . random . randn (), name = \"bias\" )","title":"\u5909\u6570\u306e\u5b9a\u7fa9"},{"location":"model_linear/tensorflow_linear01/#_4","text":"y_pred = X * W + b 1 y_pred = tf . add ( tf . mul ( x , w ), b )","title":"\u6d3b\u6027\u5316\u95a2\u6570"},{"location":"model_linear/tensorflow_linear01/#_5","text":"\u6d3b\u6027\u5316\u95a2\u6570\u3067\u5b9a\u7fa9\u3057\u305fy_pred\u3068\u3001y(\u3053\u3053\u306b\u306fy_train\u3092\u4ee3\u5165)\u306e\u5dee\u30922\u4e57\u3057\u305f(tf.pow(y_pred - y, 2))\u5e73\u5747(tf.reduce_mean)\u3092\u6c42\u3081\u3066\u3044\u308b\u30022\u4e57\u3057\u3066\u3044\u308b\u7406\u7531\u304c\u3001\u5dee\u304c+\u306b\u306a\u308b\u304b-\u306b\u306a\u308b\u304b\u4e88\u60f3\u3067\u304d\u306a\u3044\u305f\u3081\u30012\u4e57\u3057\u3001\u3059\u3079\u3066\u5dee\u3092+\u306b\u3057\u3066\u3001\u305d\u306e\u5e73\u5747\u5024\u3092\u51fa\u3057\u3066\u3044\u308b\u3002\u3053\u306e 1 loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 ))","title":"\u640d\u5931\u95a2\u6570"},{"location":"model_linear/tensorflow_linear01/#optimizer","text":"\u5b66\u7fd2\u306fOptimizer\u306b\u640d\u5931\u95a2\u6570\u3092\u30bb\u30c3\u30c8\u3059\u308b\u4e8b\u3067\u304a\u3053\u306a\u3046\u3002 1 2 learning_rate = 0.5 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) TensorFlow\u3067\u5229\u7528\u3067\u304d\u308bOptimizer\u306f\u3001\u4e0b\u8a18\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u5b58\u5728\u3057\u3066\u3044\u308b\u3002 Optimizer \u65e5\u672c\u8a9e\u540d tf.train.GradientDescentOptimizer \u52fe\u914d\u964d\u4e0b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdadeltaOptimizer Adadelta\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdagradOptimizer Adagrad\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdagradDAOptimizer Adagrad\u53cc\u5bfe\u5e73\u5747\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.MomentumOptimizer Momentum(\u6163\u6027\u9805)\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.AdamOptimizer Adam\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.FtrlOptimizer FTRL(Follow The Reaularized Leader)\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.ProximalGradientDescentOptimizer \u8fd1\u4f4d\u52fe\u914d\u964d\u4e0b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.ProximalAdagradOptimizer \u8fd1\u4f4dAdagrad\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 tf.train.RMSPropOptimizer RMSprop\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0","title":"Optimizer"},{"location":"model_linear/tensorflow_linear01/#_6","text":"1 2 # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess :","title":"\u30bb\u30c3\u30b7\u30e7\u30f3"},{"location":"model_linear/tensorflow_linear01/#_7","text":"1 2 3 # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . initialize_all_variables () sess . run ( init_op )","title":"\u5909\u6570\u306e\u521d\u671f\u5316"},{"location":"model_linear/tensorflow_linear01/#_8","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): sess . run ( train_op , feed_dict = { x : x_train , y : y_train }) # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : loss_output = sess . run ( loss , feed_dict = { x : x_train , y : y_train }) w_output = sess . run ( w ) b_output = sess . run ( b ) print \"Step %i , cost %f , weight %f , bias %f \" % ( step , loss_output , w_output , b_output )","title":"\u5b66\u7fd2"},{"location":"model_linear/tensorflow_linear01/#_9","text":"","title":"\u7d50\u679c"},{"location":"model_linear/tensorflow_linear01/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/linear_regression.ipynb","title":"Notebook"},{"location":"model_linear/tensorflow_linear02/","text":"\u7dda\u5f62\u56de\u5e30 TensorBoard1 \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b0\u30e9\u30d5\u5316 \u307e\u305a\u6700\u521d\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b0\u30e9\u30d5\u5316\u3092TensorBoard\u3092\u7528\u3044\u3066\u304a\u3053\u306a\u3044\u307e\u3059\u3002\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b0\u30e9\u30d5\u5316\u306b\u306f\u3001 tf.Graph() \u3092\u7528\u3044\u307e\u3059\u3002Data\u306e\u4fdd\u5b58\u5148\u3068\u3001\u30b0\u30e9\u30d5\u5316\u3057\u305f\u7b87\u6240\u3092 with tf.Graph().as_default(): \u3067\u56f2\u3044\u3001 tf.summary.FileWriter() \u3067\u4fdd\u5b58\u5148\u30d5\u30a9\u30eb\u30c0\u3092\u6307\u5b9a\u3057\u3001\u6700\u5f8c\u306b summary_writer.flush() \u3067\u66f8\u304d\u8fbc\u307f\u307e\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Graph\u306eReset tf . reset_default_graph () # Data\u4fdd\u5b58\u5148 LOGDIR = './data' ... # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : ... # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) with tf . Graph (). as_default (): ... summary_writer . flush () Coding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf b_train = - 1 w_train = 0.7 x_train = np . linspace ( 0 , 1.0 , 100 ) y_train = x_train * w_train + b_train plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . plot ( x_train , y_train , 'k-' , label = 'Line' ) plt . show () # Graph\u306eReset(TF\u95a2\u9023\u51e6\u7406\u306e\u4e00\u756a\u6700\u521d\u306b\u547c\u3073\u51fa\u3059) tf . reset_default_graph () # \u5909\u6570\u306e\u5b9a\u7fa9 x = tf . placeholder ( tf . float32 , name = \"input\" ) y = tf . placeholder ( tf . float32 , name = \"output\" ) w = tf . Variable ( np . random . randn (), name = \"weight\" ) b = tf . Variable ( np . random . randn (), name = \"bias\" ) # \u7dda\u5f62\u56de\u5e30\u306e\u30e2\u30c7\u30eb y_pred = tf . add ( tf . multiply ( x , w ), b ) # \u640d\u5931\u95a2\u6570 loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 )) # Optimizer # \u52fe\u914d\u964d\u4e0b\u6cd5 learning_rate = 0.1 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) # Data\u4fdd\u5b58\u5148 LOGDIR = './data' # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . global_variables_initializer () sess . run ( init_op ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) # Graph with tf . Graph () . as_default () as graph : # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1500 validation_step = 100 #\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): sess . run ( train_op , feed_dict = { x : x_train , y : y_train }) # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : loss_output = sess . run ( loss , feed_dict = { x : x_train , y : y_train }) w_output = sess . run ( w ) b_output = sess . run ( b ) print \"Step %i , cost %f , weight %f , bias %f \" % ( step , loss_output , w_output , b_output ) summary_writer . flush () TensorBoard\u306e\u8d77\u52d5 1 !tensorboard --logdir=data/ \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002TensorBoard\u304cport 6006\u3067\u8d77\u52d5\u3057\u307e\u3059\u3002\u3000localhost:6006\u306b\u63a5\u7d9a\u3059\u308b\u3068TensorBoard\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002 TensorBoard\u306e\u7d42\u4e86 Datalab\u4e0a\u3067\u306f\u3001TensorBoard\u304cForeground\u3067\u8d77\u52d5\u3057\u3063\u3071\u306a\u3057\u306b\u306a\u308b\u306e\u3067\u3001\u30bb\u30c3\u30b7\u30e7\u30f3\u3092Reset\u3057\u3001Foreground\u3067\u306e\u8d77\u52d5\u3092\u505c\u6b62\u3057\u307e\u3059\u3002 \u30b0\u30e9\u30d5\u3092\u898b\u3084\u3059\u304f\u3059\u308b \u6d3b\u6027\u5316\u95a2\u6570\u3068\u640d\u5931\u95a2\u6570\u306e\u7b87\u6240\u3092 with tf.name_scope('\u540d\u524d'): \u3067\u56f2\u3044\u307e\u3059\u3002\u3053\u3046\u3059\u308b\u4e8b\u3067\u3001Graph\u306e\u7b87\u6240\u304c\u56f2\u308f\u308c\u3066\u898b\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002 1 2 3 4 5 6 7 #\u6d3b\u6027\u5316\u95a2\u6570 with tf . name_scope ( 'forward' ): y_pred = tf . add ( tf . mul ( X , W ), b ) # \u640d\u5931\u95a2\u6570 with tf . name_scope ( 'loss' ): loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 )) TensorBoard\u306b\u53cd\u6620\u3055\u305b\u308b\u306b\u306f\u3001\u672c\u30b5\u30f3\u30d7\u30eb\u3092\u5b9f\u884c\u3057\u3001data\u30d5\u30a9\u30eb\u30c0\u306e\u30c7\u30fc\u30bf\u304c\u66f4\u65b0\u3055\u308c\u305f\u306e\u3061\u306b\u3001\u518d\u3073tensorboard\u3092\u8d77\u52d5\u3059\u308b\u3002 1 !tensorboard --logdir=data/ Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/linear_regression02.ipynb","title":"\u7dda\u5f62\u56de\u5e30 TensorBoard1"},{"location":"model_linear/tensorflow_linear02/#tensorboard1","text":"","title":"\u7dda\u5f62\u56de\u5e30 TensorBoard1"},{"location":"model_linear/tensorflow_linear02/#_1","text":"\u307e\u305a\u6700\u521d\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b0\u30e9\u30d5\u5316\u3092TensorBoard\u3092\u7528\u3044\u3066\u304a\u3053\u306a\u3044\u307e\u3059\u3002\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b0\u30e9\u30d5\u5316\u306b\u306f\u3001 tf.Graph() \u3092\u7528\u3044\u307e\u3059\u3002Data\u306e\u4fdd\u5b58\u5148\u3068\u3001\u30b0\u30e9\u30d5\u5316\u3057\u305f\u7b87\u6240\u3092 with tf.Graph().as_default(): \u3067\u56f2\u3044\u3001 tf.summary.FileWriter() \u3067\u4fdd\u5b58\u5148\u30d5\u30a9\u30eb\u30c0\u3092\u6307\u5b9a\u3057\u3001\u6700\u5f8c\u306b summary_writer.flush() \u3067\u66f8\u304d\u8fbc\u307f\u307e\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Graph\u306eReset tf . reset_default_graph () # Data\u4fdd\u5b58\u5148 LOGDIR = './data' ... # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : ... # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) with tf . Graph (). as_default (): ... summary_writer . flush ()","title":"\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30b0\u30e9\u30d5\u5316"},{"location":"model_linear/tensorflow_linear02/#coding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf b_train = - 1 w_train = 0.7 x_train = np . linspace ( 0 , 1.0 , 100 ) y_train = x_train * w_train + b_train plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . plot ( x_train , y_train , 'k-' , label = 'Line' ) plt . show () # Graph\u306eReset(TF\u95a2\u9023\u51e6\u7406\u306e\u4e00\u756a\u6700\u521d\u306b\u547c\u3073\u51fa\u3059) tf . reset_default_graph () # \u5909\u6570\u306e\u5b9a\u7fa9 x = tf . placeholder ( tf . float32 , name = \"input\" ) y = tf . placeholder ( tf . float32 , name = \"output\" ) w = tf . Variable ( np . random . randn (), name = \"weight\" ) b = tf . Variable ( np . random . randn (), name = \"bias\" ) # \u7dda\u5f62\u56de\u5e30\u306e\u30e2\u30c7\u30eb y_pred = tf . add ( tf . multiply ( x , w ), b ) # \u640d\u5931\u95a2\u6570 loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 )) # Optimizer # \u52fe\u914d\u964d\u4e0b\u6cd5 learning_rate = 0.1 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) # Data\u4fdd\u5b58\u5148 LOGDIR = './data' # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . global_variables_initializer () sess . run ( init_op ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) # Graph with tf . Graph () . as_default () as graph : # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1500 validation_step = 100 #\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): sess . run ( train_op , feed_dict = { x : x_train , y : y_train }) # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : loss_output = sess . run ( loss , feed_dict = { x : x_train , y : y_train }) w_output = sess . run ( w ) b_output = sess . run ( b ) print \"Step %i , cost %f , weight %f , bias %f \" % ( step , loss_output , w_output , b_output ) summary_writer . flush ()","title":"Coding"},{"location":"model_linear/tensorflow_linear02/#tensorboard","text":"1 !tensorboard --logdir=data/ \u3092\u5b9f\u884c\u3057\u307e\u3059\u3002TensorBoard\u304cport 6006\u3067\u8d77\u52d5\u3057\u307e\u3059\u3002\u3000localhost:6006\u306b\u63a5\u7d9a\u3059\u308b\u3068TensorBoard\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002 \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u304c\u8868\u793a\u3055\u308c\u307e\u3059\u3002","title":"TensorBoard\u306e\u8d77\u52d5"},{"location":"model_linear/tensorflow_linear02/#tensorboard_1","text":"Datalab\u4e0a\u3067\u306f\u3001TensorBoard\u304cForeground\u3067\u8d77\u52d5\u3057\u3063\u3071\u306a\u3057\u306b\u306a\u308b\u306e\u3067\u3001\u30bb\u30c3\u30b7\u30e7\u30f3\u3092Reset\u3057\u3001Foreground\u3067\u306e\u8d77\u52d5\u3092\u505c\u6b62\u3057\u307e\u3059\u3002","title":"TensorBoard\u306e\u7d42\u4e86"},{"location":"model_linear/tensorflow_linear02/#_2","text":"\u6d3b\u6027\u5316\u95a2\u6570\u3068\u640d\u5931\u95a2\u6570\u306e\u7b87\u6240\u3092 with tf.name_scope('\u540d\u524d'): \u3067\u56f2\u3044\u307e\u3059\u3002\u3053\u3046\u3059\u308b\u4e8b\u3067\u3001Graph\u306e\u7b87\u6240\u304c\u56f2\u308f\u308c\u3066\u898b\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002 1 2 3 4 5 6 7 #\u6d3b\u6027\u5316\u95a2\u6570 with tf . name_scope ( 'forward' ): y_pred = tf . add ( tf . mul ( X , W ), b ) # \u640d\u5931\u95a2\u6570 with tf . name_scope ( 'loss' ): loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 )) TensorBoard\u306b\u53cd\u6620\u3055\u305b\u308b\u306b\u306f\u3001\u672c\u30b5\u30f3\u30d7\u30eb\u3092\u5b9f\u884c\u3057\u3001data\u30d5\u30a9\u30eb\u30c0\u306e\u30c7\u30fc\u30bf\u304c\u66f4\u65b0\u3055\u308c\u305f\u306e\u3061\u306b\u3001\u518d\u3073tensorboard\u3092\u8d77\u52d5\u3059\u308b\u3002 1 !tensorboard --logdir=data/","title":"\u30b0\u30e9\u30d5\u3092\u898b\u3084\u3059\u304f\u3059\u308b"},{"location":"model_linear/tensorflow_linear02/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/linear_regression02.ipynb","title":"Notebook"},{"location":"model_linear/tensorflow_linear03/","text":"\u7dda\u5f62\u56de\u5e30 TensorBoard2 \u7d4c\u904e\u6570\u5024\u306e\u30b0\u30e9\u30d5\u5316 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Graph\u306eReset tf . reset_default_graph () # Data\u4fdd\u5b58\u5148 LOGDIR = './data' ... # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . scalar ( \"w_graph\" , w ) b_graph = tf . summary . scalar ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) loss_graph = tf . summary . scalar ( \"loss_graph\" , loss ) ... # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : ... # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () # Graph with tf . Graph (). as_default () as graph : ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): ... # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : ... # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { X : X_train , y : y_train } ) summary_writer . add_summary ( summary_str , step ) Coding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf b_train = - 1 w_train = 0.7 x_train = np . linspace ( 0 , 1.0 , 100 ) y_train = x_train * w_train + b_train plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . plot ( x_train , y_train , 'k-' , label = 'Line' ) plt . show () # Graph\u306eReset(TF\u95a2\u9023\u51e6\u7406\u306e\u4e00\u756a\u6700\u521d\u306b\u547c\u3073\u51fa\u3059) tf . reset_default_graph () # \u5909\u6570\u306e\u5b9a\u7fa9 x = tf . placeholder ( tf . float32 , name = \"input\" ) y = tf . placeholder ( tf . float32 , name = \"output\" ) w = tf . Variable ( np . random . randn (), name = \"weight\" ) b = tf . Variable ( np . random . randn (), name = \"bias\" ) # \u7dda\u5f62\u56de\u5e30\u306e\u30e2\u30c7\u30eb with tf . name_scope ( 'forward' ): y_pred = tf . add ( tf . multiply ( x , w ), b ) # \u640d\u5931\u95a2\u6570 with tf . name_scope ( 'loss' ): loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 )) # Optimizer # \u52fe\u914d\u964d\u4e0b\u6cd5 learning_rate = 0.1 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) # Data\u4fdd\u5b58\u5148 LOGDIR = './data' # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . scalar ( \"w_graph\" , w ) b_graph = tf . summary . scalar ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) loss_graph = tf . summary . scalar ( \"loss_graph\" , loss ) # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . global_variables_initializer () sess . run ( init_op ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () # Graph with tf . Graph () . as_default () as graph : # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1500 validation_step = 100 #\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): sess . run ( train_op , feed_dict = { x : x_train , y : y_train }) # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : loss_output = sess . run ( loss , feed_dict = { x : x_train , y : y_train }) w_output = sess . run ( w ) b_output = sess . run ( b ) print \"Step %i , cost %f , weight %f , bias %f \" % ( step , loss_output , w_output , b_output ) # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : x_train , y : y_train }) summary_writer . add_summary ( summary_str , step ) summary_writer . flush () TensorBoard\u306e\u8d77\u52d5 Datalab\u4e0a\u3067tensorboard\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\u3046\u307e\u304f\u8d77\u52d5\u3057\u306a\u3044\u5834\u5408\u3001Reset session\u3092\u3067Reset\u3092\u9078\u3073\u3001Notebook\u3082Browser\u306eReload\u30dc\u30bf\u30f3\u3067\u518d\u8d77\u52d5\u3057\u307e\u3059\u3002 !tensorboard --logdir=data/ \u30d6\u30e9\u30a6\u30b6\u306eSCALAR\u3068HISTGRAMS\u3092\u9078\u629e\u3057\u307e\u3059\u3002 \u8a08\u6e2c\u30bf\u30a4\u30df\u30f3\u30b0\u306e\u8abf\u6574 \u30b0\u30e9\u30d5\u304c\u76f4\u89d2\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001 1 2 3 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 \u306e\u8a2d\u5b9a\u3092\u3001validation_step\u309210\u306b\u5909\u66f4\u3057\u300110\u56de\u306b1\u56de\u306e\u30da\u30fc\u30b9\u3067\u53cd\u6620\u3059\u308b\u3088\u3046\u306b\u3059\u308b\u3002 1 2 3 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 10 Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/linear_regression03.ipynb","title":"\u7dda\u5f62\u56de\u5e30 TensorBoard2"},{"location":"model_linear/tensorflow_linear03/#tensorboard2","text":"","title":"\u7dda\u5f62\u56de\u5e30 TensorBoard2"},{"location":"model_linear/tensorflow_linear03/#_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # Graph\u306eReset tf . reset_default_graph () # Data\u4fdd\u5b58\u5148 LOGDIR = './data' ... # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . scalar ( \"w_graph\" , w ) b_graph = tf . summary . scalar ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) loss_graph = tf . summary . scalar ( \"loss_graph\" , loss ) ... # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : ... # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () # Graph with tf . Graph (). as_default () as graph : ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): ... # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : ... # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { X : X_train , y : y_train } ) summary_writer . add_summary ( summary_str , step )","title":"\u7d4c\u904e\u6570\u5024\u306e\u30b0\u30e9\u30d5\u5316"},{"location":"model_linear/tensorflow_linear03/#coding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf b_train = - 1 w_train = 0.7 x_train = np . linspace ( 0 , 1.0 , 100 ) y_train = x_train * w_train + b_train plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . plot ( x_train , y_train , 'k-' , label = 'Line' ) plt . show () # Graph\u306eReset(TF\u95a2\u9023\u51e6\u7406\u306e\u4e00\u756a\u6700\u521d\u306b\u547c\u3073\u51fa\u3059) tf . reset_default_graph () # \u5909\u6570\u306e\u5b9a\u7fa9 x = tf . placeholder ( tf . float32 , name = \"input\" ) y = tf . placeholder ( tf . float32 , name = \"output\" ) w = tf . Variable ( np . random . randn (), name = \"weight\" ) b = tf . Variable ( np . random . randn (), name = \"bias\" ) # \u7dda\u5f62\u56de\u5e30\u306e\u30e2\u30c7\u30eb with tf . name_scope ( 'forward' ): y_pred = tf . add ( tf . multiply ( x , w ), b ) # \u640d\u5931\u95a2\u6570 with tf . name_scope ( 'loss' ): loss = tf . reduce_mean ( tf . pow ( y_pred - y , 2 )) # Optimizer # \u52fe\u914d\u964d\u4e0b\u6cd5 learning_rate = 0.1 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) # Data\u4fdd\u5b58\u5148 LOGDIR = './data' # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . scalar ( \"w_graph\" , w ) b_graph = tf . summary . scalar ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) loss_graph = tf . summary . scalar ( \"loss_graph\" , loss ) # \u30bb\u30c3\u30b7\u30e7\u30f3 with tf . Session () as sess : # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . global_variables_initializer () sess . run ( init_op ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () # Graph with tf . Graph () . as_default () as graph : # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1500 validation_step = 100 #\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in range ( training_step ): sess . run ( train_op , feed_dict = { x : x_train , y : y_train }) # \u9014\u4e2d\u7d4c\u904e\u8868\u793a if step % validation_step == 0 : loss_output = sess . run ( loss , feed_dict = { x : x_train , y : y_train }) w_output = sess . run ( w ) b_output = sess . run ( b ) print \"Step %i , cost %f , weight %f , bias %f \" % ( step , loss_output , w_output , b_output ) # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : x_train , y : y_train }) summary_writer . add_summary ( summary_str , step ) summary_writer . flush ()","title":"Coding"},{"location":"model_linear/tensorflow_linear03/#tensorboard","text":"Datalab\u4e0a\u3067tensorboard\u3092\u8d77\u52d5\u3057\u307e\u3059\u3002\u3046\u307e\u304f\u8d77\u52d5\u3057\u306a\u3044\u5834\u5408\u3001Reset session\u3092\u3067Reset\u3092\u9078\u3073\u3001Notebook\u3082Browser\u306eReload\u30dc\u30bf\u30f3\u3067\u518d\u8d77\u52d5\u3057\u307e\u3059\u3002 !tensorboard --logdir=data/ \u30d6\u30e9\u30a6\u30b6\u306eSCALAR\u3068HISTGRAMS\u3092\u9078\u629e\u3057\u307e\u3059\u3002","title":"TensorBoard\u306e\u8d77\u52d5"},{"location":"model_linear/tensorflow_linear03/#_2","text":"\u30b0\u30e9\u30d5\u304c\u76f4\u89d2\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001 1 2 3 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 \u306e\u8a2d\u5b9a\u3092\u3001validation_step\u309210\u306b\u5909\u66f4\u3057\u300110\u56de\u306b1\u56de\u306e\u30da\u30fc\u30b9\u3067\u53cd\u6620\u3059\u308b\u3088\u3046\u306b\u3059\u308b\u3002 1 2 3 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 10","title":"\u8a08\u6e2c\u30bf\u30a4\u30df\u30f3\u30b0\u306e\u8abf\u6574"},{"location":"model_linear/tensorflow_linear03/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/linear_regression03.ipynb","title":"Notebook"},{"location":"model_linear/tensorflow_linear04/","text":"\u7dda\u5f62\u56de\u5e30 \u89e3\u6790 GradientDescentOptimizer\u306elearning_rate\u3092\u5909\u3048\u3066\u307f\u308b 1 2 3 4 5 6 7 8 9 # \u52fe\u914d\u964d\u4e0b\u6cd5 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 5000 validation_step = 10 AdamOptimizer 1 2 3 4 5 6 7 # Optimizer train_op = tf . train . AdamOptimizer () . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10 AdadeltaOptimizer 1 2 3 4 5 6 7 # Optimizer learning_rate = 0.5 train_op = tf . train . AdadeltaOptimizer ( learning_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10 AdagradOptimizer 1 2 3 4 5 6 7 # Optimizer learning_rate = 0.025 train_op = tf . train . AdagradOptimizer ( learning_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10 MomentumOptimizer 1 2 3 4 5 6 7 8 # Optimizer learning_rate = 0.01 momentum_rate = 0.01 train_op = tf . train . MomentumOptimizer ( learning_rate , momentum_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10","title":"\u7dda\u5f62\u56de\u5e30 \u89e3\u6790"},{"location":"model_linear/tensorflow_linear04/#_1","text":"","title":"\u7dda\u5f62\u56de\u5e30 \u89e3\u6790"},{"location":"model_linear/tensorflow_linear04/#gradientdescentoptimizerlearning_rate","text":"1 2 3 4 5 6 7 8 9 # \u52fe\u914d\u964d\u4e0b\u6cd5 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 5000 validation_step = 10","title":"GradientDescentOptimizer\u306elearning_rate\u3092\u5909\u3048\u3066\u307f\u308b"},{"location":"model_linear/tensorflow_linear04/#adamoptimizer","text":"1 2 3 4 5 6 7 # Optimizer train_op = tf . train . AdamOptimizer () . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10","title":"AdamOptimizer"},{"location":"model_linear/tensorflow_linear04/#adadeltaoptimizer","text":"1 2 3 4 5 6 7 # Optimizer learning_rate = 0.5 train_op = tf . train . AdadeltaOptimizer ( learning_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10","title":"AdadeltaOptimizer"},{"location":"model_linear/tensorflow_linear04/#adagradoptimizer","text":"1 2 3 4 5 6 7 # Optimizer learning_rate = 0.025 train_op = tf . train . AdagradOptimizer ( learning_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10","title":"AdagradOptimizer"},{"location":"model_linear/tensorflow_linear04/#momentumoptimizer","text":"1 2 3 4 5 6 7 8 # Optimizer learning_rate = 0.01 momentum_rate = 0.01 train_op = tf . train . MomentumOptimizer ( learning_rate , momentum_rate ) . minimize ( loss ) ... # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 10000 validation_step = 10","title":"MomentumOptimizer"},{"location":"model_linear/tensorflow_linear05/","text":"\u7dda\u5f62\u56de\u5e30 \u8ab2\u984c \u8ab2\u984c1 1 2 3 4 5 6 7 8 9 10 11 12 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt b_train = - 1 w_train = 0.7 x_train = np . random . random (( 1 , 100 )) y_train = x_train * w_train + b_train + 0.1 * np . random . randn ( 1 , 100 ) plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . show () \u6b63\u898f\u5206\u5e03\u3067\u6563\u3089\u3070\u3063\u305f\u5024\u306e\u7dda\u5f62\u56de\u5e30\u306eb,w\u3092\u6c42\u3081\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092TensorFlow\u3067\u4f5c\u6210\u305b\u3088\u3000 https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/kadai01.ipynb \u8ab2\u984c2 1 2 3 4 5 6 7 8 9 10 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt x_train = np . linspace ( 0 , 100 , 100 ) y_train = x_train + 10 * np . sin ( x_train / 10 ) plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . show () \u975e\u7dda\u5f62\u306b\u6563\u3089\u3070\u3063\u305f\u5024\u306e\u7dda\u5f62\u56de\u5e30\u306eb,w\u3092\u6c42\u3081\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092TensorFlow\u3067\u4f5c\u6210\u305b\u3088\u3000 https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/kadai02.ipynb","title":"\u7dda\u5f62\u56de\u5e30 \u8ab2\u984c"},{"location":"model_linear/tensorflow_linear05/#_1","text":"","title":"\u7dda\u5f62\u56de\u5e30 \u8ab2\u984c"},{"location":"model_linear/tensorflow_linear05/#1","text":"1 2 3 4 5 6 7 8 9 10 11 12 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt b_train = - 1 w_train = 0.7 x_train = np . random . random (( 1 , 100 )) y_train = x_train * w_train + b_train + 0.1 * np . random . randn ( 1 , 100 ) plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . show () \u6b63\u898f\u5206\u5e03\u3067\u6563\u3089\u3070\u3063\u305f\u5024\u306e\u7dda\u5f62\u56de\u5e30\u306eb,w\u3092\u6c42\u3081\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092TensorFlow\u3067\u4f5c\u6210\u305b\u3088\u3000 https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/kadai01.ipynb","title":"\u8ab2\u984c1"},{"location":"model_linear/tensorflow_linear05/#2","text":"1 2 3 4 5 6 7 8 9 10 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt x_train = np . linspace ( 0 , 100 , 100 ) y_train = x_train + 10 * np . sin ( x_train / 10 ) plt . figure ( 1 ) plt . plot ( x_train , y_train , 'ro' , label = 'Data' ) plt . show () \u975e\u7dda\u5f62\u306b\u6563\u3089\u3070\u3063\u305f\u5024\u306e\u7dda\u5f62\u56de\u5e30\u306eb,w\u3092\u6c42\u3081\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092TensorFlow\u3067\u4f5c\u6210\u305b\u3088\u3000 https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/kadai02.ipynb","title":"\u8ab2\u984c2"},{"location":"model_linear/tensorflow_mse/","text":"\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee Mean square error(MSE) Sample https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/mse.ipynb","title":"\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee"},{"location":"model_linear/tensorflow_mse/#_1","text":"Mean square error(MSE)","title":"\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee"},{"location":"model_linear/tensorflow_mse/#sample","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/mse.ipynb","title":"Sample"},{"location":"model_logstic/classification_sample/","text":"\u4e71\u6570\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u308a\u3001\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210 np.random.rand(N,3) \u304b\u3089\u4e71\u6570\u3092\u751f\u6210\u3057\u3001\u7279\u5fb4\u30c7\u30fc\u30bf\u3068\u3057\u3066\u6271\u3046\u3002 \u4eca\u56de\u306f * 0.0~3.0\u306e\u9593\u306e\u6570\u3092 classC (\u4f8b: [1.05,2.21,1.26]) * 3.0~6.0\u306e\u9593\u306e\u6570\u3092 classB (\u4f8b: [4.25,4.76,5.19]) * 6.0~10.0\u306e\u9593\u306e\u6570\u3092 classA (\u4f8b: [6.24,8,56,9,92]) \u3068\u3057\u30013\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046\u305f\u3081\u306e\u4e71\u6570\u3092\u751f\u6210\u3059\u308b\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np import csv # \u30d5\u30a1\u30a4\u30eb\u30aa\u30fc\u30d7\u30f3 f = open ( 'class_train.csv' , 'a' ) writer = csv . writer ( f , lineterminator = ' \\n ' ) x = np . add (( 10.0 - 6.0 ) * np . random . rand ( 100 , 3 ), 6.0 ) x = x . astype ( np . float32 ) for i in range ( 0 , x . shape [ 0 ]): # \u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u306b\u4fdd\u6301 csvlist = [] for n in x [ i ]: csvlist . append ( n ) csvlist . append ( \"classA\" ) writer . writerow ( csvlist ) x = np . add (( 6.0 - 3.0 ) * np . random . rand ( 100 , 3 ), 3.0 ) x = x . astype ( np . float32 ) for i in range ( 0 , x . shape [ 0 ]): # \u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u306b\u4fdd\u6301 csvlist = [] for n in x [ i ]: csvlist . append ( n ) csvlist . append ( \"classB\" ) writer . writerow ( csvlist ) x = np . add (( 3.0 - 0.0 ) * np . random . rand ( 100 , 3 ), 0.0 ) x = x . astype ( np . float32 ) for i in range ( 0 , x . shape [ 0 ]): # \u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u306b\u4fdd\u6301 csvlist = [] for n in x [ i ]: csvlist . append ( n ) csvlist . append ( \"classC\" ) writer . writerow ( csvlist ) # \u30d5\u30a1\u30a4\u30eb\u30af\u30ed\u30fc\u30ba f . close () \u4e0a\u8a18\u306eSample\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u305f\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b class_train.csv \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002 \u307e\u305f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u5404\u30af\u30e9\u30b9100\u500b\u4f5c\u6210\u3055\u308c\u308b\u3002 \u540c\u69d8\u306b\u30c6\u30b9\u30c8\u7528\u306ecsv\u3082\u4f5c\u6210\u3057\u3066\u304a\u304f\u3053\u3068\u3002(\u30c7\u30fc\u30bf\u6570\u306f\u4efb\u610f) \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 \u4f5c\u6210\u3057\u305fcsv\u304b\u3089\u5b66\u7fd2\u3092\u884c\u3046\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f train_dataset = np . genfromtxt ( \"./class_train.csv\" , delimiter = ',' , dtype = [ float , float , float , \"S32\" ]) test_dataset = np . genfromtxt ( \"./class_test.csv\" , delimiter = ',' , dtype = [ float , float , float , \"S32\" ]) #\u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb np . random . shuffle ( train_dataset ) np . random . shuffle ( test_dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 3 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"classA\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"classB\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"classC\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 3 ] for item in dataset ] return np . array ( raw_data ) # \u5b66\u7fd2\u30e9\u30d9\u30eb t_train = get_labels ( train_dataset ) # \u5b66\u7fd2\u30c7\u30fc\u30bf x_train = get_data ( train_dataset ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf t_test = get_labels ( test_dataset ) x_test = get_data ( test_dataset ) # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 3 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 30 w = tf . Variable ( tf . truncated_normal ([ 3 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 30 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.1 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.1 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : x_train , t : t_train }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : x_train , t : t_train }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : x_test , t : t_test }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u9014\u4e2d\u7d4c\u904e \u30e2\u30c7\u30eb\u306e\u30c6\u30b9\u30c8 \u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3068\u672a\u77e5\u306e\u30c7\u30fc\u30bf\u3067\u4e88\u6e2c\u3092\u884c\u306a\u3063\u3066\u307f\u308b\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 test = np . array ([ 8.3 , 8.1 , 8.2 ]) test = np . array ([ test ]) ans = sess . run ( p , feed_dict = { X : test }) print ( ans ) tmp = np . argmax ( ans , axis = 1 ) if ( tmp == 0 ): print ( \"classA\" ) elif ( tmp == 1 ): print ( \"classB\" ) elif ( tmp == 2 ): print ( \"classC\" ) test = np . array ([ 0.23 , 2.11 , 1.15 ]) test = np . array ([ test ]) ans = sess . run ( p , feed_dict = { X : test }) print ( ans ) tmp = np . argmax ( ans , axis = 1 ) if ( tmp == 0 ): print ( \"classA\" ) elif ( tmp == 1 ): print ( \"classB\" ) elif ( tmp == 2 ): print ( \"classC\" ) \u7d50\u679c \u3046\u307e\u304f\u4e88\u6e2c\u304c\u3067\u304d\u305f\u3002 Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/training_Classification.ipynb","title":"\u4e71\u6570\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u308a\u3001\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046"},{"location":"model_logstic/classification_sample/#_1","text":"","title":"\u4e71\u6570\u304b\u3089\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u308a\u3001\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046"},{"location":"model_logstic/classification_sample/#_2","text":"np.random.rand(N,3) \u304b\u3089\u4e71\u6570\u3092\u751f\u6210\u3057\u3001\u7279\u5fb4\u30c7\u30fc\u30bf\u3068\u3057\u3066\u6271\u3046\u3002 \u4eca\u56de\u306f * 0.0~3.0\u306e\u9593\u306e\u6570\u3092 classC (\u4f8b: [1.05,2.21,1.26]) * 3.0~6.0\u306e\u9593\u306e\u6570\u3092 classB (\u4f8b: [4.25,4.76,5.19]) * 6.0~10.0\u306e\u9593\u306e\u6570\u3092 classA (\u4f8b: [6.24,8,56,9,92]) \u3068\u3057\u30013\u30af\u30e9\u30b9\u5206\u985e\u3092\u884c\u3046\u305f\u3081\u306e\u4e71\u6570\u3092\u751f\u6210\u3059\u308b\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np import csv # \u30d5\u30a1\u30a4\u30eb\u30aa\u30fc\u30d7\u30f3 f = open ( 'class_train.csv' , 'a' ) writer = csv . writer ( f , lineterminator = ' \\n ' ) x = np . add (( 10.0 - 6.0 ) * np . random . rand ( 100 , 3 ), 6.0 ) x = x . astype ( np . float32 ) for i in range ( 0 , x . shape [ 0 ]): # \u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u306b\u4fdd\u6301 csvlist = [] for n in x [ i ]: csvlist . append ( n ) csvlist . append ( \"classA\" ) writer . writerow ( csvlist ) x = np . add (( 6.0 - 3.0 ) * np . random . rand ( 100 , 3 ), 3.0 ) x = x . astype ( np . float32 ) for i in range ( 0 , x . shape [ 0 ]): # \u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u306b\u4fdd\u6301 csvlist = [] for n in x [ i ]: csvlist . append ( n ) csvlist . append ( \"classB\" ) writer . writerow ( csvlist ) x = np . add (( 3.0 - 0.0 ) * np . random . rand ( 100 , 3 ), 0.0 ) x = x . astype ( np . float32 ) for i in range ( 0 , x . shape [ 0 ]): # \u30c7\u30fc\u30bf\u3092\u30ea\u30b9\u30c8\u306b\u4fdd\u6301 csvlist = [] for n in x [ i ]: csvlist . append ( n ) csvlist . append ( \"classC\" ) writer . writerow ( csvlist ) # \u30d5\u30a1\u30a4\u30eb\u30af\u30ed\u30fc\u30ba f . close () \u4e0a\u8a18\u306eSample\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u305f\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b class_train.csv \u304c\u4f5c\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002 \u307e\u305f\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u5404\u30af\u30e9\u30b9100\u500b\u4f5c\u6210\u3055\u308c\u308b\u3002 \u540c\u69d8\u306b\u30c6\u30b9\u30c8\u7528\u306ecsv\u3082\u4f5c\u6210\u3057\u3066\u304a\u304f\u3053\u3068\u3002(\u30c7\u30fc\u30bf\u6570\u306f\u4efb\u610f)","title":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210"},{"location":"model_logstic/classification_sample/#_3","text":"\u4f5c\u6210\u3057\u305fcsv\u304b\u3089\u5b66\u7fd2\u3092\u884c\u3046\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f train_dataset = np . genfromtxt ( \"./class_train.csv\" , delimiter = ',' , dtype = [ float , float , float , \"S32\" ]) test_dataset = np . genfromtxt ( \"./class_test.csv\" , delimiter = ',' , dtype = [ float , float , float , \"S32\" ]) #\u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb np . random . shuffle ( train_dataset ) np . random . shuffle ( test_dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 3 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"classA\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"classB\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"classC\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 3 ] for item in dataset ] return np . array ( raw_data ) # \u5b66\u7fd2\u30e9\u30d9\u30eb t_train = get_labels ( train_dataset ) # \u5b66\u7fd2\u30c7\u30fc\u30bf x_train = get_data ( train_dataset ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf t_test = get_labels ( test_dataset ) x_test = get_data ( test_dataset ) # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 3 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 30 w = tf . Variable ( tf . truncated_normal ([ 3 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 30 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.1 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.1 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : x_train , t : t_train }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : x_train , t : t_train }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : x_test , t : t_test }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u9014\u4e2d\u7d4c\u904e","title":"\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0"},{"location":"model_logstic/classification_sample/#_4","text":"\u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3068\u672a\u77e5\u306e\u30c7\u30fc\u30bf\u3067\u4e88\u6e2c\u3092\u884c\u306a\u3063\u3066\u307f\u308b\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 test = np . array ([ 8.3 , 8.1 , 8.2 ]) test = np . array ([ test ]) ans = sess . run ( p , feed_dict = { X : test }) print ( ans ) tmp = np . argmax ( ans , axis = 1 ) if ( tmp == 0 ): print ( \"classA\" ) elif ( tmp == 1 ): print ( \"classB\" ) elif ( tmp == 2 ): print ( \"classC\" ) test = np . array ([ 0.23 , 2.11 , 1.15 ]) test = np . array ([ test ]) ans = sess . run ( p , feed_dict = { X : test }) print ( ans ) tmp = np . argmax ( ans , axis = 1 ) if ( tmp == 0 ): print ( \"classA\" ) elif ( tmp == 1 ): print ( \"classB\" ) elif ( tmp == 2 ): print ( \"classC\" ) \u7d50\u679c \u3046\u307e\u304f\u4e88\u6e2c\u304c\u3067\u304d\u305f\u3002","title":"\u30e2\u30c7\u30eb\u306e\u30c6\u30b9\u30c8"},{"location":"model_logstic/classification_sample/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/training_Classification.ipynb","title":"Notebook"},{"location":"model_logstic/tensorflow_cross_entropy/","text":"\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570\u306f\u3001\u591a\u30af\u30e9\u30b9\u5206\u985e\u306e\u640d\u5931\u95a2\u6570\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u3002 tf.nn.softmax_cross_entropy_with_logits(logits, labels, dim=-1, name=None) \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u8a08\u7b97\u3092\u884c\u3046 logits \u5206\u985e\u30e2\u30c7\u30eb\u306e\u5024 labels \u30e9\u30d9\u30eb\u30c7\u30fc\u30bf 3\u30af\u30e9\u30b9\u5206\u985e\u306e\u4f8b: 3\u3064\u76ee\u306e\u30af\u30e9\u30b9\u304c\u6b63\u89e3 [0.0, 0.0, 1.0] Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import random import numpy as np import tensorflow as tf # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b random . seed ( 20200724 ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 # f1:y = -0.5x # f2:y = 0.25x # f3:y = 0.5x-5.0 # 10x1\u884c\u5217 x = tf . placeholder ( tf . float32 , shape = ( 10 , 1 )) x_data = np . arange ( 0.0 , 10.0 ) . reshape ( 10 , 1 ) # 1x3\u884c\u5217 w = tf . constant ([ - 0.5 , 0.25 , 0.5 ] , tf . float32 , shape = ( 1 , 3 )) # 10x3\u884c\u5217 b = tf . constant ( np . array ([[ 0.0 , 0.0 , - 5 ]] * 10 ), tf . float32 , shape = ( 10 , 3 )) # 10x3\u884c\u5217 \u6559\u5e2b(\u30e9\u30d9\u30eb)\u30c7\u30fc\u30bf t_data = np . zeros (( 10 , 3 )) # 3\u3064\u306e\u3046\u3061\u3069\u308c\u304b1\u3064\u30921.0\u306b\u3059\u308b for row in t_data : row [ random . randint ( 0 , 2 )] = 1.0 t = tf . constant ( t_data , tf . float32 , shape = ( 10 , 3 )) # \u884c\u5217\u306e\u7a4d \u30e2\u30c7\u30eb f = tf . matmul ( x , w ) + b # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = tf . nn . softmax_cross_entropy_with_logits ( labels = t , logits = f ) # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 cross_entropy_loss = tf . reduce_sum ( cross_entropy ) # \u4ee5\u4e0b\u3082\u540c\u69d8 # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 p = tf . nn . softmax ( f ) # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy2 = t * tf . log ( p ) # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 cross_entropy_loss2 = - tf . reduce_sum ( cross_entropy2 ) with tf . Session () as sess : print sess . run ( cross_entropy_loss , feed_dict = { x : x_data }) print sess . run ( cross_entropy_loss2 , feed_dict = { x : x_data }) \u5b9f\u884c\u7d50\u679c 1 2 34.7221 34.7221 \u53c2\u8003 \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u89e3\u8aac TensorFlow API","title":"\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570"},{"location":"model_logstic/tensorflow_cross_entropy/#_1","text":"\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570\u306f\u3001\u591a\u30af\u30e9\u30b9\u5206\u985e\u306e\u640d\u5931\u95a2\u6570\u3068\u3057\u3066\u7528\u3044\u3089\u308c\u308b\u3002 tf.nn.softmax_cross_entropy_with_logits(logits, labels, dim=-1, name=None) \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u8a08\u7b97\u3092\u884c\u3046 logits \u5206\u985e\u30e2\u30c7\u30eb\u306e\u5024 labels \u30e9\u30d9\u30eb\u30c7\u30fc\u30bf 3\u30af\u30e9\u30b9\u5206\u985e\u306e\u4f8b: 3\u3064\u76ee\u306e\u30af\u30e9\u30b9\u304c\u6b63\u89e3 [0.0, 0.0, 1.0] Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import random import numpy as np import tensorflow as tf # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b random . seed ( 20200724 ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 # f1:y = -0.5x # f2:y = 0.25x # f3:y = 0.5x-5.0 # 10x1\u884c\u5217 x = tf . placeholder ( tf . float32 , shape = ( 10 , 1 )) x_data = np . arange ( 0.0 , 10.0 ) . reshape ( 10 , 1 ) # 1x3\u884c\u5217 w = tf . constant ([ - 0.5 , 0.25 , 0.5 ] , tf . float32 , shape = ( 1 , 3 )) # 10x3\u884c\u5217 b = tf . constant ( np . array ([[ 0.0 , 0.0 , - 5 ]] * 10 ), tf . float32 , shape = ( 10 , 3 )) # 10x3\u884c\u5217 \u6559\u5e2b(\u30e9\u30d9\u30eb)\u30c7\u30fc\u30bf t_data = np . zeros (( 10 , 3 )) # 3\u3064\u306e\u3046\u3061\u3069\u308c\u304b1\u3064\u30921.0\u306b\u3059\u308b for row in t_data : row [ random . randint ( 0 , 2 )] = 1.0 t = tf . constant ( t_data , tf . float32 , shape = ( 10 , 3 )) # \u884c\u5217\u306e\u7a4d \u30e2\u30c7\u30eb f = tf . matmul ( x , w ) + b # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = tf . nn . softmax_cross_entropy_with_logits ( labels = t , logits = f ) # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 cross_entropy_loss = tf . reduce_sum ( cross_entropy ) # \u4ee5\u4e0b\u3082\u540c\u69d8 # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 p = tf . nn . softmax ( f ) # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy2 = t * tf . log ( p ) # \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 cross_entropy_loss2 = - tf . reduce_sum ( cross_entropy2 ) with tf . Session () as sess : print sess . run ( cross_entropy_loss , feed_dict = { x : x_data }) print sess . run ( cross_entropy_loss2 , feed_dict = { x : x_data }) \u5b9f\u884c\u7d50\u679c 1 2 34.7221 34.7221","title":"\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570"},{"location":"model_logstic/tensorflow_cross_entropy/#_2","text":"\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u89e3\u8aac TensorFlow API","title":"\u53c2\u8003"},{"location":"model_logstic/tensorflow_iris_adam/","text":"Adam Adam\u306f\u5b66\u7fd2\u7387\u3092\u81ea\u52d5\u7684\u306b\u8abf\u7bc0\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 # coding:utf-8 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam optimizer = tf . train . AdamOptimizer () train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : test_data , t : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [ Train ] cost : 0.017781 , acc : 0.983333 [ Test ] cost : 0.064186 , acc : 0.866667 Step : 400 [ Train ] cost : 0.013653 , acc : 0.983333 [ Test ] cost : 0.071446 , acc : 0.900000 Step : 600 [ Train ] cost : 0.010515 , acc : 0.991667 [ Test ] cost : 0.074486 , acc : 0.900000 Step : 800 [ Train ] cost : 0.007630 , acc : 0.991667 [ Test ] cost : 0.077251 , acc : 0.900000 Step : 1000 [ Train ] cost : 0.005493 , acc : 0.991667 [ Test ] cost : 0.080875 , acc : 0.900000 Step : 1200 [ Train ] cost : 0.004014 , acc : 0.991667 [ Test ] cost : 0.086093 , acc : 0.900000 Step : 1400 [ Train ] cost : 0.002870 , acc : 1.000000 [ Test ] cost : 0.095003 , acc : 0.900000 Step : 1600 [ Train ] cost : 0.002106 , acc : 1.000000 [ Test ] cost : 0.102268 , acc : 0.900000 Step : 1800 [ Train ] cost : 0.001593 , acc : 1.000000 [ Test ] cost : 0.108177 , acc : 0.900000 Step : 2000 [ Train ] cost : 0.001233 , acc : 1.000000 [ Test ] cost : 0.113778 , acc : 0.900000 \u53c2\u8003 Adam\u306e\u89e3\u8aac","title":"Adam"},{"location":"model_logstic/tensorflow_iris_adam/#adam","text":"Adam\u306f\u5b66\u7fd2\u7387\u3092\u81ea\u52d5\u7684\u306b\u8abf\u7bc0\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 # coding:utf-8 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam optimizer = tf . train . AdamOptimizer () train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : test_data , t : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [ Train ] cost : 0.017781 , acc : 0.983333 [ Test ] cost : 0.064186 , acc : 0.866667 Step : 400 [ Train ] cost : 0.013653 , acc : 0.983333 [ Test ] cost : 0.071446 , acc : 0.900000 Step : 600 [ Train ] cost : 0.010515 , acc : 0.991667 [ Test ] cost : 0.074486 , acc : 0.900000 Step : 800 [ Train ] cost : 0.007630 , acc : 0.991667 [ Test ] cost : 0.077251 , acc : 0.900000 Step : 1000 [ Train ] cost : 0.005493 , acc : 0.991667 [ Test ] cost : 0.080875 , acc : 0.900000 Step : 1200 [ Train ] cost : 0.004014 , acc : 0.991667 [ Test ] cost : 0.086093 , acc : 0.900000 Step : 1400 [ Train ] cost : 0.002870 , acc : 1.000000 [ Test ] cost : 0.095003 , acc : 0.900000 Step : 1600 [ Train ] cost : 0.002106 , acc : 1.000000 [ Test ] cost : 0.102268 , acc : 0.900000 Step : 1800 [ Train ] cost : 0.001593 , acc : 1.000000 [ Test ] cost : 0.108177 , acc : 0.900000 Step : 2000 [ Train ] cost : 0.001233 , acc : 1.000000 [ Test ] cost : 0.113778 , acc : 0.900000","title":"Adam"},{"location":"model_logstic/tensorflow_iris_adam/#_1","text":"Adam\u306e\u89e3\u8aac","title":"\u53c2\u8003"},{"location":"model_logstic/tensorflow_iris_momentum/","text":"\u30e2\u30fc\u30e1\u30f3\u30bf\u30e0 tf.train.MomentumOptimizer.__init__(learning_rate, momentum, use_locking=False, name='Momentum', use_nesterov=False) learning_rate : \u5b66\u7fd2\u7387 momentum : \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0\u3092\u4f7f\u3063\u305f\u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u7387:0.001 \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0:0.6 optimizer = tf . train . MomentumOptimizer ( learning_rate = 0.001 , momentum = 0.6 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : test_data , t : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Step : 200 [ Train ] cost : 0.043178 , acc : 0.975000 [ Test ] cost : 0.055161 , acc : 0.966667 Step : 400 [ Train ] cost : 0.033914 , acc : 0.975000 [ Test ] cost : 0.043906 , acc : 0.966667 Step : 600 [ Train ] cost : 0.030400 , acc : 0.975000 [ Test ] cost : 0.039310 , acc : 0.966667 Step : 800 [ Train ] cost : 0.028546 , acc : 0.975000 [ Test ] cost : 0.036760 , acc : 0.966667 Step : 1000 [ Train ] cost : 0.027405 , acc : 0.983333 [ Test ] cost : 0.035135 , acc : 0.966667 Step : 1200 [ Train ] cost : 0.026637 , acc : 0.983333 [ Test ] cost : 0.034016 , acc : 0.966667 Step : 1400 [ Train ] cost : 0.026089 , acc : 0.975000 [ Test ] cost : 0.033204 , acc : 0.933333 Step : 1600 [ Train ] cost : 0.025680 , acc : 0.975000 [ Test ] cost : 0.032592 , acc : 0.933333 Step : 1800 [ Train ] cost : 0.025366 , acc : 0.975000 [ Test ] cost : 0.032118 , acc : 0.933333 Step : 2000 [ Train ] cost : 0.025118 , acc : 0.975000 [ Test ] cost : 0.031744 , acc : 0.933333 \u53c2\u8003 \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0\u306e\u89e3\u8aac","title":"\u30e2\u30fc\u30e1\u30f3\u30bf\u30e0"},{"location":"model_logstic/tensorflow_iris_momentum/#_1","text":"tf.train.MomentumOptimizer.__init__(learning_rate, momentum, use_locking=False, name='Momentum', use_nesterov=False) learning_rate : \u5b66\u7fd2\u7387 momentum : \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0\u3092\u4f7f\u3063\u305f\u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u7387:0.001 \u30e2\u30fc\u30e1\u30f3\u30bf\u30e0:0.6 optimizer = tf . train . MomentumOptimizer ( learning_rate = 0.001 , momentum = 0.6 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : test_data , t : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Step : 200 [ Train ] cost : 0.043178 , acc : 0.975000 [ Test ] cost : 0.055161 , acc : 0.966667 Step : 400 [ Train ] cost : 0.033914 , acc : 0.975000 [ Test ] cost : 0.043906 , acc : 0.966667 Step : 600 [ Train ] cost : 0.030400 , acc : 0.975000 [ Test ] cost : 0.039310 , acc : 0.966667 Step : 800 [ Train ] cost : 0.028546 , acc : 0.975000 [ Test ] cost : 0.036760 , acc : 0.966667 Step : 1000 [ Train ] cost : 0.027405 , acc : 0.983333 [ Test ] cost : 0.035135 , acc : 0.966667 Step : 1200 [ Train ] cost : 0.026637 , acc : 0.983333 [ Test ] cost : 0.034016 , acc : 0.966667 Step : 1400 [ Train ] cost : 0.026089 , acc : 0.975000 [ Test ] cost : 0.033204 , acc : 0.933333 Step : 1600 [ Train ] cost : 0.025680 , acc : 0.975000 [ Test ] cost : 0.032592 , acc : 0.933333 Step : 1800 [ Train ] cost : 0.025366 , acc : 0.975000 [ Test ] cost : 0.032118 , acc : 0.933333 Step : 2000 [ Train ] cost : 0.025118 , acc : 0.975000 [ Test ] cost : 0.031744 , acc : 0.933333","title":"\u30e2\u30fc\u30e1\u30f3\u30bf\u30e0"},{"location":"model_logstic/tensorflow_iris_momentum/#_2","text":"\u30e2\u30fc\u30e1\u30f3\u30bf\u30e0\u306e\u89e3\u8aac","title":"\u53c2\u8003"},{"location":"model_logstic/tensorflow_logistic_cancer01/","text":"\u30ac\u30f3\u8a55\u4fa1 \u6e96\u5099 breast-cancer-wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3044\u3001TensorFlow\u306b\u3088\u308b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u884c\u3046\u3002breast-cancer-wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u304c\u3093\u7d30\u80de\u306e\u60c5\u5831\u3068\u304c\u3093\u306e\u60aa\u6027\u304b\u826f\u6027\u304b\u3092\u307e\u3068\u3081\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3042\u308b\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 1 $ curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./breast-cancer-wisconsin.data\" , delimiter = ',' , dtype = np . uint32 , filling_values = ( 0 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # 2\u304c\u826f\u6027 => 0\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 2 )] = 0 # 4\u304c\u60aa\u6027 => 1\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 4 )] = 1 # \u60a3\u8005\u306e\u30c7\u30fc\u30bf data = dataset [:, 1 : 10 ] # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) labels = dataset [:, 10 ] # \u30c7\u30fc\u30bf\u30927:1\u3067\u5206\u5272\u3059\u308b # \u5168\u30c7\u30fc\u30bf\u6570 : 699 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf\u6570 : 612 # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u6570 : 87 train_data_size = len ( dataset ) - len ( dataset ) // 8 test_data_size = len ( dataset ) // 8 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_data = data [: train_data_size ] train_labels = labels [: train_data_size ] . reshape ( train_data_size , 1 ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_data = data [ train_data_size :] test_labels = labels [ train_data_size :] . reshape ( test_data_size , 1 ) # \u30c7\u30fc\u30bf\u30921\u4ef6\u62bd\u51fa\u3057\u3001\u8868\u793a\u3059\u308b print train_data [ 1 ] print train_labels [ 1 ] \u5b9f\u884c\u7d50\u679c 1 2 [ 9 1 2 6 4 10 7 7 2] [1] \u53c2\u8003 Breast Cancer Wisconsin (Prognostic) Data Set","title":"\u30ac\u30f3\u8a55\u4fa1 \u6e96\u5099"},{"location":"model_logstic/tensorflow_logistic_cancer01/#_1","text":"breast-cancer-wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3044\u3001TensorFlow\u306b\u3088\u308b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u884c\u3046\u3002breast-cancer-wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u304c\u3093\u7d30\u80de\u306e\u60c5\u5831\u3068\u304c\u3093\u306e\u60aa\u6027\u304b\u826f\u6027\u304b\u3092\u307e\u3068\u3081\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3042\u308b\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 1 $ curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./breast-cancer-wisconsin.data\" , delimiter = ',' , dtype = np . uint32 , filling_values = ( 0 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # 2\u304c\u826f\u6027 => 0\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 2 )] = 0 # 4\u304c\u60aa\u6027 => 1\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 4 )] = 1 # \u60a3\u8005\u306e\u30c7\u30fc\u30bf data = dataset [:, 1 : 10 ] # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) labels = dataset [:, 10 ] # \u30c7\u30fc\u30bf\u30927:1\u3067\u5206\u5272\u3059\u308b # \u5168\u30c7\u30fc\u30bf\u6570 : 699 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf\u6570 : 612 # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u6570 : 87 train_data_size = len ( dataset ) - len ( dataset ) // 8 test_data_size = len ( dataset ) // 8 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_data = data [: train_data_size ] train_labels = labels [: train_data_size ] . reshape ( train_data_size , 1 ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_data = data [ train_data_size :] test_labels = labels [ train_data_size :] . reshape ( test_data_size , 1 ) # \u30c7\u30fc\u30bf\u30921\u4ef6\u62bd\u51fa\u3057\u3001\u8868\u793a\u3059\u308b print train_data [ 1 ] print train_labels [ 1 ] \u5b9f\u884c\u7d50\u679c 1 2 [ 9 1 2 6 4 10 7 7 2] [1]","title":"\u30ac\u30f3\u8a55\u4fa1 \u6e96\u5099"},{"location":"model_logstic/tensorflow_logistic_cancer01/#_2","text":"Breast Cancer Wisconsin (Prognostic) Data Set","title":"\u53c2\u8003"},{"location":"model_logstic/tensorflow_logistic_cancer02/","text":"\u30ac\u30f3\u8a55\u4fa1 \u5b66\u7fd2 breast-cancer-wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3044\u3001TensorFlow\u306b\u3088\u308b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u884c\u3046\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./breast-cancer-wisconsin.data\" , delimiter = ',' , dtype = np . uint32 , filling_values = ( 0 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # 2\u304c\u826f\u6027 => 0\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 2 )] = 0 # 4\u304c\u60aa\u6027 => 1\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 4 )] = 1 # \u60a3\u8005\u306e\u30c7\u30fc\u30bf data = dataset [:, 1 : 10 ] # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) labels = dataset [:, 10 ] # \u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3068\u30c6\u30b9\u30c8\u7528\u30677:1\u306b\u5206\u5272\u3059\u308b train_data_size = len ( data ) - len ( data ) // 8 test_data_size = len ( data ) // 8 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_data = data [: train_data_size ] train_labels = labels [: train_data_size ] . reshape ( train_data_size , 1 ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_data = data [ train_data_size :] test_labels = labels [ train_data_size :] . reshape ( test_data_size , 1 ) # Nx9\u884c\u5217 (N\u306f\u30c7\u30fc\u30bf\u6570) X = tf . placeholder ( tf . float32 , shape = ( None , 9 )) # 9x1\u884c\u5217 w = tf . Variable ( tf . zeros ( shape = ( 9 , 1 ))) w0 = tf . Variable ( tf . zeros ( shape = ( 1 ))) # \u884c\u5217\u306e\u7a4d f = tf . matmul ( X , w ) + w0 # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 p = tf . sigmoid ( f ) # \u6b63\u89e3\u30c7\u30fc\u30bf Nx1\u884c\u5217 (N\u306f\u30c7\u30fc\u30bf\u6570) t = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # \u5024\u304c0\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3059\u308b cp = tf . clip_by_value ( p , 1e-10 , 1.0 ) cnp = tf . clip_by_value ( 1 - p , 1e-10 , 1.0 ) cost_f = t * tf . log ( cp ) + ( 1 - t ) * tf . log ( cnp ) # \u30b3\u30b9\u30c8\u95a2\u6570\u306e\u5b9a\u7fa9 cost = - tf . reduce_sum ( cost_f ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u3057\u3066Adam\u3092\u63a1\u7528 train_step = tf . train . AdamOptimizer () . minimize ( cost ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3059\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . sign ( p - 0.5 ), tf . sign ( t - 0.5 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 20000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) if i % 1000 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b c , acc = sess . run ([ cost , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 tacc = sess . run ( accuracy , feed_dict = { X : test_data , t : test_labels }) print \"step: %d , cost: %f , acc: %f , test acc: %f \" % ( i , c , acc , tacc ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 step : 1000 , cost : 173.826935 , acc : 0.909734 , test acc : 0.950000 step : 2000 , cost : 137.149139 , acc : 0.932743 , test acc : 0.975000 step : 3000 , cost : 111.714836 , acc : 0.950442 , test acc : 0.987500 step : 4000 , cost : 92.708443 , acc : 0.955752 , test acc : 0.987500 step : 5000 , cost : 78.816345 , acc : 0.961062 , test acc : 0.987500 step : 6000 , cost : 68.754555 , acc : 0.961062 , test acc : 0.987500 step : 7000 , cost : 61.481083 , acc : 0.962832 , test acc : 1.000000 step : 8000 , cost : 56.248322 , acc : 0.962832 , test acc : 1.000000 step : 9000 , cost : 52.524582 , acc : 0.961062 , test acc : 1.000000 step : 10000 , cost : 49.924149 , acc : 0.962832 , test acc : 1.000000 step : 11000 , cost : 48.161461 , acc : 0.966372 , test acc : 1.000000 step : 12000 , cost : 47.021065 , acc : 0.966372 , test acc : 1.000000 step : 13000 , cost : 46.336304 , acc : 0.966372 , test acc : 1.000000 step : 14000 , cost : 45.972946 , acc : 0.966372 , test acc : 1.000000 step : 15000 , cost : 45.817200 , acc : 0.966372 , test acc : 1.000000 step : 16000 , cost : 45.771610 , acc : 0.966372 , test acc : 1.000000 step : 17000 , cost : 45.764835 , acc : 0.966372 , test acc : 1.000000 step : 18000 , cost : 45.764572 , acc : 0.966372 , test acc : 1.000000 step : 19000 , cost : 45.764626 , acc : 0.966372 , test acc : 1.000000 step : 20000 , cost : 45.764568 , acc : 0.966372 , test acc : 1.000000 \u53c2\u8003 Breast Cancer Wisconsin (Prognostic) Data Set","title":"\u30ac\u30f3\u8a55\u4fa1 \u5b66\u7fd2"},{"location":"model_logstic/tensorflow_logistic_cancer02/#_1","text":"breast-cancer-wisconsin\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3044\u3001TensorFlow\u306b\u3088\u308b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u884c\u3046\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./breast-cancer-wisconsin.data\" , delimiter = ',' , dtype = np . uint32 , filling_values = ( 0 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # 2\u304c\u826f\u6027 => 0\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 2 )] = 0 # 4\u304c\u60aa\u6027 => 1\u306b\u7f6e\u304d\u63db\u3048\u308b dataset [:, 10 ][ np . where ( dataset [:, 10 ] == 4 )] = 1 # \u60a3\u8005\u306e\u30c7\u30fc\u30bf data = dataset [:, 1 : 10 ] # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) labels = dataset [:, 10 ] # \u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3068\u30c6\u30b9\u30c8\u7528\u30677:1\u306b\u5206\u5272\u3059\u308b train_data_size = len ( data ) - len ( data ) // 8 test_data_size = len ( data ) // 8 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_data = data [: train_data_size ] train_labels = labels [: train_data_size ] . reshape ( train_data_size , 1 ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_data = data [ train_data_size :] test_labels = labels [ train_data_size :] . reshape ( test_data_size , 1 ) # Nx9\u884c\u5217 (N\u306f\u30c7\u30fc\u30bf\u6570) X = tf . placeholder ( tf . float32 , shape = ( None , 9 )) # 9x1\u884c\u5217 w = tf . Variable ( tf . zeros ( shape = ( 9 , 1 ))) w0 = tf . Variable ( tf . zeros ( shape = ( 1 ))) # \u884c\u5217\u306e\u7a4d f = tf . matmul ( X , w ) + w0 # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 p = tf . sigmoid ( f ) # \u6b63\u89e3\u30c7\u30fc\u30bf Nx1\u884c\u5217 (N\u306f\u30c7\u30fc\u30bf\u6570) t = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # \u5024\u304c0\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3059\u308b cp = tf . clip_by_value ( p , 1e-10 , 1.0 ) cnp = tf . clip_by_value ( 1 - p , 1e-10 , 1.0 ) cost_f = t * tf . log ( cp ) + ( 1 - t ) * tf . log ( cnp ) # \u30b3\u30b9\u30c8\u95a2\u6570\u306e\u5b9a\u7fa9 cost = - tf . reduce_sum ( cost_f ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u3057\u3066Adam\u3092\u63a1\u7528 train_step = tf . train . AdamOptimizer () . minimize ( cost ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3059\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . sign ( p - 0.5 ), tf . sign ( t - 0.5 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 20000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) if i % 1000 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b c , acc = sess . run ([ cost , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 tacc = sess . run ( accuracy , feed_dict = { X : test_data , t : test_labels }) print \"step: %d , cost: %f , acc: %f , test acc: %f \" % ( i , c , acc , tacc ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 step : 1000 , cost : 173.826935 , acc : 0.909734 , test acc : 0.950000 step : 2000 , cost : 137.149139 , acc : 0.932743 , test acc : 0.975000 step : 3000 , cost : 111.714836 , acc : 0.950442 , test acc : 0.987500 step : 4000 , cost : 92.708443 , acc : 0.955752 , test acc : 0.987500 step : 5000 , cost : 78.816345 , acc : 0.961062 , test acc : 0.987500 step : 6000 , cost : 68.754555 , acc : 0.961062 , test acc : 0.987500 step : 7000 , cost : 61.481083 , acc : 0.962832 , test acc : 1.000000 step : 8000 , cost : 56.248322 , acc : 0.962832 , test acc : 1.000000 step : 9000 , cost : 52.524582 , acc : 0.961062 , test acc : 1.000000 step : 10000 , cost : 49.924149 , acc : 0.962832 , test acc : 1.000000 step : 11000 , cost : 48.161461 , acc : 0.966372 , test acc : 1.000000 step : 12000 , cost : 47.021065 , acc : 0.966372 , test acc : 1.000000 step : 13000 , cost : 46.336304 , acc : 0.966372 , test acc : 1.000000 step : 14000 , cost : 45.972946 , acc : 0.966372 , test acc : 1.000000 step : 15000 , cost : 45.817200 , acc : 0.966372 , test acc : 1.000000 step : 16000 , cost : 45.771610 , acc : 0.966372 , test acc : 1.000000 step : 17000 , cost : 45.764835 , acc : 0.966372 , test acc : 1.000000 step : 18000 , cost : 45.764572 , acc : 0.966372 , test acc : 1.000000 step : 19000 , cost : 45.764626 , acc : 0.966372 , test acc : 1.000000 step : 20000 , cost : 45.764568 , acc : 0.966372 , test acc : 1.000000","title":"\u30ac\u30f3\u8a55\u4fa1 \u5b66\u7fd2"},{"location":"model_logstic/tensorflow_logistic_cancer02/#_2","text":"Breast Cancer Wisconsin (Prognostic) Data Set","title":"\u53c2\u8003"},{"location":"model_logstic/tensorflow_logistic_virus01/","text":"\u30a6\u30a3\u30eb\u30b9\u5206\u5e03\u3000\u30c7\u30fc\u30bf\u4f5c\u6210 \u5206\u5e03\u306e\u4f5c\u6210 \u30a6\u30a3\u30eb\u30b9\u306e\u611f\u67d3\u5206\u5e03\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u8d64\u304c\u611f\u67d3\u6e08\u307f\u3001\u9752\u304c\u975e\u611f\u67d3\u3067\u6b63\u898f\u90e8\u5206\u5e03\u3092x,y\u5ea7\u6a19\u3068\u3082\u306b\u4e2d\u5fc3\u70b9\u304b\u3089(2,2)\u3065\u3064\u305a\u3089\u3057\u3066\u6b63\u898f\u5206\u5e03\u3092\u305a\u3089\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt x_positive = np . random . randn ( 500 , 1 ) + 2 y_positive = np . random . randn ( 500 , 1 ) + 2 x_negative = np . random . randn ( 500 , 1 ) - 2 y_negative = np . random . randn ( 500 , 1 ) - 2 plt . figure ( 1 ) plt . plot ( x_positive , y_positive , 'ro' , label = 'Data' ) plt . plot ( x_negative , y_negative , 'bo' , label = 'Data' ) plt . show () \u30c7\u30fc\u30bf\u306e\u4f5c\u6210 \u6b21\u306b\u3001\u73fe\u5728 1x500\u306e\u884c\u5217\u3067\u3042\u308b\u3001x_positive,y_positive\u3068x_negative,y_negative\u30922x1000\u306e1\u3064\u306e\u914d\u5217\u3067\u3042\u308bVIRUS\u306b\u683c\u7d0d\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 N = len ( x_positive ) POSITIVE = np . zeros (( N , 2 )) for i in xrange ( N ): POSITIVE [ i ][ 0 ] = x_positive [ i ] POSITIVE [ i ][ 1 ] = y_positive [ i ] NEGATIVE = np . zeros (( N , 2 )) for i in xrange ( N ): NEGATIVE [ i ][ 0 ] = x_negative [ i ] NEGATIVE [ i ][ 1 ] = y_negative [ i ] VIRUS = np . vstack ([ NEGATIVE , POSITIVE ]) . astype ( np . float32 ) \u611f\u67d3/\u975e\u611f\u67d3\u306e\u30e9\u30d9\u30eb\u4ed8\u3051 \u307e\u305f\u3001\u611f\u67d3\u72b6\u6cc1\u3092\u8868\u3059\u884c\u5217 STATE\u30922x1000\u884c\u5217\u3067\u4f5c\u6210\u3057\u3001\u611f\u67d3\u3057\u3066\u3044\u308b\u5834\u5408\u306f[1,0], \u611f\u67d3\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001[0, 1]\u3092\u4ee3\u5165\u3059\u308b\u3002\u524d\u306e\u9805\u76ee\u3067\u3001\u524d\u534a500\u500b\u3092\u611f\u67d3\u6e08\u307f\u3001\u5f8c\u534a\u306e500\u500b\u3092\u975e\u611f\u67d3\u3068\u3057\u30662x1000\u884c\u5217\u306b\u683c\u7d0d\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u524d\u534a\u306e500\u500b\u3092\u611f\u67d3\u6e08\u307f\u3068\u3057\u3066[1,0]\u3068\u3057\u3001\u5f8c\u534a\u306e5000\u500b\u3092\u672a\u611f\u67d3\u3068\u3057\u3066[0,1]\u3092\u4ee3\u5165\u3059\u308b\u3002 [1,0] \u611f\u67d3 [0,1] \u975e\u611f\u67d3 1 2 3 4 5 6 STATE = np . zeros (( N * 2 , 2 )) for i in xrange ( N * 2 ) : if i < N : STATE [ i ][ 0 ] = 1 else : STATE [ i ][ 1 ] = 1 Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/virus01.ipynb","title":"\u30a6\u30a3\u30eb\u30b9\u5206\u5e03\u3000\u30c7\u30fc\u30bf\u4f5c\u6210"},{"location":"model_logstic/tensorflow_logistic_virus01/#_1","text":"","title":"\u30a6\u30a3\u30eb\u30b9\u5206\u5e03\u3000\u30c7\u30fc\u30bf\u4f5c\u6210"},{"location":"model_logstic/tensorflow_logistic_virus01/#_2","text":"\u30a6\u30a3\u30eb\u30b9\u306e\u611f\u67d3\u5206\u5e03\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\u8d64\u304c\u611f\u67d3\u6e08\u307f\u3001\u9752\u304c\u975e\u611f\u67d3\u3067\u6b63\u898f\u90e8\u5206\u5e03\u3092x,y\u5ea7\u6a19\u3068\u3082\u306b\u4e2d\u5fc3\u70b9\u304b\u3089(2,2)\u3065\u3064\u305a\u3089\u3057\u3066\u6b63\u898f\u5206\u5e03\u3092\u305a\u3089\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt x_positive = np . random . randn ( 500 , 1 ) + 2 y_positive = np . random . randn ( 500 , 1 ) + 2 x_negative = np . random . randn ( 500 , 1 ) - 2 y_negative = np . random . randn ( 500 , 1 ) - 2 plt . figure ( 1 ) plt . plot ( x_positive , y_positive , 'ro' , label = 'Data' ) plt . plot ( x_negative , y_negative , 'bo' , label = 'Data' ) plt . show ()","title":"\u5206\u5e03\u306e\u4f5c\u6210"},{"location":"model_logstic/tensorflow_logistic_virus01/#_3","text":"\u6b21\u306b\u3001\u73fe\u5728 1x500\u306e\u884c\u5217\u3067\u3042\u308b\u3001x_positive,y_positive\u3068x_negative,y_negative\u30922x1000\u306e1\u3064\u306e\u914d\u5217\u3067\u3042\u308bVIRUS\u306b\u683c\u7d0d\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 N = len ( x_positive ) POSITIVE = np . zeros (( N , 2 )) for i in xrange ( N ): POSITIVE [ i ][ 0 ] = x_positive [ i ] POSITIVE [ i ][ 1 ] = y_positive [ i ] NEGATIVE = np . zeros (( N , 2 )) for i in xrange ( N ): NEGATIVE [ i ][ 0 ] = x_negative [ i ] NEGATIVE [ i ][ 1 ] = y_negative [ i ] VIRUS = np . vstack ([ NEGATIVE , POSITIVE ]) . astype ( np . float32 )","title":"\u30c7\u30fc\u30bf\u306e\u4f5c\u6210"},{"location":"model_logstic/tensorflow_logistic_virus01/#_4","text":"\u307e\u305f\u3001\u611f\u67d3\u72b6\u6cc1\u3092\u8868\u3059\u884c\u5217 STATE\u30922x1000\u884c\u5217\u3067\u4f5c\u6210\u3057\u3001\u611f\u67d3\u3057\u3066\u3044\u308b\u5834\u5408\u306f[1,0], \u611f\u67d3\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001[0, 1]\u3092\u4ee3\u5165\u3059\u308b\u3002\u524d\u306e\u9805\u76ee\u3067\u3001\u524d\u534a500\u500b\u3092\u611f\u67d3\u6e08\u307f\u3001\u5f8c\u534a\u306e500\u500b\u3092\u975e\u611f\u67d3\u3068\u3057\u30662x1000\u884c\u5217\u306b\u683c\u7d0d\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u524d\u534a\u306e500\u500b\u3092\u611f\u67d3\u6e08\u307f\u3068\u3057\u3066[1,0]\u3068\u3057\u3001\u5f8c\u534a\u306e5000\u500b\u3092\u672a\u611f\u67d3\u3068\u3057\u3066[0,1]\u3092\u4ee3\u5165\u3059\u308b\u3002 [1,0] \u611f\u67d3 [0,1] \u975e\u611f\u67d3 1 2 3 4 5 6 STATE = np . zeros (( N * 2 , 2 )) for i in xrange ( N * 2 ) : if i < N : STATE [ i ][ 0 ] = 1 else : STATE [ i ][ 1 ] = 1","title":"\u611f\u67d3/\u975e\u611f\u67d3\u306e\u30e9\u30d9\u30eb\u4ed8\u3051"},{"location":"model_logstic/tensorflow_logistic_virus01/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/virus01.ipynb","title":"Notebook"},{"location":"model_logstic/tensorflow_logistic_virus02/","text":"\u30a6\u30a3\u30eb\u30b9\u5206\u5e03\u3000\u5b66\u7fd2 TensorBoard\u306b\u8868\u793a\u3059\u308b\u30b0\u30e9\u30d5\u3092\u30ea\u30bb\u30c3\u30c8\u3057\u3001TensorFlow\u306e\u5909\u6570\u3068\u30d7\u30ec\u30fc\u30b9\u30d5\u30a9\u30eb\u30c0\u3092\u5b9a\u7fa9\u3059\u308b\u3002 \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b\u3002 y = x * w + b \u30b3\u30b9\u30c8\u306e\u5b9a\u7fa9\u3092\u3059\u308b\u3002 \u7cbe\u5ea6\u306e\u5b9a\u7fa9\u3092\u3059\u308b\u3002 \u30bb\u30c3\u30b7\u30e7\u30f3\u5185\u3067\u521d\u671f\u5316\u3092\u3057\u3001\u5b66\u7fd2\u3092\u5b9f\u65bd\u3059\u308b\u3002 Coding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt x_positive = np . random . randn ( 500 , 1 ) + 2 y_positive = np . random . randn ( 500 , 1 ) + 2 x_negative = np . random . randn ( 500 , 1 ) - 2 y_negative = np . random . randn ( 500 , 1 ) - 2 plt . figure ( 1 ) plt . plot ( x_positive , y_positive , 'ro' , label = 'Data1' ) plt . plot ( x_negative , y_negative , 'bo' , label = 'Data2' ) N = len ( x_positive ) POSITIVE = np . zeros (( N , 2 )) for i in xrange ( N ): POSITIVE [ i ][ 0 ] = x_positive [ i ] POSITIVE [ i ][ 1 ] = y_positive [ i ] NEGATIVE = np . zeros (( N , 2 )) for i in xrange ( N ): NEGATIVE [ i ][ 0 ] = x_negative [ i ] NEGATIVE [ i ][ 1 ] = y_negative [ i ] VIRUS = np . vstack ([ NEGATIVE , POSITIVE ]) . astype ( np . float32 ) print VIRUS STATE = np . zeros (( N * 2 , 2 ), dtype = np . float32 ) for i in xrange ( N * 2 ): if i < N : STATE [ i ][ 1 ] = 1 else : STATE [ i ][ 0 ] = 1 print STATE tf . reset_default_graph () LOGDIR = \"./data_virus\" x = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"input\" ) y = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"output\" ) w = tf . Variable ( tf . random_normal ([ 2 , 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"weight\" ) b = tf . Variable ( tf . random_normal ([ 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"bias\" ) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9 with tf . name_scope ( 'forward' ): y_pred = tf . nn . softmax ( tf . matmul ( x , w ) + b , name = \"forward\" ) # \u30b3\u30b9\u30c8\u306e\u8a08\u7b97 with tf . name_scope ( 'cost' ): loss = tf . nn . softmax_cross_entropy_with_logits ( labels = y , logits = y_pred ) cost = tf . reduce_mean ( loss , 0 ) # \u7cbe\u5ea6\u306e\u8a08\u7b97 with tf . name_scope ( 'accuracy' ): correct_pred = tf . equal ( tf . argmax ( y_pred , 1 ), tf . argmax ( STATE , 1 )) accuracy_op = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . histogram ( \"W_graph\" , w ) b_graph = tf . summary . histogram ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) cost_graph = tf . summary . scalar ( \"cost_graph\" , cost ) with tf . Session () as sess : # \u521d\u671f\u5316\u51e6\u7406 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () with tf . Graph () . as_default (): # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in xrange ( training_step ): sess . run ( train_op , feed_dict = { x : VIRUS , y : STATE }) if step % validation_step == 0 : accuracy_output , cost_output = sess . run ([ accuracy_op , cost ], feed_dict = { x : VIRUS , y : STATE }) print \"step %d , cost %f , accuracy %f \" % ( step , cost_output , accuracy_output ) # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : VIRUS , y : STATE }) summary_writer . add_summary ( summary_str , step ) summary_writer . flush () TensorBoard TensorBoard\u306f\u3001\u3069\u3093\u3069\u3093\u30c7\u30fc\u30bf\u304c\u84c4\u7a4d\u3055\u308c\u3066\u3044\u308b\u304f\u306e\u3067\u3001TensorFlow\u306e\u5b66\u7fd2\u524d\u306b\u3001data_virus\u30d5\u30a9\u30eb\u30c0\u3092\u524a\u9664\u3059\u308b\u3002 !rm -r ./data_virus TensorBoard\u3092\u8d77\u52d5\u3059\u308b\u3002TensorBoard\u306f\u3001Datalab\u3067\u306fTensorBoard\u3092Foreground\u3067\u3057\u304b\u5b9f\u884c\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u505c\u6b62\u3059\u308b\u5834\u5408\u306f\u3001\u30e1\u30cb\u30e5\u30fc\u306e[Reset Session]\u306eReset\u3067\u505c\u6b62\u3059\u308b\u3002 !tensorboard --logdir=data_virus/ docker\u74b0\u5883\u3067tensorboard\u3092\u5229\u7528\u3059\u308b\u306b\u306f6066\u30dd\u30fc\u30c8\u3078\u306e\u30c8\u30f3\u30cd\u30eb\u3082\u5fc5\u8981 docker run -it -p 6006:6006 -p 8888:8888 tensorflow/tensorflow Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/virus02.ipynb","title":"\u30a6\u30a3\u30eb\u30b9\u5206\u5e03\u3000\u5b66\u7fd2"},{"location":"model_logstic/tensorflow_logistic_virus02/#_1","text":"TensorBoard\u306b\u8868\u793a\u3059\u308b\u30b0\u30e9\u30d5\u3092\u30ea\u30bb\u30c3\u30c8\u3057\u3001TensorFlow\u306e\u5909\u6570\u3068\u30d7\u30ec\u30fc\u30b9\u30d5\u30a9\u30eb\u30c0\u3092\u5b9a\u7fa9\u3059\u308b\u3002 \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b\u3002 y = x * w + b \u30b3\u30b9\u30c8\u306e\u5b9a\u7fa9\u3092\u3059\u308b\u3002 \u7cbe\u5ea6\u306e\u5b9a\u7fa9\u3092\u3059\u308b\u3002 \u30bb\u30c3\u30b7\u30e7\u30f3\u5185\u3067\u521d\u671f\u5316\u3092\u3057\u3001\u5b66\u7fd2\u3092\u5b9f\u65bd\u3059\u308b\u3002","title":"\u30a6\u30a3\u30eb\u30b9\u5206\u5e03\u3000\u5b66\u7fd2"},{"location":"model_logstic/tensorflow_logistic_virus02/#coding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt x_positive = np . random . randn ( 500 , 1 ) + 2 y_positive = np . random . randn ( 500 , 1 ) + 2 x_negative = np . random . randn ( 500 , 1 ) - 2 y_negative = np . random . randn ( 500 , 1 ) - 2 plt . figure ( 1 ) plt . plot ( x_positive , y_positive , 'ro' , label = 'Data1' ) plt . plot ( x_negative , y_negative , 'bo' , label = 'Data2' ) N = len ( x_positive ) POSITIVE = np . zeros (( N , 2 )) for i in xrange ( N ): POSITIVE [ i ][ 0 ] = x_positive [ i ] POSITIVE [ i ][ 1 ] = y_positive [ i ] NEGATIVE = np . zeros (( N , 2 )) for i in xrange ( N ): NEGATIVE [ i ][ 0 ] = x_negative [ i ] NEGATIVE [ i ][ 1 ] = y_negative [ i ] VIRUS = np . vstack ([ NEGATIVE , POSITIVE ]) . astype ( np . float32 ) print VIRUS STATE = np . zeros (( N * 2 , 2 ), dtype = np . float32 ) for i in xrange ( N * 2 ): if i < N : STATE [ i ][ 1 ] = 1 else : STATE [ i ][ 0 ] = 1 print STATE tf . reset_default_graph () LOGDIR = \"./data_virus\" x = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"input\" ) y = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"output\" ) w = tf . Variable ( tf . random_normal ([ 2 , 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"weight\" ) b = tf . Variable ( tf . random_normal ([ 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"bias\" ) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9 with tf . name_scope ( 'forward' ): y_pred = tf . nn . softmax ( tf . matmul ( x , w ) + b , name = \"forward\" ) # \u30b3\u30b9\u30c8\u306e\u8a08\u7b97 with tf . name_scope ( 'cost' ): loss = tf . nn . softmax_cross_entropy_with_logits ( labels = y , logits = y_pred ) cost = tf . reduce_mean ( loss , 0 ) # \u7cbe\u5ea6\u306e\u8a08\u7b97 with tf . name_scope ( 'accuracy' ): correct_pred = tf . equal ( tf . argmax ( y_pred , 1 ), tf . argmax ( STATE , 1 )) accuracy_op = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . histogram ( \"W_graph\" , w ) b_graph = tf . summary . histogram ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) cost_graph = tf . summary . scalar ( \"cost_graph\" , cost ) with tf . Session () as sess : # \u521d\u671f\u5316\u51e6\u7406 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () with tf . Graph () . as_default (): # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in xrange ( training_step ): sess . run ( train_op , feed_dict = { x : VIRUS , y : STATE }) if step % validation_step == 0 : accuracy_output , cost_output = sess . run ([ accuracy_op , cost ], feed_dict = { x : VIRUS , y : STATE }) print \"step %d , cost %f , accuracy %f \" % ( step , cost_output , accuracy_output ) # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : VIRUS , y : STATE }) summary_writer . add_summary ( summary_str , step ) summary_writer . flush ()","title":"Coding"},{"location":"model_logstic/tensorflow_logistic_virus02/#tensorboard","text":"TensorBoard\u306f\u3001\u3069\u3093\u3069\u3093\u30c7\u30fc\u30bf\u304c\u84c4\u7a4d\u3055\u308c\u3066\u3044\u308b\u304f\u306e\u3067\u3001TensorFlow\u306e\u5b66\u7fd2\u524d\u306b\u3001data_virus\u30d5\u30a9\u30eb\u30c0\u3092\u524a\u9664\u3059\u308b\u3002 !rm -r ./data_virus TensorBoard\u3092\u8d77\u52d5\u3059\u308b\u3002TensorBoard\u306f\u3001Datalab\u3067\u306fTensorBoard\u3092Foreground\u3067\u3057\u304b\u5b9f\u884c\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u505c\u6b62\u3059\u308b\u5834\u5408\u306f\u3001\u30e1\u30cb\u30e5\u30fc\u306e[Reset Session]\u306eReset\u3067\u505c\u6b62\u3059\u308b\u3002 !tensorboard --logdir=data_virus/ docker\u74b0\u5883\u3067tensorboard\u3092\u5229\u7528\u3059\u308b\u306b\u306f6066\u30dd\u30fc\u30c8\u3078\u306e\u30c8\u30f3\u30cd\u30eb\u3082\u5fc5\u8981 docker run -it -p 6006:6006 -p 8888:8888 tensorflow/tensorflow","title":"TensorBoard"},{"location":"model_logstic/tensorflow_logistic_virus02/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/virus02.ipynb","title":"Notebook"},{"location":"model_logstic/tensorflow_logistic_virus03/","text":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 \u5b66\u7fd2\u6e08\u307f\u30c7\u30fc\u30bf \u7d50\u679c\u306e\u8a55\u4fa1 \u4e88\u6e2c\u7528\u306eOperation\u3092\u7528\u610f\u3057\u3066\u3001[-2,-2]\u306e\u5ea7\u6a19\u3068[2,2]\u306e\u5ea7\u6a19\u3092\u6e21\u3057\u3001\u30a6\u30a3\u30eb\u30b9\u306b\u611f\u67d3\u3057\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # \u4e88\u6e2c with tf . name_scope ( 'predict' ): predict_op = tf . argmax ( y_pred , 1 ) ... # check anser data = [[ - 2 , - 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos ) data = [[ 2 , 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos ) \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u6e96\u5099 saver = tf . train . Saver () with tf . Session () as sess : ... # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : VIRUS , y : STATE }) summary_writer . add_summary ( summary_str , step ) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u3000\u30d5\u30a1\u30a4\u30eb\u540d\uff1acheckpoint\u3068virus-model-100.[data-00000-of-00001|index|meta] (step=100\u306e\u5834\u5408)\u304c\u4f5c\u3089\u308c\u308b saver . save ( sess , \"virus-model\" , global_step = step ) summary_writer . flush () \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u6e96\u5099 saver = tf . train . Saver () with tf . Session () as sess : # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306echeckpoint\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308b\u304b\u3069\u3046\u304b ckpt = tf . train . get_checkpoint_state ( './' ) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print \u3000 \"load \" + last_model # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) else : print \"initialization\" # \u521d\u671f\u5316\u51e6\u7406 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) ... Coding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt x_positive = np . random . randn ( 500 , 1 ) + 2 y_positive = np . random . randn ( 500 , 1 ) + 2 x_negative = np . random . randn ( 500 , 1 ) - 2 y_negative = np . random . randn ( 500 , 1 ) - 2 plt . figure ( 1 ) plt . plot ( x_positive , y_positive , 'ro' , label = 'Data1' ) plt . plot ( x_negative , y_negative , 'bo' , label = 'Data2' ) N = len ( x_positive ) POSITIVE = np . zeros (( N , 2 )) for i in xrange ( N ): POSITIVE [ i ][ 0 ] = x_positive [ i ] POSITIVE [ i ][ 1 ] = y_positive [ i ] NEGATIVE = np . zeros (( N , 2 )) for i in xrange ( N ): NEGATIVE [ i ][ 0 ] = x_negative [ i ] NEGATIVE [ i ][ 1 ] = y_negative [ i ] VIRUS = np . vstack ([ NEGATIVE , POSITIVE ]) . astype ( np . float32 ) print VIRUS STATE = np . zeros (( N * 2 , 2 ), dtype = np . float32 ) for i in xrange ( N * 2 ): if i < N : STATE [ i ][ 1 ] = 1 else : STATE [ i ][ 0 ] = 1 print STATE tf . reset_default_graph () LOGDIR = \"./data_virus\" x = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"input\" ) y = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"output\" ) w = tf . Variable ( tf . random_normal ([ 2 , 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"weight\" ) b = tf . Variable ( tf . random_normal ([ 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"bias\" ) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9 with tf . name_scope ( 'forward' ): y_pred = tf . nn . softmax ( tf . matmul ( x , w ) + b , name = \"forward\" ) # \u30b3\u30b9\u30c8\u306e\u8a08\u7b97 with tf . name_scope ( 'cost' ): loss = tf . nn . softmax_cross_entropy_with_logits ( labels = y , logits = y_pred ) cost = tf . reduce_mean ( loss , 0 ) # \u7cbe\u5ea6\u306e\u8a08\u7b97 with tf . name_scope ( 'accuracy' ): correct_pred = tf . equal ( tf . argmax ( y_pred , 1 ), tf . argmax ( STATE , 1 )) accuracy_op = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) # \u4e88\u6e2c with tf . name_scope ( 'predict' ): predict_op = tf . argmax ( y_pred , 1 ) # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . histogram ( \"W_graph\" , w ) b_graph = tf . summary . histogram ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) cost_graph = tf . summary . scalar ( \"cost_graph\" , cost ) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u6e96\u5099 saver = tf . train . Saver () with tf . Session () as sess : # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306echeckpoint\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308b\u304b\u3069\u3046\u304b ckpt = tf . train . get_checkpoint_state ( './' ) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) else : print ( \"initialization\" ) # \u521d\u671f\u5316\u51e6\u7406 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () with tf . Graph () . as_default (): # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in xrange ( training_step ): sess . run ( train_op , feed_dict = { x : VIRUS , y : STATE }) if step % validation_step == 0 : accuracy_output , cost_output = sess . run ([ accuracy_op , cost ], feed_dict = { x : VIRUS , y : STATE }) print \"step %d , cost %f , accuracy %f \" % ( step , cost_output , accuracy_output ) # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : VIRUS , y : STATE }) summary_writer . add_summary ( summary_str , step ) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u3000\u30d5\u30a1\u30a4\u30eb\u540d\uff1acheckpoint\u3068virus-model-100.[data-00000-of-00001|index|meta] (step=100\u306e\u5834\u5408)\u304c\u4f5c\u3089\u308c\u308b saver . save ( sess , \"virus-model\" , global_step = step ) summary_writer . flush () # check anser data = [[ - 2 , - 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos ) data = [[ 2 , 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos ) 1\u306eflag\u304c\u305f\u3063\u3066\u3044\u308b\u914d\u5217\u306e\u5834\u6240\u304c\u7d50\u679c\u3068\u3057\u3066\u53d6\u5f97\u3067\u304d\u308b\u3002 1\u306a\u3089[1,0], 0\u306a\u3089[0,1] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/virus03.ipynb","title":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 \u5b66\u7fd2\u6e08\u307f\u30c7\u30fc\u30bf"},{"location":"model_logstic/tensorflow_logistic_virus03/#_1","text":"","title":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 \u5b66\u7fd2\u6e08\u307f\u30c7\u30fc\u30bf"},{"location":"model_logstic/tensorflow_logistic_virus03/#_2","text":"\u4e88\u6e2c\u7528\u306eOperation\u3092\u7528\u610f\u3057\u3066\u3001[-2,-2]\u306e\u5ea7\u6a19\u3068[2,2]\u306e\u5ea7\u6a19\u3092\u6e21\u3057\u3001\u30a6\u30a3\u30eb\u30b9\u306b\u611f\u67d3\u3057\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3057\u307e\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # \u4e88\u6e2c with tf . name_scope ( 'predict' ): predict_op = tf . argmax ( y_pred , 1 ) ... # check anser data = [[ - 2 , - 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos ) data = [[ 2 , 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos )","title":"\u7d50\u679c\u306e\u8a55\u4fa1"},{"location":"model_logstic/tensorflow_logistic_virus03/#_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u6e96\u5099 saver = tf . train . Saver () with tf . Session () as sess : ... # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : VIRUS , y : STATE }) summary_writer . add_summary ( summary_str , step ) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u3000\u30d5\u30a1\u30a4\u30eb\u540d\uff1acheckpoint\u3068virus-model-100.[data-00000-of-00001|index|meta] (step=100\u306e\u5834\u5408)\u304c\u4f5c\u3089\u308c\u308b saver . save ( sess , \"virus-model\" , global_step = step ) summary_writer . flush ()","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58"},{"location":"model_logstic/tensorflow_logistic_virus03/#_4","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u6e96\u5099 saver = tf . train . Saver () with tf . Session () as sess : # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306echeckpoint\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308b\u304b\u3069\u3046\u304b ckpt = tf . train . get_checkpoint_state ( './' ) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print \u3000 \"load \" + last_model # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) else : print \"initialization\" # \u521d\u671f\u5316\u51e6\u7406 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) ...","title":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080"},{"location":"model_logstic/tensorflow_logistic_virus03/#coding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt x_positive = np . random . randn ( 500 , 1 ) + 2 y_positive = np . random . randn ( 500 , 1 ) + 2 x_negative = np . random . randn ( 500 , 1 ) - 2 y_negative = np . random . randn ( 500 , 1 ) - 2 plt . figure ( 1 ) plt . plot ( x_positive , y_positive , 'ro' , label = 'Data1' ) plt . plot ( x_negative , y_negative , 'bo' , label = 'Data2' ) N = len ( x_positive ) POSITIVE = np . zeros (( N , 2 )) for i in xrange ( N ): POSITIVE [ i ][ 0 ] = x_positive [ i ] POSITIVE [ i ][ 1 ] = y_positive [ i ] NEGATIVE = np . zeros (( N , 2 )) for i in xrange ( N ): NEGATIVE [ i ][ 0 ] = x_negative [ i ] NEGATIVE [ i ][ 1 ] = y_negative [ i ] VIRUS = np . vstack ([ NEGATIVE , POSITIVE ]) . astype ( np . float32 ) print VIRUS STATE = np . zeros (( N * 2 , 2 ), dtype = np . float32 ) for i in xrange ( N * 2 ): if i < N : STATE [ i ][ 1 ] = 1 else : STATE [ i ][ 0 ] = 1 print STATE tf . reset_default_graph () LOGDIR = \"./data_virus\" x = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"input\" ) y = tf . placeholder ( tf . float32 , shape = ( None , 2 ), name = \"output\" ) w = tf . Variable ( tf . random_normal ([ 2 , 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"weight\" ) b = tf . Variable ( tf . random_normal ([ 2 ], stddev = 0.01 ), dtype = tf . float32 , name = \"bias\" ) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9 with tf . name_scope ( 'forward' ): y_pred = tf . nn . softmax ( tf . matmul ( x , w ) + b , name = \"forward\" ) # \u30b3\u30b9\u30c8\u306e\u8a08\u7b97 with tf . name_scope ( 'cost' ): loss = tf . nn . softmax_cross_entropy_with_logits ( labels = y , logits = y_pred ) cost = tf . reduce_mean ( loss , 0 ) # \u7cbe\u5ea6\u306e\u8a08\u7b97 with tf . name_scope ( 'accuracy' ): correct_pred = tf . equal ( tf . argmax ( y_pred , 1 ), tf . argmax ( STATE , 1 )) accuracy_op = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) # \u4e88\u6e2c with tf . name_scope ( 'predict' ): predict_op = tf . argmax ( y_pred , 1 ) # TensorBoard\u3078\u306e\u53cd\u6620 w_graph = tf . summary . histogram ( \"W_graph\" , w ) b_graph = tf . summary . histogram ( \"b_graph\" , b ) y_graph = tf . summary . histogram ( \"y_graph\" , y ) cost_graph = tf . summary . scalar ( \"cost_graph\" , cost ) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u6e96\u5099 saver = tf . train . Saver () with tf . Session () as sess : # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306echeckpoint\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308b\u304b\u3069\u3046\u304b ckpt = tf . train . get_checkpoint_state ( './' ) if ckpt : # checkpoint\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u6700\u5f8c\u306b\u4fdd\u5b58\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306e\u30d1\u30b9\u3092\u53d6\u5f97\u3059\u308b last_model = ckpt . model_checkpoint_path print ( \"load {0} \" . format ( last_model )) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u3080 saver . restore ( sess , last_model ) else : print ( \"initialization\" ) # \u521d\u671f\u5316\u51e6\u7406 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 learning_rate = 0.01 train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) summary_op = tf . summary . merge_all () with tf . Graph () . as_default (): # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 1000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in xrange ( training_step ): sess . run ( train_op , feed_dict = { x : VIRUS , y : STATE }) if step % validation_step == 0 : accuracy_output , cost_output = sess . run ([ accuracy_op , cost ], feed_dict = { x : VIRUS , y : STATE }) print \"step %d , cost %f , accuracy %f \" % ( step , cost_output , accuracy_output ) # TensorBoard\u306b\u3082\u53cd\u6620 summary_str = sess . run ( summary_op , feed_dict = { x : VIRUS , y : STATE }) summary_writer . add_summary ( summary_str , step ) # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u3000\u30d5\u30a1\u30a4\u30eb\u540d\uff1acheckpoint\u3068virus-model-100.[data-00000-of-00001|index|meta] (step=100\u306e\u5834\u5408)\u304c\u4f5c\u3089\u308c\u308b saver . save ( sess , \"virus-model\" , global_step = step ) summary_writer . flush () # check anser data = [[ - 2 , - 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos ) data = [[ 2 , 2 ]] x_check = np . array ( data ) flag_pos = sess . run ( predict_op , feed_dict = { x : x_check }) print \"flag position is %d \" % ( flag_pos ) 1\u306eflag\u304c\u305f\u3063\u3066\u3044\u308b\u914d\u5217\u306e\u5834\u6240\u304c\u7d50\u679c\u3068\u3057\u3066\u53d6\u5f97\u3067\u304d\u308b\u3002 1\u306a\u3089[1,0], 0\u306a\u3089[0,1]","title":"Coding"},{"location":"model_logstic/tensorflow_logistic_virus03/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/notebooks/virus03.ipynb","title":"Notebook"},{"location":"model_logstic/tensorflow_logistic_virus_anroid/","text":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 Android\u3067\u52d5\u4f5c TensorFlow Notebook \u51fa\u529b\u3057\u305f\u30b0\u30e9\u30d5 \u51fa\u529b\u3057\u305f\u30b0\u30e9\u30d5(\u30d0\u30a4\u30ca\u30ea) Android\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package fabo.io.hellotensorflow ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ){ super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/graph-virus.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"---------\" ); Log . i ( TAG , \"initializeTensorFlow:result:\" + result ); float [] x_value = new float [ 2 ] ; x_value [ 0 ] = ( float ) 2.0 ; x_value [ 1 ] = ( float ) 2.0 ; mTensorFlowIF . fillNodeFloat ( \"input\" , new int [] { 0 , 2 }, x_value ); // Add mTensorFlowIF . runInference ( new String [] { \"add_op\" }); float [] result_value1 = new float [ 2 ] ; mTensorFlowIF . readNodeFloat ( \"add_op\" , result_value1 ); Log . i ( TAG , \"result_add: \" + result_value1 [ 0 ] ); Log . i ( TAG , \"result_add: \" + result_value1 [ 1 ] ); // Predict mTensorFlowIF . runInference ( new String [] { \"predict_op200\" }); float [] result_value2 = new float [ 2 ] ; mTensorFlowIF . readNodeFloat ( \"predict_op200\" , result_value2 ); Log . i ( TAG , \"result_predict: \" + result_value2 [ 0 ] ); Log . i ( TAG , \"result_predict: \" + result_value2 [ 1 ] ); } } predict_op200, app_op\u3082\u51e6\u7406\u304c\u8d70\u308b\u304c\u5024\u304c\u5909\u308f\u3089\u306a\u3044\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 package fabo.io.hellotensorflow ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ){ super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/graph-virus.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"---------\" ); Log . i ( TAG , \"initializeTensorFlow:result:\" + result ); float [] x_value = new float [ 2 ] ; x_value [ 0 ] = ( float ) 9.0 ; x_value [ 1 ] = ( float ) 3.0 ; mTensorFlowIF . fillNodeFloat ( \"input\" , new int [] { 2 }, x_value ); // Add mTensorFlowIF . runInference ( new String [] { \"add_op\" }); float [] result_value1 = new float [ 2 ] ; mTensorFlowIF . readNodeFloat ( \"add_op\" , result_value1 ); Log . i ( TAG , \"result_add: \" + result_value1 [ 0 ] ); Log . i ( TAG , \"result_add: \" + result_value1 [ 1 ] ); // Predict int x_cols = 2 ; int x_rows = 10 ; float x_value2 [] = { /* x_rows[0] */ - 2 , - 2 , /* x_rows[1] */ 0 , 0 , /* x_rows[2] */ - 2 , 2 , /* x_rows[3] */ - 2 /* error? */ /* x_rows[4] */ /* error? */ /* x_rows[...] */ /* error? */ }; mTensorFlowIF . fillNodeFloat ( \"input\" , new int [] { x_rows , x_cols }, x_value2 ); int runInference = mTensorFlowIF . runInference ( new String [] { \"predict_op200\" }); float [] result_value2 = new float [ x_rows ] ; mTensorFlowIF . readNodeFloat ( \"predict_op200\" , result_value2 ); // \u5165\u529b\u5024->\u51fa\u529b\u3092\u6574\u5f62 String x []= new String [ x_rows ] ; String message = \"\" ; for ( int row = 0 ; row < x_rows ; row ++ ) { message = \"x(\" ; for ( int col = 0 ; col < x_cols ; col ++ ) { x [ col ] = x_value2 . length > ( row * x_cols + col ) ? Float . toString ( x_value2 [ row * x_cols + col ] ) : \"?\" ; if ( col > 0 ) { message += \",\" ; } message += x [ col ] ; } message += \") -> result_predict: \" + result_value2 [ row ] ; Log . i ( TAG , message ); } } } } \u51fa\u529b 1 2 3 4 5 6 7 8 9 10 11 12 13 I / TF_LOG : result_add : 18 . 0 I / TF_LOG : result_add : 6 . 0 I / TF_LOG : x ( - 2 . 0 , - 2 . 0 ) -> result_predict : 1 . 0 I / TF_LOG : x ( 0 . 0 , 0 . 0 ) -> result_predict : 0 . 0 I / TF_LOG : x ( - 2 . 0 , 2 . 0 ) -> result_predict : 1 . 0 I / TF_LOG : x ( - 2 . 0 , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0","title":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 Android\u3067\u52d5\u4f5c"},{"location":"model_logstic/tensorflow_logistic_virus_anroid/#android","text":"","title":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 Android\u3067\u52d5\u4f5c"},{"location":"model_logstic/tensorflow_logistic_virus_anroid/#tensorflow","text":"Notebook \u51fa\u529b\u3057\u305f\u30b0\u30e9\u30d5 \u51fa\u529b\u3057\u305f\u30b0\u30e9\u30d5(\u30d0\u30a4\u30ca\u30ea)","title":"TensorFlow"},{"location":"model_logstic/tensorflow_logistic_virus_anroid/#android_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package fabo.io.hellotensorflow ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ){ super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/graph-virus.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"---------\" ); Log . i ( TAG , \"initializeTensorFlow:result:\" + result ); float [] x_value = new float [ 2 ] ; x_value [ 0 ] = ( float ) 2.0 ; x_value [ 1 ] = ( float ) 2.0 ; mTensorFlowIF . fillNodeFloat ( \"input\" , new int [] { 0 , 2 }, x_value ); // Add mTensorFlowIF . runInference ( new String [] { \"add_op\" }); float [] result_value1 = new float [ 2 ] ; mTensorFlowIF . readNodeFloat ( \"add_op\" , result_value1 ); Log . i ( TAG , \"result_add: \" + result_value1 [ 0 ] ); Log . i ( TAG , \"result_add: \" + result_value1 [ 1 ] ); // Predict mTensorFlowIF . runInference ( new String [] { \"predict_op200\" }); float [] result_value2 = new float [ 2 ] ; mTensorFlowIF . readNodeFloat ( \"predict_op200\" , result_value2 ); Log . i ( TAG , \"result_predict: \" + result_value2 [ 0 ] ); Log . i ( TAG , \"result_predict: \" + result_value2 [ 1 ] ); } } predict_op200, app_op\u3082\u51e6\u7406\u304c\u8d70\u308b\u304c\u5024\u304c\u5909\u308f\u3089\u306a\u3044\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 package fabo.io.hellotensorflow ; import android.content.res.AssetManager ; import android.support.v7.app.AppCompatActivity ; import android.os.Bundle ; import android.util.Log ; import org.tensorflow.contrib.android.TensorFlowInferenceInterface ; public class MainActivity extends AppCompatActivity { private final static String TAG = \"TF_LOG\" ; static { System . loadLibrary ( \"tensorflow_inference\" ); } @Override protected void onCreate ( Bundle savedInstanceState ){ super . onCreate ( savedInstanceState ); setContentView ( R . layout . activity_main ); TensorFlowInferenceInterface mTensorFlowIF = new TensorFlowInferenceInterface (); AssetManager mAssetManager = getAssets (); int result = mTensorFlowIF . initializeTensorFlow ( mAssetManager , \"file:///android_asset/graph-virus.pb\" ); mTensorFlowIF . enableStatLogging ( true ); Log . i ( TAG , \"---------\" ); Log . i ( TAG , \"initializeTensorFlow:result:\" + result ); float [] x_value = new float [ 2 ] ; x_value [ 0 ] = ( float ) 9.0 ; x_value [ 1 ] = ( float ) 3.0 ; mTensorFlowIF . fillNodeFloat ( \"input\" , new int [] { 2 }, x_value ); // Add mTensorFlowIF . runInference ( new String [] { \"add_op\" }); float [] result_value1 = new float [ 2 ] ; mTensorFlowIF . readNodeFloat ( \"add_op\" , result_value1 ); Log . i ( TAG , \"result_add: \" + result_value1 [ 0 ] ); Log . i ( TAG , \"result_add: \" + result_value1 [ 1 ] ); // Predict int x_cols = 2 ; int x_rows = 10 ; float x_value2 [] = { /* x_rows[0] */ - 2 , - 2 , /* x_rows[1] */ 0 , 0 , /* x_rows[2] */ - 2 , 2 , /* x_rows[3] */ - 2 /* error? */ /* x_rows[4] */ /* error? */ /* x_rows[...] */ /* error? */ }; mTensorFlowIF . fillNodeFloat ( \"input\" , new int [] { x_rows , x_cols }, x_value2 ); int runInference = mTensorFlowIF . runInference ( new String [] { \"predict_op200\" }); float [] result_value2 = new float [ x_rows ] ; mTensorFlowIF . readNodeFloat ( \"predict_op200\" , result_value2 ); // \u5165\u529b\u5024->\u51fa\u529b\u3092\u6574\u5f62 String x []= new String [ x_rows ] ; String message = \"\" ; for ( int row = 0 ; row < x_rows ; row ++ ) { message = \"x(\" ; for ( int col = 0 ; col < x_cols ; col ++ ) { x [ col ] = x_value2 . length > ( row * x_cols + col ) ? Float . toString ( x_value2 [ row * x_cols + col ] ) : \"?\" ; if ( col > 0 ) { message += \",\" ; } message += x [ col ] ; } message += \") -> result_predict: \" + result_value2 [ row ] ; Log . i ( TAG , message ); } } } } \u51fa\u529b 1 2 3 4 5 6 7 8 9 10 11 12 13 I / TF_LOG : result_add : 18 . 0 I / TF_LOG : result_add : 6 . 0 I / TF_LOG : x ( - 2 . 0 , - 2 . 0 ) -> result_predict : 1 . 0 I / TF_LOG : x ( 0 . 0 , 0 . 0 ) -> result_predict : 0 . 0 I / TF_LOG : x ( - 2 . 0 , 2 . 0 ) -> result_predict : 1 . 0 I / TF_LOG : x ( - 2 . 0 , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0 I / TF_LOG : x ( ? , ? ) -> result_predict : 0 . 0","title":"Android\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9"},{"location":"model_logstic/tensorflow_logistic_wine01/","text":"\u30ef\u30a4\u30f3\u8a55\u4fa1 !curl -o \"./wine.csv\" http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv \u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306f\u3001\u4e0b\u8a18\u306e\u901a\u308a\u3002 \u3053\u306e\u30c7\u30fc\u30bf\u3092data\u3068label\u306b\u5206\u96e2\u3059\u308b\u3002 Label\u306e1-10\u306e\u8a55\u4fa1\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # coding:utf-8 import numpy as np import tensorflow as tf import pandas as pd # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./wine.csv\" , delimiter = ';' , dtype = np . float32 , filling_values = ( 0 ), skip_header = 1 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # Wine\u306e\u30c7\u30fc\u30bf datas = dataset [:, 0 : 11 ] # Wine\u306e\u30e9\u30d9\u30eb(\u54c1\u8cea) labels = dataset [:, 11 ] N = len ( labels ) vector_labels = np . zeros (( N , 10 )) for i in xrange ( N ): vector_labels [ i ][ int ( labels [ i ])] = 1.0 # \u30c7\u30fc\u30bf\u30927:1\u3067\u5206\u5272\u3059\u308b train_data_size = len ( dataset ) - len ( dataset ) // 8 test_data_size = len ( dataset ) // 8 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_datas = datas [: train_data_size ] train_labels = vector_labels [: train_data_size ] . reshape ( train_data_size , 1 ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_datas = datas [ train_data_size :] test_labels = vector_labels [ train_data_size :] . reshape ( test_data_size , 1 ) # \u30c7\u30fc\u30bf\u30921\u4ef6\u62bd\u51fa\u3057\u3001\u8868\u793a\u3059\u308b print train_datas [ 0 ] print train_labels [ 0 ] \u51fa\u529b\u7d50\u679c 1 2 3 4 [ 4 .59999990e+00 5 .19999981e-01 1 .50000006e-01 2 .09999990e+00 5 .40000014e-02 8 .00000000e+00 6 .50000000e+01 9 .93399978e-01 3 .90000010e+00 5 .60000002e-01 1 .31000004e+01 ] [ 0 ,0,0,0.0,0,0,1,0,0,0 ]","title":"\u30ef\u30a4\u30f3\u8a55\u4fa1"},{"location":"model_logstic/tensorflow_logistic_wine01/#_1","text":"!curl -o \"./wine.csv\" http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv \u30c7\u30fc\u30bf\u30fb\u30bb\u30c3\u30c8\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306f\u3001\u4e0b\u8a18\u306e\u901a\u308a\u3002 \u3053\u306e\u30c7\u30fc\u30bf\u3092data\u3068label\u306b\u5206\u96e2\u3059\u308b\u3002 Label\u306e1-10\u306e\u8a55\u4fa1\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # coding:utf-8 import numpy as np import tensorflow as tf import pandas as pd # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./wine.csv\" , delimiter = ';' , dtype = np . float32 , filling_values = ( 0 ), skip_header = 1 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # Wine\u306e\u30c7\u30fc\u30bf datas = dataset [:, 0 : 11 ] # Wine\u306e\u30e9\u30d9\u30eb(\u54c1\u8cea) labels = dataset [:, 11 ] N = len ( labels ) vector_labels = np . zeros (( N , 10 )) for i in xrange ( N ): vector_labels [ i ][ int ( labels [ i ])] = 1.0 # \u30c7\u30fc\u30bf\u30927:1\u3067\u5206\u5272\u3059\u308b train_data_size = len ( dataset ) - len ( dataset ) // 8 test_data_size = len ( dataset ) // 8 # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_datas = datas [: train_data_size ] train_labels = vector_labels [: train_data_size ] . reshape ( train_data_size , 1 ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_datas = datas [ train_data_size :] test_labels = vector_labels [ train_data_size :] . reshape ( test_data_size , 1 ) # \u30c7\u30fc\u30bf\u30921\u4ef6\u62bd\u51fa\u3057\u3001\u8868\u793a\u3059\u308b print train_datas [ 0 ] print train_labels [ 0 ] \u51fa\u529b\u7d50\u679c 1 2 3 4 [ 4 .59999990e+00 5 .19999981e-01 1 .50000006e-01 2 .09999990e+00 5 .40000014e-02 8 .00000000e+00 6 .50000000e+01 9 .93399978e-01 3 .90000010e+00 5 .60000002e-01 1 .31000004e+01 ] [ 0 ,0,0,0.0,0,0,1,0,0,0 ]","title":"\u30ef\u30a4\u30f3\u8a55\u4fa1"},{"location":"model_logstic/tensorflow_logistic_wine02/","text":"\u30ef\u30a4\u30f3\u8a55\u4fa1 Coding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # coding:utf-8 import numpy as np import tensorflow as tf import pandas as pd # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./wine.csv\" , delimiter = ';' , dtype = np . float32 , filling_values = ( 0 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # Wine\u306e\u30c7\u30fc\u30bf datas = dataset [:, 0 : 11 ] # Wine\u306e\u30e9\u30d9\u30eb(\u54c1\u8cea) labels = dataset [:, 11 ] N = len ( labels ) vector_labels = np . zeros (( N , 10 )) for i in xrange ( N ): vector_labels [ i ][ int ( labels [ i ])] = 1.0 # \u30c7\u30fc\u30bf\u30927:1\u3067\u5206\u5272\u3059\u308b test_data_size = len ( dataset ) // 8 # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_datas = datas [ train_data_size :] test_labels = vector_labels [ train_data_size :] . reshape ( test_data_size , 10 ) # Graph\u306eReset tf . reset_default_graph () # Data\u4fdd\u5b58\u5148 LOGDIR = './data' def single_layer ( X , num_in , num_out ): \"\"\"\u96a0\u308c\u5c64\"\"\" W = tf . Variable ( tf . truncated_normal ([ num_in , num_out ])) b = tf . Variable ( tf . zeros ([ num_out ])) f = tf . matmul ( X , W ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer , num_in , num_out ): \"\"\"\u51fa\u529b\u5c64\"\"\" W = tf . Variable ( tf . zeros ([ num_in , num_out ])) b = tf . Variable ( tf . zeros ([ num_out ])) f = tf . matmul ( layer , W ) + b p = tf . nn . softmax ( f ) return p with tf . Graph () . as_default (): # \u5909\u6570\u306e\u5b9a\u7fa9 X = tf . placeholder ( tf . float32 , shape = ( None , 11 ), name = \"input\" ) # Nx11\u884c\u5217 y_ = tf . placeholder ( tf . float32 , shape = ( None , 10 ), name = \"output\" ) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb with tf . name_scope ( 'forward' ): hidden_layer = single_layer ( X , 11 , 20 ) y = output_layer ( hidden_layer , 20 , 10 ) # \u640d\u5931\u95a2\u6570 with tf . name_scope ( 'cost' ): cost = - tf . reduce_sum ( y_ * tf . log ( y )) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u3057\u3066Adagrad\u3092\u63a1\u7528 train_op = tf . train . AdagradOptimizer ( 0.01 ) . minimize ( cost ) # \u4e88\u6e2c with tf . name_scope ( 'accuracy' ): # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( y , 1 ), tf . argmax ( y_ , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy_op = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) # Session sess = tf . Session () # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 20000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in xrange ( training_step ): sess . run ( train_op , feed_dict = { X : test_datas , y_ : test_labels }) if step % validation_step == 0 : accuracy_output , cost_output = sess . run ([ accuracy_op , cost ], feed_dict = { X : test_datas , y_ : test_labels }) print \"step %d , cost %f , accuracy %f \" % ( step , cost_output , accuracy_output ) summary_writer . flush ()","title":"\u30ef\u30a4\u30f3\u8a55\u4fa1"},{"location":"model_logstic/tensorflow_logistic_wine02/#_1","text":"","title":"\u30ef\u30a4\u30f3\u8a55\u4fa1"},{"location":"model_logstic/tensorflow_logistic_wine02/#coding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # coding:utf-8 import numpy as np import tensorflow as tf import pandas as pd # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3080 # \u306a\u304a\u6b20\u640d\u5024\u306f0\u3068\u3057\u305f dataset = np . genfromtxt ( \"./wine.csv\" , delimiter = ';' , dtype = np . float32 , filling_values = ( 0 )) # \u91cd\u8907\u3057\u305f\u30c7\u30fc\u30bf\u3092\u7701\u304f _ , index = np . unique ( dataset [:, 0 ], return_index = True ) dataset = dataset [ index ] # Wine\u306e\u30c7\u30fc\u30bf datas = dataset [:, 0 : 11 ] # Wine\u306e\u30e9\u30d9\u30eb(\u54c1\u8cea) labels = dataset [:, 11 ] N = len ( labels ) vector_labels = np . zeros (( N , 10 )) for i in xrange ( N ): vector_labels [ i ][ int ( labels [ i ])] = 1.0 # \u30c7\u30fc\u30bf\u30927:1\u3067\u5206\u5272\u3059\u308b test_data_size = len ( dataset ) // 8 # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_datas = datas [ train_data_size :] test_labels = vector_labels [ train_data_size :] . reshape ( test_data_size , 10 ) # Graph\u306eReset tf . reset_default_graph () # Data\u4fdd\u5b58\u5148 LOGDIR = './data' def single_layer ( X , num_in , num_out ): \"\"\"\u96a0\u308c\u5c64\"\"\" W = tf . Variable ( tf . truncated_normal ([ num_in , num_out ])) b = tf . Variable ( tf . zeros ([ num_out ])) f = tf . matmul ( X , W ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer , num_in , num_out ): \"\"\"\u51fa\u529b\u5c64\"\"\" W = tf . Variable ( tf . zeros ([ num_in , num_out ])) b = tf . Variable ( tf . zeros ([ num_out ])) f = tf . matmul ( layer , W ) + b p = tf . nn . softmax ( f ) return p with tf . Graph () . as_default (): # \u5909\u6570\u306e\u5b9a\u7fa9 X = tf . placeholder ( tf . float32 , shape = ( None , 11 ), name = \"input\" ) # Nx11\u884c\u5217 y_ = tf . placeholder ( tf . float32 , shape = ( None , 10 ), name = \"output\" ) # \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u30e2\u30c7\u30eb with tf . name_scope ( 'forward' ): hidden_layer = single_layer ( X , 11 , 20 ) y = output_layer ( hidden_layer , 20 , 10 ) # \u640d\u5931\u95a2\u6570 with tf . name_scope ( 'cost' ): cost = - tf . reduce_sum ( y_ * tf . log ( y )) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u3057\u3066Adagrad\u3092\u63a1\u7528 train_op = tf . train . AdagradOptimizer ( 0.01 ) . minimize ( cost ) # \u4e88\u6e2c with tf . name_scope ( 'accuracy' ): # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( y , 1 ), tf . argmax ( y_ , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy_op = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) # Session sess = tf . Session () # Summary summary_writer = tf . summary . FileWriter ( LOGDIR , sess . graph ) # \u5909\u6570\u306e\u521d\u671f\u5316 init_op = tf . global_variables_initializer () sess . run ( init_op ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u56de\u6570 training_step = 20000 validation_step = 100 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 for step in xrange ( training_step ): sess . run ( train_op , feed_dict = { X : test_datas , y_ : test_labels }) if step % validation_step == 0 : accuracy_output , cost_output = sess . run ([ accuracy_op , cost ], feed_dict = { X : test_datas , y_ : test_labels }) print \"step %d , cost %f , accuracy %f \" % ( step , cost_output , accuracy_output ) summary_writer . flush ()","title":"Coding"},{"location":"model_logstic/tensorflow_three_classification_first/","text":"3\u30af\u30e9\u30b9\u5206\u985e \u6e96\u5099\u7de8 iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u30a2\u30e4\u30e1\u306e\u7279\u5fb4\u3068\u305d\u306e\u30a2\u30e4\u30e1\u306e\u7a2e\u985e\u3092\u307e\u3068\u3081\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3042\u308b\u3002 iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 1 $ curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 \"\"\" iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e \"\"\" import numpy as np import tensorflow as tf # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f # \u5404\u5217\u306e\u5024\u306e\u578b\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1 , 0 , 0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0 , 1 , 0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0 , 0 , 1 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5f62\u5f0f print labels . shape print data . shape # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] print train_labels . shape print train_data . shape # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] print test_labels . shape print test_data . shape \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 (150, 3) (150, 4) (120, 3) (120, 4) (30, 3) (30, 4) \u53c2\u8003 Iris Data Set","title":"3\u30af\u30e9\u30b9\u5206\u985e \u6e96\u5099\u7de8"},{"location":"model_logstic/tensorflow_three_classification_first/#3","text":"iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u30a2\u30e4\u30e1\u306e\u7279\u5fb4\u3068\u305d\u306e\u30a2\u30e4\u30e1\u306e\u7a2e\u985e\u3092\u307e\u3068\u3081\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3042\u308b\u3002 iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 1 $ curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 \"\"\" iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e \"\"\" import numpy as np import tensorflow as tf # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f # \u5404\u5217\u306e\u5024\u306e\u578b\u3092\u6307\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1 , 0 , 0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0 , 1 , 0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0 , 0 , 1 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5f62\u5f0f print labels . shape print data . shape # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] print train_labels . shape print train_data . shape # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] print test_labels . shape print test_data . shape \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 (150, 3) (150, 4) (120, 3) (120, 4) (30, 3) (30, 4)","title":"3\u30af\u30e9\u30b9\u5206\u985e \u6e96\u5099\u7de8"},{"location":"model_logstic/tensorflow_three_classification_first/#_1","text":"Iris Data Set","title":"\u53c2\u8003"},{"location":"model_logstic/tensorflow_three_classification_last/","text":"3\u30af\u30e9\u30b9\u5206\u985e iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 \"\"\" iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e \"\"\" import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.001 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : test_data , t : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : \u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7cbe\u5ea6\u306f\u5411\u4e0a\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002\u3057\u304b\u3057\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7cbe\u5ea6\u304b\u3089\u30e2\u30c7\u30eb\u306e\u6c4e\u5316\u80fd\u529b\u306f\u5411\u4e0a\u3057\u3066\u3044\u306a\u3044\u3068\u8003\u3048\u3089\u308c\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Step : 200 [ Train ] cost : 0.102086 , acc : 0.858333 [ Test ] cost : 0.055979 , acc : 0.933333 Step : 400 [ Train ] cost : 0.056832 , acc : 0.966667 [ Test ] cost : 0.063199 , acc : 0.933333 Step : 600 [ Train ] cost : 0.048237 , acc : 0.966667 [ Test ] cost : 0.055863 , acc : 0.933333 Step : 800 [ Train ] cost : 0.043067 , acc : 0.975000 [ Test ] cost : 0.051634 , acc : 0.933333 Step : 1000 [ Train ] cost : 0.039580 , acc : 0.975000 [ Test ] cost : 0.048939 , acc : 0.933333 Step : 1200 [ Train ] cost : 0.037053 , acc : 0.975000 [ Test ] cost : 0.047116 , acc : 0.933333 Step : 1400 [ Train ] cost : 0.035127 , acc : 0.975000 [ Test ] cost : 0.045837 , acc : 0.933333 Step : 1600 [ Train ] cost : 0.033606 , acc : 0.975000 [ Test ] cost : 0.044920 , acc : 0.933333 Step : 1800 [ Train ] cost : 0.032370 , acc : 0.975000 [ Test ] cost : 0.044256 , acc : 0.933333 Step : 2000 [ Train ] cost : 0.031345 , acc : 0.975000 [ Test ] cost : 0.043776 , acc : 0.933333","title":"3\u30af\u30e9\u30b9\u5206\u985e"},{"location":"model_logstic/tensorflow_three_classification_last/#3","text":"iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 \"\"\" iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e \"\"\" import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) def single_layer ( X ): \"\"\"\u96a0\u308c\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( X , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer ): \"\"\"\u51fa\u529b\u5c64\"\"\" node_num = 1024 w = tf . Variable ( tf . zeros ([ node_num , 3 ])) b = tf . Variable ( tf . zeros ([ 3 ])) f = tf . matmul ( layer , w ) + b p = tf . nn . softmax ( f ) return p # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X ) # \u51fa\u529b\u5c64 p = output_layer ( hidden_layer ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.001 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { X : test_data , t : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : \u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7cbe\u5ea6\u306f\u5411\u4e0a\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002\u3057\u304b\u3057\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u7cbe\u5ea6\u304b\u3089\u30e2\u30c7\u30eb\u306e\u6c4e\u5316\u80fd\u529b\u306f\u5411\u4e0a\u3057\u3066\u3044\u306a\u3044\u3068\u8003\u3048\u3089\u308c\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Step : 200 [ Train ] cost : 0.102086 , acc : 0.858333 [ Test ] cost : 0.055979 , acc : 0.933333 Step : 400 [ Train ] cost : 0.056832 , acc : 0.966667 [ Test ] cost : 0.063199 , acc : 0.933333 Step : 600 [ Train ] cost : 0.048237 , acc : 0.966667 [ Test ] cost : 0.055863 , acc : 0.933333 Step : 800 [ Train ] cost : 0.043067 , acc : 0.975000 [ Test ] cost : 0.051634 , acc : 0.933333 Step : 1000 [ Train ] cost : 0.039580 , acc : 0.975000 [ Test ] cost : 0.048939 , acc : 0.933333 Step : 1200 [ Train ] cost : 0.037053 , acc : 0.975000 [ Test ] cost : 0.047116 , acc : 0.933333 Step : 1400 [ Train ] cost : 0.035127 , acc : 0.975000 [ Test ] cost : 0.045837 , acc : 0.933333 Step : 1600 [ Train ] cost : 0.033606 , acc : 0.975000 [ Test ] cost : 0.044920 , acc : 0.933333 Step : 1800 [ Train ] cost : 0.032370 , acc : 0.975000 [ Test ] cost : 0.044256 , acc : 0.933333 Step : 2000 [ Train ] cost : 0.031345 , acc : 0.975000 [ Test ] cost : 0.043776 , acc : 0.933333","title":"3\u30af\u30e9\u30b9\u5206\u985e"},{"location":"model_logstic/tensorflow_three_classification_makepb/","text":"3\u30af\u30e9\u30b9\u5206\u985e PB\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210 iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u5b66\u7fd2\u30e2\u30c7\u30eb\u306epb\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # coding:utf-8 # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] g = tf . Graph () with g . as_default (): # Create the model x = tf . placeholder ( tf . float32 , shape = ( None , 4 )) W = tf . Variable ( tf . zeros ([ 4 , 3 ]), name = \"vaiable_W\" ) b = tf . Variable ( tf . zeros ([ 3 ]), name = \"variable_b\" ) y = tf . nn . softmax ( tf . matmul ( x , W ) + b ) # Define loss and optimizer y_ = tf . placeholder ( tf . float32 , shape = ( None , 3 )) cross_entropy = y_ * tf . log ( y ) loss = - tf . reduce_mean ( cross_entropy ) train_step = tf . train . GradientDescentOptimizer ( 0.001 ) . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 correct_pred = tf . equal ( tf . argmax ( y , 1 ), tf . argmax ( y_ , 1 )) accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) sess = tf . Session () # Train init = tf . global_variables_initializer () sess . run ( init ) for i in range ( 2000 ): sess . run ( train_step , feed_dict = { x : train_data , y_ : train_labels }) if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { x : train_data , y_ : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { x : test_data , y_ : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) print ( accuracy . eval ({ x : train_data , y_ : train_labels }, sess )) # Store variable _W = W . eval ( sess ) _b = b . eval ( sess ) sess . close () g_2 = tf . Graph () with g_2 . as_default (): x_2 = tf . placeholder ( tf . float32 , shape = ( None , 4 ), name = \"input\" ) W_2 = tf . constant ( _W , name = \"constant_W\" ) b_2 = tf . constant ( _b , name = \"constant_b\" ) y_2 = tf . nn . softmax ( tf . matmul ( x_2 , W_2 ) + b_2 , name = \"output\" ) sess_2 = tf . Session () init_2 = tf . global_variables_initializer () sess_2 . run ( init_2 ) graph_def = g_2 . as_graph_def () tf . train . write_graph ( graph_def , './tmp/iris-practice' , 'iris-graph.pb' , as_text = False ) \u5b66\u7fd2\u3067\u5f97\u305f\u91cd\u307f\u3068\u30d0\u30a4\u30a2\u30b9\u306e\u5024\u3092\u30b0\u30e9\u30d5\u306b\u66f8\u304d\u8fbc\u3080\u305f\u3081\u3001\u5b66\u7fd2\u3057\u305f\u5f8c\u306b\u518d\u5ea6\u30b0\u30e9\u30d5\u3092\u751f\u6210\u3057\u3066\u3044\u308b \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Step : 0 [ Train ] cost : 0.366078 , acc : 0.366667 [ Test ] cost : 0.366079 , acc : 0.200000 Step : 200 [ Train ] cost : 0.348545 , acc : 0.366667 [ Test ] cost : 0.348238 , acc : 0.200000 Step : 400 [ Train ] cost : 0.335609 , acc : 0.391667 [ Test ] cost : 0.334272 , acc : 0.200000 Step : 600 [ Train ] cost : 0.323969 , acc : 0.733333 [ Test ] cost : 0.321557 , acc : 0.600000 Step : 800 [ Train ] cost : 0.313308 , acc : 0.866667 [ Test ] cost : 0.309940 , acc : 0.833333 Step : 1000 [ Train ] cost : 0.303530 , acc : 0.916667 [ Test ] cost : 0.299337 , acc : 0.933333 Step : 1200 [ Train ] cost : 0.294558 , acc : 0.975000 [ Test ] cost : 0.289659 , acc : 0.933333 Step : 1400 [ Train ] cost : 0.286318 , acc : 0.983333 [ Test ] cost : 0.280818 , acc : 0.933333 Step : 1600 [ Train ] cost : 0.278742 , acc : 0.975000 [ Test ] cost : 0.272732 , acc : 1.000000 Step : 1800 [ Train ] cost : 0.271767 , acc : 0.975000 [ Test ] cost : 0.265324 , acc : 1.000000 0.966667","title":"3\u30af\u30e9\u30b9\u5206\u985e PB\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210"},{"location":"model_logstic/tensorflow_three_classification_makepb/#3-pb","text":"iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u5b66\u7fd2\u30e2\u30c7\u30eb\u306epb\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # coding:utf-8 # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] g = tf . Graph () with g . as_default (): # Create the model x = tf . placeholder ( tf . float32 , shape = ( None , 4 )) W = tf . Variable ( tf . zeros ([ 4 , 3 ]), name = \"vaiable_W\" ) b = tf . Variable ( tf . zeros ([ 3 ]), name = \"variable_b\" ) y = tf . nn . softmax ( tf . matmul ( x , W ) + b ) # Define loss and optimizer y_ = tf . placeholder ( tf . float32 , shape = ( None , 3 )) cross_entropy = y_ * tf . log ( y ) loss = - tf . reduce_mean ( cross_entropy ) train_step = tf . train . GradientDescentOptimizer ( 0.001 ) . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 correct_pred = tf . equal ( tf . argmax ( y , 1 ), tf . argmax ( y_ , 1 )) accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) sess = tf . Session () # Train init = tf . global_variables_initializer () sess . run ( init ) for i in range ( 2000 ): sess . run ( train_step , feed_dict = { x : train_data , y_ : train_labels }) if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_loss , train_acc = sess . run ([ loss , accuracy ], feed_dict = { x : train_data , y_ : train_labels }) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u8a55\u4fa1 test_loss , test_acc = sess . run ([ loss , accuracy ], feed_dict = { x : test_data , y_ : test_labels }) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) print \"[Test] cost: %f , acc: %f \" % ( test_loss , test_acc ) print ( accuracy . eval ({ x : train_data , y_ : train_labels }, sess )) # Store variable _W = W . eval ( sess ) _b = b . eval ( sess ) sess . close () g_2 = tf . Graph () with g_2 . as_default (): x_2 = tf . placeholder ( tf . float32 , shape = ( None , 4 ), name = \"input\" ) W_2 = tf . constant ( _W , name = \"constant_W\" ) b_2 = tf . constant ( _b , name = \"constant_b\" ) y_2 = tf . nn . softmax ( tf . matmul ( x_2 , W_2 ) + b_2 , name = \"output\" ) sess_2 = tf . Session () init_2 = tf . global_variables_initializer () sess_2 . run ( init_2 ) graph_def = g_2 . as_graph_def () tf . train . write_graph ( graph_def , './tmp/iris-practice' , 'iris-graph.pb' , as_text = False ) \u5b66\u7fd2\u3067\u5f97\u305f\u91cd\u307f\u3068\u30d0\u30a4\u30a2\u30b9\u306e\u5024\u3092\u30b0\u30e9\u30d5\u306b\u66f8\u304d\u8fbc\u3080\u305f\u3081\u3001\u5b66\u7fd2\u3057\u305f\u5f8c\u306b\u518d\u5ea6\u30b0\u30e9\u30d5\u3092\u751f\u6210\u3057\u3066\u3044\u308b \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Step : 0 [ Train ] cost : 0.366078 , acc : 0.366667 [ Test ] cost : 0.366079 , acc : 0.200000 Step : 200 [ Train ] cost : 0.348545 , acc : 0.366667 [ Test ] cost : 0.348238 , acc : 0.200000 Step : 400 [ Train ] cost : 0.335609 , acc : 0.391667 [ Test ] cost : 0.334272 , acc : 0.200000 Step : 600 [ Train ] cost : 0.323969 , acc : 0.733333 [ Test ] cost : 0.321557 , acc : 0.600000 Step : 800 [ Train ] cost : 0.313308 , acc : 0.866667 [ Test ] cost : 0.309940 , acc : 0.833333 Step : 1000 [ Train ] cost : 0.303530 , acc : 0.916667 [ Test ] cost : 0.299337 , acc : 0.933333 Step : 1200 [ Train ] cost : 0.294558 , acc : 0.975000 [ Test ] cost : 0.289659 , acc : 0.933333 Step : 1400 [ Train ] cost : 0.286318 , acc : 0.983333 [ Test ] cost : 0.280818 , acc : 0.933333 Step : 1600 [ Train ] cost : 0.278742 , acc : 0.975000 [ Test ] cost : 0.272732 , acc : 1.000000 Step : 1800 [ Train ] cost : 0.271767 , acc : 0.975000 [ Test ] cost : 0.265324 , acc : 1.000000 0.966667","title":"3\u30af\u30e9\u30b9\u5206\u985e PB\u30d5\u30a1\u30a4\u30eb\u4f5c\u6210"},{"location":"model_logstic/tensorflow_three_classification_tensorboard/","text":"3\u30af\u30e9\u30b9\u5206\u985e Tensorboard\u7de8 Tensorboard\u306b\u3088\u308a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u69d8\u5b50\u3092\u53ef\u8996\u5316\u3059\u308b\u3002 \u5148\u306bIRIS\u30c7\u30fc\u30bf\u4f5c\u6210\u3092\u5b9f\u884c\u3057\u3001bazdekIris.data\u3092\u4f5c\u6210\u3057\u3066\u304a\u304f\u3053\u3068\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 \"\"\" iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e \"\"\" import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) # \u96a0\u308c\u5c64\u306e\u30ce\u30fc\u30c9\u6570 node_num = 1024 w_hidden = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b_hidden = tf . Variable ( tf . zeros ([ node_num ])) f_hidden = tf . matmul ( X , w_hidden ) + b_hidden hidden_layer = tf . nn . relu ( f_hidden ) # \u51fa\u529b\u5c64 w_output = tf . Variable ( tf . zeros ([ node_num , 3 ])) b_output = tf . Variable ( tf . zeros ([ 3 ])) f_output = tf . matmul ( hidden_layer , w_output ) + b_output p = tf . nn . softmax ( f_output ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.001 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \"Hidden_layer_wights\" , w_hidden ) tf . summary . histogram ( \"Hidden_layer_biases\" , b_hidden ) tf . summary . histogram ( \"Output_layer_wights\" , w_output ) tf . summary . histogram ( \"Output_layer_wights\" , b_output ) tf . summary . scalar ( \"Accuracy\" , accuracy ) tf . summary . scalar ( \"Loss\" , loss ) summary = tf . summary . merge_all () writer = tf . summary . FileWriter ( \"./iris_cassification_log\" , sess . graph ) sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_summary , train_loss , train_acc = sess . run ([ summary , loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) writer . add_summary ( train_summary , i ) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) \u5b9f\u884c\u7d50\u679c : 1 $ tensorboard --logdir = ./iris_cassification_log \u8aa4\u5dee\u3068\u7cbe\u5ea6\u306e\u63a8\u79fb : \u91cd\u307f\u3068\u30d0\u30a4\u30a2\u30b9\u306e\u5206\u5e03 :","title":"3\u30af\u30e9\u30b9\u5206\u985e Tensorboard\u7de8"},{"location":"model_logstic/tensorflow_three_classification_tensorboard/#3-tensorboard","text":"Tensorboard\u306b\u3088\u308a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u69d8\u5b50\u3092\u53ef\u8996\u5316\u3059\u308b\u3002 \u5148\u306bIRIS\u30c7\u30fc\u30bf\u4f5c\u6210\u3092\u5b9f\u884c\u3057\u3001bazdekIris.data\u3092\u4f5c\u6210\u3057\u3066\u304a\u304f\u3053\u3068\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 #!/usr/bin/env python # -*- coding: utf-8 -*- # TensorFlow r1.0.0 # Python 2.7.6 \"\"\" iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e \"\"\" import numpy as np import tensorflow as tf ### \u30c7\u30fc\u30bf\u306e\u6e96\u5099 # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f dataset = np . genfromtxt ( \"./bezdekIris.data\" , delimiter = ',' , dtype = [ float , float , float , float , \"S32\" ]) # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u9806\u5e8f\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u4e26\u3079\u66ff\u3048\u308b np . random . shuffle ( dataset ) def get_labels ( dataset ): \"\"\"\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u30921ofK\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\"\"\" raw_labels = [ item [ 4 ] for item in dataset ] labels = [] for l in raw_labels : if l == \"Iris-setosa\" : labels . append ([ 1.0 , 0.0 , 0.0 ]) elif l == \"Iris-versicolor\" : labels . append ([ 0.0 , 1.0 , 0.0 ]) elif l == \"Iris-virginica\" : labels . append ([ 0.0 , 0.0 , 1.0 ]) return np . array ( labels ) def get_data ( dataset ): \"\"\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092nparray\u306b\u5909\u63db\u3059\u308b\"\"\" raw_data = [ list ( item )[: 4 ] for item in dataset ] return np . array ( raw_data ) # \u30e9\u30d9\u30eb labels = get_labels ( dataset ) # \u30c7\u30fc\u30bf data = get_data ( dataset ) # \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3059\u308b # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf train_labels = labels [: 120 ] train_data = data [: 120 ] # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf test_labels = labels [ 120 :] test_data = data [ 120 :] ### \u30e2\u30c7\u30eb\u3092Tensor\u5f62\u5f0f\u3067\u5b9f\u88c5 # \u30e9\u30d9\u30eb\u3092\u683c\u7d0d\u3059\u308bPlaceholder t = tf . placeholder ( tf . float32 , shape = ( None , 3 )) # \u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308bPlaceholder X = tf . placeholder ( tf . float32 , shape = ( None , 4 )) # \u96a0\u308c\u5c64\u306e\u30ce\u30fc\u30c9\u6570 node_num = 1024 w_hidden = tf . Variable ( tf . truncated_normal ([ 4 , node_num ])) b_hidden = tf . Variable ( tf . zeros ([ node_num ])) f_hidden = tf . matmul ( X , w_hidden ) + b_hidden hidden_layer = tf . nn . relu ( f_hidden ) # \u51fa\u529b\u5c64 w_output = tf . Variable ( tf . zeros ([ node_num , 3 ])) b_output = tf . Variable ( tf . zeros ([ 3 ])) f_output = tf . matmul ( hidden_layer , w_output ) + b_output p = tf . nn . softmax ( f_output ) # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc cross_entropy = t * tf . log ( p ) # \u8aa4\u5dee\u95a2\u6570 loss = - tf . reduce_mean ( cross_entropy ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u73870.001 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.001 ) train_step = optimizer . minimize ( loss ) # \u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3068\u6b63\u89e3\u304c\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u8abf\u3079\u308b correct_pred = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( t , 1 )) # \u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_pred , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c with tf . Session () as sess : # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \"Hidden_layer_wights\" , w_hidden ) tf . summary . histogram ( \"Hidden_layer_biases\" , b_hidden ) tf . summary . histogram ( \"Output_layer_wights\" , w_output ) tf . summary . histogram ( \"Output_layer_wights\" , b_output ) tf . summary . scalar ( \"Accuracy\" , accuracy ) tf . summary . scalar ( \"Loss\" , loss ) summary = tf . summary . merge_all () writer = tf . summary . FileWriter ( \"./iris_cassification_log\" , sess . graph ) sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 sess . run ( train_step , feed_dict = { X : train_data , t : train_labels }) # 200\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306b\u7cbe\u5ea6\u3092\u51fa\u529b if i % 200 == 0 : # \u30b3\u30b9\u30c8\u3068\u7cbe\u5ea6\u3092\u51fa\u529b train_summary , train_loss , train_acc = sess . run ([ summary , loss , accuracy ], feed_dict = { X : train_data , t : train_labels }) writer . add_summary ( train_summary , i ) print \"Step: %d \" % i print \"[Train] cost: %f , acc: %f \" % ( train_loss , train_acc ) \u5b9f\u884c\u7d50\u679c : 1 $ tensorboard --logdir = ./iris_cassification_log \u8aa4\u5dee\u3068\u7cbe\u5ea6\u306e\u63a8\u79fb : \u91cd\u307f\u3068\u30d0\u30a4\u30a2\u30b9\u306e\u5206\u5e03 :","title":"3\u30af\u30e9\u30b9\u5206\u985e Tensorboard\u7de8"},{"location":"model_logstic/tensorflow_three_classification_testpb/","text":"3\u30af\u30e9\u30b9\u5206\u985e PB\u30d5\u30a1\u30a4\u30eb\u30c6\u30b9\u30c8\u7de8 iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u5b66\u7fd2\u30e2\u30c7\u30eb\u306epb\u30d5\u30a1\u30a4\u30eb\u306e\u7cbe\u5ea6\u306e\u78ba\u8a8d\u3092\u884c\u3046 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # coding:utf-8 # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf from tensorflow.python.platform import gfile #pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 with tf . gfile . FastGFile ( \"./tmp/iris-practice/iris-graph.pb\" , 'rb' ) as f : graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) _ = tf . import_graph_def ( graph_def , name = '' ) #\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u30c6\u30b9\u30c8 sess = tf . Session () # Iris-virginica\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf input_data = [[ 7.7 , 3.8 , 6.7 , 2.2 ]] #\u30b0\u30e9\u30d5\u306b\u3064\u3051\u305fname=\"output\"\u3068name=\"input\"\u306b\u5165\u529b\u30c7\u30fc\u30bf\u3068\u51fa\u529b\u7d50\u679c\u3092\u4ee3\u5165\u3059\u308b predictions = sess . run ( 'output:0' ,{ 'input:0' : np . array ( input_data )}) print predictions Index = np . argmax ( predictions ) if ( Index == 0 ): print \"answer : Iris-setosa\" elif ( Index == 1 ): print \"answer : Iris-versicolor\" elif ( Index == 2 ): print \"answer : Iris-virginica\" \u5b9f\u884c\u7d50\u679c : 1 2 [[ 0.11481573 0.42691165 0.45827267]] answer : Iris-virginica Python 1 2 3 4 5 6 7 Index = np . argmax ( predictions ) if ( Index == 0 ): print \"answer : Iris-setosa\" elif ( Index == 1 ): print \"answer : Iris-versicolor\" elif ( Index == 2 ): print \"answer : Iris-virginica\" predictions\u306b\u306f\u5b9f\u884c\u7d50\u679c\u306e\u901a\u308a\u3001\u3069\u306e\u30af\u30e9\u30b9\u304c\u5c24\u3082\u3089\u3057\u3044\u304b\u306e\u5024\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u3002 \u305d\u306e\u5024\u304c\u4e00\u756a\u5927\u304d\u3044\u914d\u5217\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u5165\u529b\u30c7\u30fc\u30bf\u306e\u7b54\u3048\u3068\u306a\u308b","title":"3\u30af\u30e9\u30b9\u5206\u985e PB\u30d5\u30a1\u30a4\u30eb\u30c6\u30b9\u30c8\u7de8"},{"location":"model_logstic/tensorflow_three_classification_testpb/#3-pb","text":"iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f3\u30af\u30e9\u30b9\u5206\u985e\u306e\u5b66\u7fd2\u30e2\u30c7\u30eb\u306epb\u30d5\u30a1\u30a4\u30eb\u306e\u7cbe\u5ea6\u306e\u78ba\u8a8d\u3092\u884c\u3046 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # coding:utf-8 # TensorFlow r1.0.0 # Python 2.7.6 import numpy as np import tensorflow as tf from tensorflow.python.platform import gfile #pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 with tf . gfile . FastGFile ( \"./tmp/iris-practice/iris-graph.pb\" , 'rb' ) as f : graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) _ = tf . import_graph_def ( graph_def , name = '' ) #\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u30c6\u30b9\u30c8 sess = tf . Session () # Iris-virginica\u306e\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf input_data = [[ 7.7 , 3.8 , 6.7 , 2.2 ]] #\u30b0\u30e9\u30d5\u306b\u3064\u3051\u305fname=\"output\"\u3068name=\"input\"\u306b\u5165\u529b\u30c7\u30fc\u30bf\u3068\u51fa\u529b\u7d50\u679c\u3092\u4ee3\u5165\u3059\u308b predictions = sess . run ( 'output:0' ,{ 'input:0' : np . array ( input_data )}) print predictions Index = np . argmax ( predictions ) if ( Index == 0 ): print \"answer : Iris-setosa\" elif ( Index == 1 ): print \"answer : Iris-versicolor\" elif ( Index == 2 ): print \"answer : Iris-virginica\" \u5b9f\u884c\u7d50\u679c : 1 2 [[ 0.11481573 0.42691165 0.45827267]] answer : Iris-virginica Python 1 2 3 4 5 6 7 Index = np . argmax ( predictions ) if ( Index == 0 ): print \"answer : Iris-setosa\" elif ( Index == 1 ): print \"answer : Iris-versicolor\" elif ( Index == 2 ): print \"answer : Iris-virginica\" predictions\u306b\u306f\u5b9f\u884c\u7d50\u679c\u306e\u901a\u308a\u3001\u3069\u306e\u30af\u30e9\u30b9\u304c\u5c24\u3082\u3089\u3057\u3044\u304b\u306e\u5024\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u3002 \u305d\u306e\u5024\u304c\u4e00\u756a\u5927\u304d\u3044\u914d\u5217\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u5165\u529b\u30c7\u30fc\u30bf\u306e\u7b54\u3048\u3068\u306a\u308b","title":"3\u30af\u30e9\u30b9\u5206\u985e PB\u30d5\u30a1\u30a4\u30eb\u30c6\u30b9\u30c8\u7de8"},{"location":"model_mnist/Learning_myDataset/","text":"\u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u8a8d\u8b58\u3092\u884c\u3046\u3000\u305d\u306e2 \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0 \u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5b66\u7fd2\u3092\u884c\u3046\u3002 \u3053\u3053\u3067\u306f\u7573\u307f\u8fbc\u307f\u3067\u306f\u306a\u304f\u3001\u5168\u7d50\u5408\u306e\u307f\u3067\u5b66\u7fd2\u3092\u884c\u3063\u305f\u3002 \u8aad\u307f\u8fbc\u307f\u306eSample\u30b3\u30fc\u30c9\u3092 load_dataset.py \u3067\u4fdd\u5b58\u3057\u3001\u4f7f\u7528\u3057\u3066\u3044\u308b\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np import tensorflow as tf from load_dataset import load_dataset import matplotlib.pyplot as plt x_train , t_train = load_dataset ( './train_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) x_test , t_test = load_dataset ( './test_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) #\u30c7\u30fc\u30bf\u306e\u30b7\u30e3\u30c3\u30d5\u30eb def list_shuffle ( datas , labels ): index_list = np . arange ( 0 , datas . shape [ 0 ]) np . random . shuffle ( index_list ) x_data = datas [ index_list ] t_data = labels [ index_list ] return x_data , t_data x_train_shuffle , t_train_shuffle = list_shuffle ( x_train , t_train ) x_test_shuffle , t_test_shuffle = list_shuffle ( x_test , t_test ) # \u5b66\u7fd2 # \u5165\u529b\u5c64 # \u4efb\u610f\u306e\u7528\u610f\u3057\u305f\u753b\u50cf\u30b5\u30a4\u30ba,\u5206\u985e\u306e\u30af\u30e9\u30b9\u6570\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 x = tf . placeholder ( tf . float32 , shape = [ None , 64 * 64 * 3 ]) y_ = tf . placeholder ( tf . float32 , shape = [ None , 2 ]) W = tf . Variable ( tf . zeros ([ 64 * 64 * 3 , 2 ])) b = tf . Variable ( tf . zeros ([ 2 ])) # \u51fa\u529b\u5c64 y = tf . matmul ( x , W ) + b p = tf . nn . softmax ( y ) # \u640d\u5931\u95a2\u6570 cross_entropy = y_ * tf . log ( p ) loss = - tf . reduce_mean ( cross_entropy ) # \u52fe\u914d optimizer = tf . train . GradientDescentOptimizer ( 0.01 ) train_step = optimizer . minimize ( loss ) correct_prediction = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( y_ , 1 )) accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) sess = tf . Session () sess . run ( tf . global_variables_initializer ()) for i in range ( 5000 ): sess . run ( train_step , feed_dict = { x : x_train_shuffle , y_ : t_train_shuffle }) if i % 100 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { x : x_train_shuffle , y_ : t_train_shuffle }) test_acc = sess . run ( accuracy , feed_dict = { x : x_test_shuffle , y_ : t_test_shuffle }) print \"[Train] step: %d , loss: %f , acc: %f , [Test] acc : %f \" % ( i , train_loss , train_acc , test_acc ) \u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u307f\u308b \u3053\u3053\u3067\u306f\u30af\u30e9\u30b9\u5206\u985e\u3092\u9053\u8def\u6a19\u8b58\u306e\u6b62\u307e\u308c\u3068\u5236\u9650\u901f\u5ea6\u306e\u6a19\u8b58\u306e2\u30af\u30e9\u30b9\u5206\u985e\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 from PIL import Image im = Image . open ( \"test.jpg\" , \"r\" ) plt . imshow ( np . array ( im )) img = np . frombuffer ( np . array ( Image . open ( 'test.jpg' ) . convert ( 'RGB' )), dtype = np . uint8 ) img = img . astype ( np . float32 ) # \u6b63\u898f\u5316 img /= 255.0 predict_img = np . array ([ img ]) ans = np . argmax ( sess . run ( p , feed_dict = { x : predict_img })) if ( ans == 0 ): print ( 'Mark is Stop' ) elif ( ans == 1 ): print ( 'Mark is limitspeed' ) \u7d50\u679c","title":"\u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u8a8d\u8b58\u3092\u884c\u3046\u3000\u305d\u306e2"},{"location":"model_mnist/Learning_myDataset/#mnist-2","text":"","title":"\u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u8a8d\u8b58\u3092\u884c\u3046\u3000\u305d\u306e2"},{"location":"model_mnist/Learning_myDataset/#_1","text":"\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5b66\u7fd2\u3092\u884c\u3046\u3002 \u3053\u3053\u3067\u306f\u7573\u307f\u8fbc\u307f\u3067\u306f\u306a\u304f\u3001\u5168\u7d50\u5408\u306e\u307f\u3067\u5b66\u7fd2\u3092\u884c\u3063\u305f\u3002 \u8aad\u307f\u8fbc\u307f\u306eSample\u30b3\u30fc\u30c9\u3092 load_dataset.py \u3067\u4fdd\u5b58\u3057\u3001\u4f7f\u7528\u3057\u3066\u3044\u308b\u3002 Sample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import numpy as np import tensorflow as tf from load_dataset import load_dataset import matplotlib.pyplot as plt x_train , t_train = load_dataset ( './train_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) x_test , t_test = load_dataset ( './test_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) #\u30c7\u30fc\u30bf\u306e\u30b7\u30e3\u30c3\u30d5\u30eb def list_shuffle ( datas , labels ): index_list = np . arange ( 0 , datas . shape [ 0 ]) np . random . shuffle ( index_list ) x_data = datas [ index_list ] t_data = labels [ index_list ] return x_data , t_data x_train_shuffle , t_train_shuffle = list_shuffle ( x_train , t_train ) x_test_shuffle , t_test_shuffle = list_shuffle ( x_test , t_test ) # \u5b66\u7fd2 # \u5165\u529b\u5c64 # \u4efb\u610f\u306e\u7528\u610f\u3057\u305f\u753b\u50cf\u30b5\u30a4\u30ba,\u5206\u985e\u306e\u30af\u30e9\u30b9\u6570\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 x = tf . placeholder ( tf . float32 , shape = [ None , 64 * 64 * 3 ]) y_ = tf . placeholder ( tf . float32 , shape = [ None , 2 ]) W = tf . Variable ( tf . zeros ([ 64 * 64 * 3 , 2 ])) b = tf . Variable ( tf . zeros ([ 2 ])) # \u51fa\u529b\u5c64 y = tf . matmul ( x , W ) + b p = tf . nn . softmax ( y ) # \u640d\u5931\u95a2\u6570 cross_entropy = y_ * tf . log ( p ) loss = - tf . reduce_mean ( cross_entropy ) # \u52fe\u914d optimizer = tf . train . GradientDescentOptimizer ( 0.01 ) train_step = optimizer . minimize ( loss ) correct_prediction = tf . equal ( tf . argmax ( p , 1 ), tf . argmax ( y_ , 1 )) accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) sess = tf . Session () sess . run ( tf . global_variables_initializer ()) for i in range ( 5000 ): sess . run ( train_step , feed_dict = { x : x_train_shuffle , y_ : t_train_shuffle }) if i % 100 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { x : x_train_shuffle , y_ : t_train_shuffle }) test_acc = sess . run ( accuracy , feed_dict = { x : x_test_shuffle , y_ : t_test_shuffle }) print \"[Train] step: %d , loss: %f , acc: %f , [Test] acc : %f \" % ( i , train_loss , train_acc , test_acc ) \u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u5165\u529b\u3057\u3066\u307f\u308b \u3053\u3053\u3067\u306f\u30af\u30e9\u30b9\u5206\u985e\u3092\u9053\u8def\u6a19\u8b58\u306e\u6b62\u307e\u308c\u3068\u5236\u9650\u901f\u5ea6\u306e\u6a19\u8b58\u306e2\u30af\u30e9\u30b9\u5206\u985e\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 from PIL import Image im = Image . open ( \"test.jpg\" , \"r\" ) plt . imshow ( np . array ( im )) img = np . frombuffer ( np . array ( Image . open ( 'test.jpg' ) . convert ( 'RGB' )), dtype = np . uint8 ) img = img . astype ( np . float32 ) # \u6b63\u898f\u5316 img /= 255.0 predict_img = np . array ([ img ]) ans = np . argmax ( sess . run ( p , feed_dict = { x : predict_img })) if ( ans == 0 ): print ( 'Mark is Stop' ) elif ( ans == 1 ): print ( 'Mark is limitspeed' ) \u7d50\u679c","title":"\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0"},{"location":"model_mnist/createDataset/","text":"\u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u8a8d\u8b58\u3092\u884c\u3046\u3000\u305d\u306e1 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210 \u5b66\u7fd2\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306e\u753b\u50cf\u3092\u30cd\u30c3\u30c8\u4e0a\u304b\u3089\u96c6\u3081\u3066\u304f\u308b\u3001\u3082\u3057\u304f\u306f\u914d\u5e03\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u3002 \u7279\u5fb4\u306e\u5207\u308a\u629c\u304d MNIST\u306e\u3088\u3046\u306b\u753b\u50cf\u3092\u30ea\u30b5\u30a4\u30ba\u3001\u307e\u305f\u306f\u7279\u5fb4\u3092\u5207\u308a\u629c\u3044\u3066\u30ea\u30b5\u30a4\u30ba\u3092\u884c\u3046\u3002 \u4eca\u56de\u7279\u5fb4\u306e\u5207\u308a\u629c\u304d\u3068\u30ea\u30b5\u30a4\u30ba\u306b\u306f CattingImage \u3092\u4f7f\u7528\u3057\u305f\u3002 \u30ea\u30b5\u30a4\u30ba\u304c\u7d42\u308f\u3063\u305f\u3089\u3001\u5b66\u7fd2\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u753b\u50cf\u3092\u4fdd\u5b58\u3059\u308b\u3002 \u753b\u50cf\u306e\u8aad\u307f\u8fbc\u307f \u5b66\u7fd2\u306e\u305f\u3081\u306b\u3001\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080 \u8aad\u307f\u8fbc\u3080\u305f\u3081\u306eSample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # coding: utf-8 from PIL import Image import numpy as np import os # coding: utf-8 try : import urllib.request except ImportError : raise ImportError ( 'You should use Python 3.x' ) import os.path import gzip import pickle def _load_label ( file_dir ): data_files = os . listdir ( file_dir ) labels = [] for file in data_files : l = file . split ( '-' )[ 0 ] if ( l == 'stop' ): labels . append ( 0 ) elif ( l == 'limitspeed' ): labels . append ( 1 ) print ( \"Load label : Done!\" ) return np . array ( labels ) def _load_img ( file_dir , convert_type = 'L' ): data_files = os . listdir ( file_dir ) imgs = [] for file in data_files : img = np . frombuffer ( np . array ( Image . open ( file_dir + '/' + file ) . convert ( convert_type )), dtype = np . uint8 ) imgs . append ( img ) print ( \"Load img : Done!\" ) return np . array ( imgs ) def _change_one_hot_label ( X ): #2\u306e\u3068\u3053\u308d\u306f\u5206\u985e\u3057\u305f\u3044\u30af\u30e9\u30b9\u306e\u6570\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 T = np . zeros (( X . size , 2 )) for idx , row in enumerate ( T ): row [ X [ idx ]] = 1 return T def load_dataset ( DIR_PATH , convert_type = 'L' , normalize = True , flatten = True , one_hot_label = False ): labels = _load_label ( DIR_PATH ) imgs = _load_img ( DIR_PATH , convert_type ) if normalize : imgs = imgs . astype ( np . float32 ) imgs /= 255.0 if not flatten : #64\u306e\u3068\u3053\u308d\u306f\u7528\u610f\u3057\u305f\u753b\u50cf\u30b5\u30a4\u30ba\u306e\u5024\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 if ( convert_type == 'L' ): imgs = imgs . reshape ( - 1 , 1 , 64 , 64 ) elif ( convert_type == 'RGB' ): imgs = imgs . reshape ( - 1 , 3 , 64 , 64 ) if one_hot_label : labels = _change_one_hot_label ( labels ) return imgs , labels \u4f7f\u7528\u4f8b 1 2 x_train , t_train = load_dataset ( './train_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) x_test , t_test = load_dataset ( './test_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) \u8fd4\u308a\u5024\u306f2\u3064\u8fd4\u3063\u3066\u304f\u308b\u306e\u30672\u3064\u7528\u610f\u3059\u308b\u3053\u3068 \u5f15\u6570 \u610f\u5473 './train_dataset' \u5b66\u7fd2\u3001\u30c6\u30b9\u30c8\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u5b9a convert_type='RGB' \u5b66\u7fd2\u3092\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u3067\u3001\u3082\u3057\u304f\u306f\u30ab\u30e9\u30fc\u3067\u884c\u3046\u304b\u306e\u6307\u5b9a\u3002default\u3067\u306f\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306econvert_type='L' flatten=True \u5e73\u5766\u5316\u3092\u884c\u3046\u304b\u306e\u6307\u5b9a\u3002default\u3067\u306fTrue normalize=True \u6b63\u898f\u5316\u3092\u884c\u3046\u304b\u306e\u6307\u5b9a\u3002(\u30c7\u30fc\u30bf\u306e\u5024\u304c0.0~1.0\u306e\u7bc4\u56f2\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u3059\u308b) default\u3067\u306fTrue one_hot_label=True Onehot\u8868\u73fe\u306b\u3059\u308b\u304b\u306e\u6307\u5b9a\u3002default\u3067\u306fTrue","title":"\u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u8a8d\u8b58\u3092\u884c\u3046\u3000\u305d\u306e1"},{"location":"model_mnist/createDataset/#mnist-1","text":"","title":"\u4efb\u610f\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067MNIST\u306e\u3088\u3046\u306a\u753b\u50cf\u8a8d\u8b58\u3092\u884c\u3046\u3000\u305d\u306e1"},{"location":"model_mnist/createDataset/#_1","text":"\u5b66\u7fd2\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306e\u753b\u50cf\u3092\u30cd\u30c3\u30c8\u4e0a\u304b\u3089\u96c6\u3081\u3066\u304f\u308b\u3001\u3082\u3057\u304f\u306f\u914d\u5e03\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u3002","title":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210"},{"location":"model_mnist/createDataset/#_2","text":"MNIST\u306e\u3088\u3046\u306b\u753b\u50cf\u3092\u30ea\u30b5\u30a4\u30ba\u3001\u307e\u305f\u306f\u7279\u5fb4\u3092\u5207\u308a\u629c\u3044\u3066\u30ea\u30b5\u30a4\u30ba\u3092\u884c\u3046\u3002 \u4eca\u56de\u7279\u5fb4\u306e\u5207\u308a\u629c\u304d\u3068\u30ea\u30b5\u30a4\u30ba\u306b\u306f CattingImage \u3092\u4f7f\u7528\u3057\u305f\u3002 \u30ea\u30b5\u30a4\u30ba\u304c\u7d42\u308f\u3063\u305f\u3089\u3001\u5b66\u7fd2\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u753b\u50cf\u3092\u4fdd\u5b58\u3059\u308b\u3002","title":"\u7279\u5fb4\u306e\u5207\u308a\u629c\u304d"},{"location":"model_mnist/createDataset/#_3","text":"\u5b66\u7fd2\u306e\u305f\u3081\u306b\u3001\u7528\u610f\u3057\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080 \u8aad\u307f\u8fbc\u3080\u305f\u3081\u306eSample\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 # coding: utf-8 from PIL import Image import numpy as np import os # coding: utf-8 try : import urllib.request except ImportError : raise ImportError ( 'You should use Python 3.x' ) import os.path import gzip import pickle def _load_label ( file_dir ): data_files = os . listdir ( file_dir ) labels = [] for file in data_files : l = file . split ( '-' )[ 0 ] if ( l == 'stop' ): labels . append ( 0 ) elif ( l == 'limitspeed' ): labels . append ( 1 ) print ( \"Load label : Done!\" ) return np . array ( labels ) def _load_img ( file_dir , convert_type = 'L' ): data_files = os . listdir ( file_dir ) imgs = [] for file in data_files : img = np . frombuffer ( np . array ( Image . open ( file_dir + '/' + file ) . convert ( convert_type )), dtype = np . uint8 ) imgs . append ( img ) print ( \"Load img : Done!\" ) return np . array ( imgs ) def _change_one_hot_label ( X ): #2\u306e\u3068\u3053\u308d\u306f\u5206\u985e\u3057\u305f\u3044\u30af\u30e9\u30b9\u306e\u6570\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 T = np . zeros (( X . size , 2 )) for idx , row in enumerate ( T ): row [ X [ idx ]] = 1 return T def load_dataset ( DIR_PATH , convert_type = 'L' , normalize = True , flatten = True , one_hot_label = False ): labels = _load_label ( DIR_PATH ) imgs = _load_img ( DIR_PATH , convert_type ) if normalize : imgs = imgs . astype ( np . float32 ) imgs /= 255.0 if not flatten : #64\u306e\u3068\u3053\u308d\u306f\u7528\u610f\u3057\u305f\u753b\u50cf\u30b5\u30a4\u30ba\u306e\u5024\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068 if ( convert_type == 'L' ): imgs = imgs . reshape ( - 1 , 1 , 64 , 64 ) elif ( convert_type == 'RGB' ): imgs = imgs . reshape ( - 1 , 3 , 64 , 64 ) if one_hot_label : labels = _change_one_hot_label ( labels ) return imgs , labels \u4f7f\u7528\u4f8b 1 2 x_train , t_train = load_dataset ( './train_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) x_test , t_test = load_dataset ( './test_dataset' , convert_type = 'RGB' , flatten = True , normalize = True , one_hot_label = True ) \u8fd4\u308a\u5024\u306f2\u3064\u8fd4\u3063\u3066\u304f\u308b\u306e\u30672\u3064\u7528\u610f\u3059\u308b\u3053\u3068 \u5f15\u6570 \u610f\u5473 './train_dataset' \u5b66\u7fd2\u3001\u30c6\u30b9\u30c8\u7528\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u5b9a convert_type='RGB' \u5b66\u7fd2\u3092\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u3067\u3001\u3082\u3057\u304f\u306f\u30ab\u30e9\u30fc\u3067\u884c\u3046\u304b\u306e\u6307\u5b9a\u3002default\u3067\u306f\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306econvert_type='L' flatten=True \u5e73\u5766\u5316\u3092\u884c\u3046\u304b\u306e\u6307\u5b9a\u3002default\u3067\u306fTrue normalize=True \u6b63\u898f\u5316\u3092\u884c\u3046\u304b\u306e\u6307\u5b9a\u3002(\u30c7\u30fc\u30bf\u306e\u5024\u304c0.0~1.0\u306e\u7bc4\u56f2\u306b\u53ce\u307e\u308b\u3088\u3046\u306b\u3059\u308b) default\u3067\u306fTrue one_hot_label=True Onehot\u8868\u73fe\u306b\u3059\u308b\u304b\u306e\u6307\u5b9a\u3002default\u3067\u306fTrue","title":"\u753b\u50cf\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"model_mnist/tensorflow_activation_func/","text":"\u6d3b\u6027\u5316\u95a2\u6570\u30fb\u51fa\u529b\u5c64\u306e\u95a2\u6570 \u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u3069\u306e\u3088\u3046\u306b\u6d3b\u6027\u5316\u3055\u305b\u308b\u304b\u3092\u6c7a\u3081\u308b\u6d3b\u6027\u5316\u95a2\u6570\u304a\u3088\u3073\u51fa\u529b\u5c64\u306e\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u304b\u3064\u3066\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u304c\u4e00\u822c\u7684\u306b\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066\u4f7f\u308f\u308c\u3066\u3044\u305f\u304c\u3001\u73fe\u5728\u306fReLU\u95a2\u6570\u304c\u3088\u304f\u7528\u3044\u3089\u308c\u3066\u3044\u308b\u3002 \u51fa\u529b\u5c64\u306b\u7528\u3044\u308b\u95a2\u6570\u306f\u4ee5\u4e0b\u306e\u8868\u306e\u3088\u3046\u306b\u76ee\u7684\u306b\u5fdc\u3058\u3066\u9078\u629e\u3059\u308b\u3002 \u554f\u984c\u306e\u7a2e\u5225 \u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570 \u56de\u5e30 \u6052\u7b49\u5199\u50cf \u4e8c\u5024\u5206\u985e \u30ed\u30b8\u30b9\u30c6\u30a3\u30af\u30b9\u95a2\u6570(\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570) \u591a\u30af\u30e9\u30b9\u5206\u985e \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 \u5ca1\u8c37\u8cb4\u4e4b,\"\u6df1\u5c64\u5b66\u7fd2\" p.15 \u4e00\u90e8\u4fee\u6b63(\u8aa4\u5dee\u95a2\u6570\u306f\u7701\u7565) Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf # -1.0\u304b\u30891.0\u307e\u3067\u5024\u30920.01\u9593\u9694\u3067\u53d6\u5f97 x_data = np . arange ( - 30.0 , 30.0 , 0.1 ) x = tf . constant ( x_data , tf . float32 ) # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 sigmoid = tf . sigmoid ( x ) # \u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8(\u53cc\u66f2\u7dda\u95a2\u6570) tanh = tf . tanh ( x ) # ReLU\u95a2\u6570 relu = tf . nn . relu ( x ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 # f1:y = -0.5x # f2:y = 0.25x # f3:y = 0.5x-5.0 # 600x1\u884c\u5217 xx = tf . placeholder ( tf . float32 , shape = ( 600 , 1 )) # 1x3\u884c\u5217 w = tf . constant ([ - 0.5 , 0.25 , 0.5 ] , tf . float32 , shape = ( 1 , 3 )) # 600x3\u884c\u5217 b = tf . constant ( np . array ([[ 0.0 , 0.0 , - 5 ]] * 600 ), tf . float32 , shape = ( 600 , 3 )) # \u884c\u5217\u306e\u7a4d f = tf . matmul ( xx , w ) + b softmax = tf . nn . softmax ( f ) with tf . Session () as sess : # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 sigmoid_y = sess . run ( sigmoid ) # \u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8 tanh_y = sess . run ( tanh ) # ReLU\u95a2\u6570 relu_y = sess . run ( relu ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 softmax_y = sess . run ( softmax , feed_dict = { xx : x_data . reshape ( 600 , 1 )}) # \u30d7\u30ed\u30c3\u30c8(\u30b0\u30e9\u30d5\u30924\u5206\u5272) fig , (( ax1 , ax2 ), ( ax3 , ax4 )) = plt . subplots ( nrows = 2 , ncols = 2 ) # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 ax1 . set_ylim ([ - 0.1 , 1.1 ]) ax1 . plot ( x_data , sigmoid_y , label = \"sigmoid\" ) ax1 . legend ( loc = \"upper left\" , fontsize = 8 ) # \u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8 ax2 . set_ylim ([ - 1.1 , 1.1 ]) ax2 . plot ( x_data , tanh_y , label = \"tanh\" ) ax2 . legend ( loc = \"upper left\" , fontsize = 8 ) # ReLU\u95a2\u6570 ax3 . set_ylim ([ - 0.1 , 30 ]) ax3 . plot ( x_data , relu_y , label = \"Relu\" ) ax3 . legend ( loc = \"upper left\" , fontsize = 8 ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 ax4 . set_ylim ([ - 0.05 , 1.1 ]) ax4 . plot ( x_data , softmax_y [:, 0 ], label = \"softmax(f1)\" ) ax4 . plot ( x_data , softmax_y [:, 1 ], label = \"softmax(f2)\" ) ax4 . plot ( x_data , softmax_y [:, 2 ], label = \"softmax(f3)\" ) ax4 . legend ( loc = \"upper left\" , fontsize = 8 ) # \u30b0\u30e9\u30d5\u3092\u8868\u793a\u3059\u308b plt . show () \u5b9f\u884c\u7d50\u679c : \u53c2\u8003 https://www.tensorflow.org/versions/master/api_docs/python/nn.html#activation-functions \u6d3b\u6027\u5316\u95a2\u6570\u306e\u89e3\u8aac https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0","title":"\u6d3b\u6027\u5316\u95a2\u6570\u30fb\u51fa\u529b\u5c64\u306e\u95a2\u6570"},{"location":"model_mnist/tensorflow_activation_func/#_1","text":"\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u3069\u306e\u3088\u3046\u306b\u6d3b\u6027\u5316\u3055\u305b\u308b\u304b\u3092\u6c7a\u3081\u308b\u6d3b\u6027\u5316\u95a2\u6570\u304a\u3088\u3073\u51fa\u529b\u5c64\u306e\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u304b\u3064\u3066\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u304c\u4e00\u822c\u7684\u306b\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066\u4f7f\u308f\u308c\u3066\u3044\u305f\u304c\u3001\u73fe\u5728\u306fReLU\u95a2\u6570\u304c\u3088\u304f\u7528\u3044\u3089\u308c\u3066\u3044\u308b\u3002 \u51fa\u529b\u5c64\u306b\u7528\u3044\u308b\u95a2\u6570\u306f\u4ee5\u4e0b\u306e\u8868\u306e\u3088\u3046\u306b\u76ee\u7684\u306b\u5fdc\u3058\u3066\u9078\u629e\u3059\u308b\u3002 \u554f\u984c\u306e\u7a2e\u5225 \u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570 \u56de\u5e30 \u6052\u7b49\u5199\u50cf \u4e8c\u5024\u5206\u985e \u30ed\u30b8\u30b9\u30c6\u30a3\u30af\u30b9\u95a2\u6570(\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570) \u591a\u30af\u30e9\u30b9\u5206\u985e \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 \u5ca1\u8c37\u8cb4\u4e4b,\"\u6df1\u5c64\u5b66\u7fd2\" p.15 \u4e00\u90e8\u4fee\u6b63(\u8aa4\u5dee\u95a2\u6570\u306f\u7701\u7565) Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf # -1.0\u304b\u30891.0\u307e\u3067\u5024\u30920.01\u9593\u9694\u3067\u53d6\u5f97 x_data = np . arange ( - 30.0 , 30.0 , 0.1 ) x = tf . constant ( x_data , tf . float32 ) # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 sigmoid = tf . sigmoid ( x ) # \u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8(\u53cc\u66f2\u7dda\u95a2\u6570) tanh = tf . tanh ( x ) # ReLU\u95a2\u6570 relu = tf . nn . relu ( x ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 # f1:y = -0.5x # f2:y = 0.25x # f3:y = 0.5x-5.0 # 600x1\u884c\u5217 xx = tf . placeholder ( tf . float32 , shape = ( 600 , 1 )) # 1x3\u884c\u5217 w = tf . constant ([ - 0.5 , 0.25 , 0.5 ] , tf . float32 , shape = ( 1 , 3 )) # 600x3\u884c\u5217 b = tf . constant ( np . array ([[ 0.0 , 0.0 , - 5 ]] * 600 ), tf . float32 , shape = ( 600 , 3 )) # \u884c\u5217\u306e\u7a4d f = tf . matmul ( xx , w ) + b softmax = tf . nn . softmax ( f ) with tf . Session () as sess : # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 sigmoid_y = sess . run ( sigmoid ) # \u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8 tanh_y = sess . run ( tanh ) # ReLU\u95a2\u6570 relu_y = sess . run ( relu ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 softmax_y = sess . run ( softmax , feed_dict = { xx : x_data . reshape ( 600 , 1 )}) # \u30d7\u30ed\u30c3\u30c8(\u30b0\u30e9\u30d5\u30924\u5206\u5272) fig , (( ax1 , ax2 ), ( ax3 , ax4 )) = plt . subplots ( nrows = 2 , ncols = 2 ) # \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570 ax1 . set_ylim ([ - 0.1 , 1.1 ]) ax1 . plot ( x_data , sigmoid_y , label = \"sigmoid\" ) ax1 . legend ( loc = \"upper left\" , fontsize = 8 ) # \u30cf\u30a4\u30d1\u30dc\u30ea\u30c3\u30af\u30bf\u30f3\u30b8\u30a7\u30f3\u30c8 ax2 . set_ylim ([ - 1.1 , 1.1 ]) ax2 . plot ( x_data , tanh_y , label = \"tanh\" ) ax2 . legend ( loc = \"upper left\" , fontsize = 8 ) # ReLU\u95a2\u6570 ax3 . set_ylim ([ - 0.1 , 30 ]) ax3 . plot ( x_data , relu_y , label = \"Relu\" ) ax3 . legend ( loc = \"upper left\" , fontsize = 8 ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 ax4 . set_ylim ([ - 0.05 , 1.1 ]) ax4 . plot ( x_data , softmax_y [:, 0 ], label = \"softmax(f1)\" ) ax4 . plot ( x_data , softmax_y [:, 1 ], label = \"softmax(f2)\" ) ax4 . plot ( x_data , softmax_y [:, 2 ], label = \"softmax(f3)\" ) ax4 . legend ( loc = \"upper left\" , fontsize = 8 ) # \u30b0\u30e9\u30d5\u3092\u8868\u793a\u3059\u308b plt . show () \u5b9f\u884c\u7d50\u679c :","title":"\u6d3b\u6027\u5316\u95a2\u6570\u30fb\u51fa\u529b\u5c64\u306e\u95a2\u6570"},{"location":"model_mnist/tensorflow_activation_func/#_2","text":"https://www.tensorflow.org/versions/master/api_docs/python/nn.html#activation-functions \u6d3b\u6027\u5316\u95a2\u6570\u306e\u89e3\u8aac https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_cnn_mnist_01/","text":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u6e96\u5099\u7de8 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u6210: 1 \u5165\u529b\u30c7\u30fc\u30bf \u2192 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) # \u91cd\u307f 784x10\u884c\u5217 W = tf . Variable ( tf . zeros ([ 784 , 10 ]), trainable = True ) # \u30d0\u30a4\u30a2\u30b9 b = tf . Variable ( tf . zeros ([ 10 ]), trainable = True ) # Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570 y = tf . matmul ( X , W ) + b ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( logits = y , labels = t )) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u7387:0.5 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.5 ) train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 batch = mnist . train . next_batch ( 100 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ]}) if i % 200 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ]}) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 test_acc = sess . run ( accuracy , feed_dict = { X : mnist . test . images , t : mnist . test . labels }) print \"[Test] acc: %f \" % test_acc sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Extracting MNIST_data / train - images - idx3 - ubyte . gz Extracting MNIST_data / train - labels - idx1 - ubyte . gz Extracting MNIST_data / t10k - images - idx3 - ubyte . gz Extracting MNIST_data / t10k - labels - idx1 - ubyte . gz [ Train ] step : 200 , loss : 0.407333 , acc : 0.890000 [ Train ] step : 400 , loss : 0.243457 , acc : 0.910000 [ Train ] step : 600 , loss : 0.361597 , acc : 0.900000 [ Train ] step : 800 , loss : 0.315523 , acc : 0.920000 [ Train ] step : 1000 , loss : 0.269812 , acc : 0.940000 [ Train ] step : 1200 , loss : 0.353051 , acc : 0.880000 [ Train ] step : 1400 , loss : 0.243116 , acc : 0.950000 [ Train ] step : 1600 , loss : 0.158794 , acc : 0.980000 [ Train ] step : 1800 , loss : 0.271256 , acc : 0.930000 [ Train ] step : 2000 , loss : 0.207677 , acc : 0.930000 [ Test ] acc : 0.919700","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u6e96\u5099\u7de8"},{"location":"model_mnist/tensorflow_cnn_mnist_01/#mnist","text":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u6210: 1 \u5165\u529b\u30c7\u30fc\u30bf \u2192 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) # \u91cd\u307f 784x10\u884c\u5217 W = tf . Variable ( tf . zeros ([ 784 , 10 ]), trainable = True ) # \u30d0\u30a4\u30a2\u30b9 b = tf . Variable ( tf . zeros ([ 10 ]), trainable = True ) # Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570 y = tf . matmul ( X , W ) + b ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( logits = y , labels = t )) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # \u52fe\u914d\u964d\u4e0b\u6cd5 \u5b66\u7fd2\u7387:0.5 optimizer = tf . train . GradientDescentOptimizer ( learning_rate = 0.5 ) train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 2000 ): i += 1 batch = mnist . train . next_batch ( 100 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ]}) if i % 200 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ]}) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 test_acc = sess . run ( accuracy , feed_dict = { X : mnist . test . images , t : mnist . test . labels }) print \"[Test] acc: %f \" % test_acc sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Extracting MNIST_data / train - images - idx3 - ubyte . gz Extracting MNIST_data / train - labels - idx1 - ubyte . gz Extracting MNIST_data / t10k - images - idx3 - ubyte . gz Extracting MNIST_data / t10k - labels - idx1 - ubyte . gz [ Train ] step : 200 , loss : 0.407333 , acc : 0.890000 [ Train ] step : 400 , loss : 0.243457 , acc : 0.910000 [ Train ] step : 600 , loss : 0.361597 , acc : 0.900000 [ Train ] step : 800 , loss : 0.315523 , acc : 0.920000 [ Train ] step : 1000 , loss : 0.269812 , acc : 0.940000 [ Train ] step : 1200 , loss : 0.353051 , acc : 0.880000 [ Train ] step : 1400 , loss : 0.243116 , acc : 0.950000 [ Train ] step : 1600 , loss : 0.158794 , acc : 0.980000 [ Train ] step : 1800 , loss : 0.271256 , acc : 0.930000 [ Train ] step : 2000 , loss : 0.207677 , acc : 0.930000 [ Test ] acc : 0.919700","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u6e96\u5099\u7de8"},{"location":"model_mnist/tensorflow_cnn_mnist_02/","text":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af Deep MNIST for Experts \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u6210 : 1 2 3 4 5 \u5165\u529b\u5c64 => \u7573\u8fbc\u307f\u5c64 => \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 => \u7573\u8fbc\u307f\u5c64 => \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 => \u5168\u7d50\u5408\u5c64(\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u4ed8) => \u51fa\u529b\u5c64(\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570) Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) def input_image ( X ): \"\"\"\u5165\u529b\u5c64\"\"\" return tf . reshape ( X , [ - 1 , 28 , 28 , 1 ]) def weight_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial , trainable = True ) def bias_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial , trainable = True ) def conv2d ( X , W ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) def max_pool_2x2 ( X ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) ### \u5165\u529b\u5c64 input_layer = input_image ( X ) ### \u7573\u8fbc\u307f\u5c64 1 W_conv1 = weight_variable ([ 5 , 5 , 1 , 32 ]) b_conv1 = bias_variable ([ 32 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 1 h_conv1 = tf . nn . relu ( conv2d ( input_layer , W_conv1 ) + b_conv1 ) h_pool1 = max_pool_2x2 ( h_conv1 ) ### \u7573\u8fbc\u307f\u5c64 2 W_conv2 = weight_variable ([ 5 , 5 , 32 , 64 ]) b_conv2 = bias_variable ([ 64 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 2 h_conv2 = tf . nn . relu ( conv2d ( h_pool1 , W_conv2 ) + b_conv2 ) h_pool2 = max_pool_2x2 ( h_conv2 ) ### \u5168\u7d50\u5408\u5c64 W_fc1 = weight_variable ([ 7 * 7 * 64 , 1024 ]) b_fc1 = bias_variable ([ 1024 ]) h_pool2_flat = tf . reshape ( h_pool2 , [ - 1 , 7 * 7 * 64 ]) h_fc1 = tf . nn . relu ( tf . matmul ( h_pool2_flat , W_fc1 ) + b_fc1 ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 keep_prob = tf . placeholder ( tf . float32 ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc1_drop = tf . nn . dropout ( h_fc1 , keep_prob ) ### \u51fa\u529b\u5c64 W_fc2 = weight_variable ([ 1024 , 10 ]) b_fc2 = bias_variable ([ 10 ]) y_conv = tf . matmul ( h_fc1_drop , W_fc2 ) + b_fc2 ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = t , logits = y_conv )) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y_conv , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 20000 ): i += 1 batch = mnist . train . next_batch ( 50 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 0.5 }) if i % 500 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u9069\u7528\u3057\u306a\u3044\u305f\u3081\u3001keep_prob:1.0 test_acc = sess . run ( accuracy , feed_dict = { X : mnist . test . images , t : mnist . test . labels , keep_prob : 1.0 }) print \"[Test] acc: %f \" % test_acc sess . close () \u5b9f\u884c\u7d50\u679c : 1 \u53c2\u8003 Deep MNIST for Experts","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af"},{"location":"model_mnist/tensorflow_cnn_mnist_02/#mnist","text":"Deep MNIST for Experts \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u6210 : 1 2 3 4 5 \u5165\u529b\u5c64 => \u7573\u8fbc\u307f\u5c64 => \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 => \u7573\u8fbc\u307f\u5c64 => \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 => \u5168\u7d50\u5408\u5c64(\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u4ed8) => \u51fa\u529b\u5c64(\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570) Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) def input_image ( X ): \"\"\"\u5165\u529b\u5c64\"\"\" return tf . reshape ( X , [ - 1 , 28 , 28 , 1 ]) def weight_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial , trainable = True ) def bias_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial , trainable = True ) def conv2d ( X , W ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) def max_pool_2x2 ( X ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) ### \u5165\u529b\u5c64 input_layer = input_image ( X ) ### \u7573\u8fbc\u307f\u5c64 1 W_conv1 = weight_variable ([ 5 , 5 , 1 , 32 ]) b_conv1 = bias_variable ([ 32 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 1 h_conv1 = tf . nn . relu ( conv2d ( input_layer , W_conv1 ) + b_conv1 ) h_pool1 = max_pool_2x2 ( h_conv1 ) ### \u7573\u8fbc\u307f\u5c64 2 W_conv2 = weight_variable ([ 5 , 5 , 32 , 64 ]) b_conv2 = bias_variable ([ 64 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 2 h_conv2 = tf . nn . relu ( conv2d ( h_pool1 , W_conv2 ) + b_conv2 ) h_pool2 = max_pool_2x2 ( h_conv2 ) ### \u5168\u7d50\u5408\u5c64 W_fc1 = weight_variable ([ 7 * 7 * 64 , 1024 ]) b_fc1 = bias_variable ([ 1024 ]) h_pool2_flat = tf . reshape ( h_pool2 , [ - 1 , 7 * 7 * 64 ]) h_fc1 = tf . nn . relu ( tf . matmul ( h_pool2_flat , W_fc1 ) + b_fc1 ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 keep_prob = tf . placeholder ( tf . float32 ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc1_drop = tf . nn . dropout ( h_fc1 , keep_prob ) ### \u51fa\u529b\u5c64 W_fc2 = weight_variable ([ 1024 , 10 ]) b_fc2 = bias_variable ([ 10 ]) y_conv = tf . matmul ( h_fc1_drop , W_fc2 ) + b_fc2 ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( labels = t , logits = y_conv )) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y_conv , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( 20000 ): i += 1 batch = mnist . train . next_batch ( 50 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 0.5 }) if i % 500 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u9069\u7528\u3057\u306a\u3044\u305f\u3081\u3001keep_prob:1.0 test_acc = sess . run ( accuracy , feed_dict = { X : mnist . test . images , t : mnist . test . labels , keep_prob : 1.0 }) print \"[Test] acc: %f \" % test_acc sess . close () \u5b9f\u884c\u7d50\u679c : 1","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af"},{"location":"model_mnist/tensorflow_cnn_mnist_02/#_1","text":"Deep MNIST for Experts","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_cnn_mnist_03/","text":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u91cd\u307f\u6e1b\u8870 \u91cd\u307f\u6e1b\u8870\u306f\u904e\u9069\u5408\u3092\u907f\u3051\u308b\u305f\u3081\u306b\u4f7f\u308f\u308c\u308b Deep MNIST for Experts \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) def input_image ( X ): \"\"\"\u5165\u529b\u5c64\"\"\" return tf . reshape ( X , [ - 1 , 28 , 28 , 1 ]) def weight_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial ) def bias_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial ) def conv2d ( X , W ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) def max_pool_2x2 ( X ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) ### \u5165\u529b\u5c64 input_layer = input_image ( X ) ### \u7573\u8fbc\u307f\u5c64 1 W_conv1 = weight_variable ([ 5 , 5 , 1 , 32 ]) b_conv1 = bias_variable ([ 32 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 1 h_conv1 = tf . nn . relu ( conv2d ( input_layer , W_conv1 ) + b_conv1 ) h_pool1 = max_pool_2x2 ( h_conv1 ) ### \u7573\u8fbc\u307f\u5c64 2 W_conv2 = weight_variable ([ 5 , 5 , 32 , 64 ]) b_conv2 = bias_variable ([ 64 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 2 h_conv2 = tf . nn . relu ( conv2d ( h_pool1 , W_conv2 ) + b_conv2 ) h_pool2 = max_pool_2x2 ( h_conv2 ) ### \u5168\u7d50\u5408\u5c64 W_fc1 = weight_variable ([ 7 * 7 * 64 , 1024 ]) b_fc1 = bias_variable ([ 1024 ]) h_pool2_flat = tf . reshape ( h_pool2 , [ - 1 , 7 * 7 * 64 ]) h_fc1 = tf . nn . relu ( tf . matmul ( h_pool2_flat , W_fc1 ) + b_fc1 ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 keep_prob = tf . placeholder ( tf . float32 ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc1_drop = tf . nn . dropout ( h_fc1 , keep_prob ) ### \u51fa\u529b\u5c64 W_fc2 = weight_variable ([ 1024 , 10 ]) b_fc2 = bias_variable ([ 10 ]) y_conv = tf . matmul ( h_fc1_drop , W_fc2 ) + b_fc2 ### \u6b63\u5247\u5316\u9805 \u91cd\u307f\u6e1b\u8870 norm_term = tf . nn . l2_loss ( W_conv1 ) + tf . nn . l2_loss ( W_conv2 ) + tf . nn . l2_loss ( W_fc1 ) + tf . nn . l2_loss ( W_fc2 ) # \u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30bf lambda_ = 0.001 ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 cross_entropy = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( logits = y_conv , labels = t )) # \u30b3\u30b9\u30c8\u95a2\u6570 loss = cross_entropy + lambda_ * norm_term ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y_conv , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 40000 ): i += 1 batch = mnist . train . next_batch ( 50 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 0.5 }) if i % 1000 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 test_acc , test_loss = sess . run ([ accuracy , loss ], feed_dict = { X : mnist . test . images , t : mnist . test . labels , keep_prob : 1.0 }) print \"[Test] loss: %f , acc: %f \" % ( test_loss , test_acc ) row = \" %d , %f , %f , %f , %f \\n \" % ( i , train_loss , train_acc , test_loss , test_acc ) with open ( \"evaluation.csv\" , \"a\" ) as fout : fout . write ( row ) sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [ Train ] step : 1000 , loss : 9.564764 , acc : 0.960000 [ Test ] loss : 9.496043 , acc : 0.962800 [ Train ] step : 2000 , loss : 7.332607 , acc : 0.960000 [ Test ] loss : 7.343444 , acc : 0.973200 [ Train ] step : 3000 , loss : 5.587842 , acc : 0.980000 [ Test ] loss : 5.605972 , acc : 0.981300 [ Train ] step : 4000 , loss : 4.164217 , acc : 0.960000 [ Test ] loss : 4.161895 , acc : 0.984500 [ Train ] step : 5000 , loss : 3.013694 , acc : 0.960000 [ Test ] loss : 3.012430 , acc : 0.984300 [ Train ] step : 6000 , loss : 2.119012 , acc : 1.000000 [ Test ] loss : 2.136513 , acc : 0.986900 [ Train ] step : 7000 , loss : 1.490582 , acc : 1.000000 [ Test ] loss : 1.522875 , acc : 0.986200 [ Train ] step : 8000 , loss : 1.118178 , acc : 0.980000 [ Test ] loss : 1.097470 , acc : 0.987600 [ Train ] step : 9000 , loss : 0.854345 , acc : 0.980000 [ Test ] loss : 0.810288 , acc : 0.988400 [ Train ] step : 10000 , loss : 0.598223 , acc : 1.000000 [ Test ] loss : 0.611443 , acc : 0.990100 [ Train ] step : 11000 , loss : 0.462429 , acc : 1.000000 [ Test ] loss : 0.483126 , acc : 0.989000 [ Train ] step : 12000 , loss : 0.362479 , acc : 1.000000 [ Test ] loss : 0.387866 , acc : 0.990300 [ Train ] step : 13000 , loss : 0.298344 , acc : 1.000000 [ Test ] loss : 0.322925 , acc : 0.989400 [ Train ] step : 14000 , loss : 0.259400 , acc : 1.000000 [ Test ] loss : 0.271788 , acc : 0.990300 [ Train ] step : 15000 , loss : 0.217047 , acc : 1.000000 [ Test ] loss : 0.235201 , acc : 0.991100 [ Train ] step : 16000 , loss : 0.190560 , acc : 1.000000 [ Test ] loss : 0.211423 , acc : 0.988800 [ Train ] step : 17000 , loss : 0.177036 , acc : 1.000000 [ Test ] loss : 0.184945 , acc : 0.990000 [ Train ] step : 18000 , loss : 0.145949 , acc : 1.000000 [ Test ] loss : 0.170616 , acc : 0.989400 [ Train ] step : 19000 , loss : 0.139694 , acc : 1.000000 [ Test ] loss : 0.153126 , acc : 0.990900 [ Train ] step : 20000 , loss : 0.130082 , acc : 1.000000 \u91cd\u307f\u6e1b\u8870\u3092\u4f7f\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3068\u306e\u6bd4\u8f03 : \u91cd\u307f\u6e1b\u8870\u3092\u7528\u3044\u3066\u3044\u306a\u3044\u5834\u5408\u3068\u6bd4\u3079\u308b\u3068\u3001\u5b66\u7fd2\u304c\u505c\u6ede\u3057\u306a\u3044\u3053\u3068\u304c\u5206\u304b\u308b\u3002","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u91cd\u307f\u6e1b\u8870"},{"location":"model_mnist/tensorflow_cnn_mnist_03/#mnist","text":"\u91cd\u307f\u6e1b\u8870\u306f\u904e\u9069\u5408\u3092\u907f\u3051\u308b\u305f\u3081\u306b\u4f7f\u308f\u308c\u308b Deep MNIST for Experts \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) def input_image ( X ): \"\"\"\u5165\u529b\u5c64\"\"\" return tf . reshape ( X , [ - 1 , 28 , 28 , 1 ]) def weight_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial ) def bias_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial ) def conv2d ( X , W ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) def max_pool_2x2 ( X ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) ### \u5165\u529b\u5c64 input_layer = input_image ( X ) ### \u7573\u8fbc\u307f\u5c64 1 W_conv1 = weight_variable ([ 5 , 5 , 1 , 32 ]) b_conv1 = bias_variable ([ 32 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 1 h_conv1 = tf . nn . relu ( conv2d ( input_layer , W_conv1 ) + b_conv1 ) h_pool1 = max_pool_2x2 ( h_conv1 ) ### \u7573\u8fbc\u307f\u5c64 2 W_conv2 = weight_variable ([ 5 , 5 , 32 , 64 ]) b_conv2 = bias_variable ([ 64 ]) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 2 h_conv2 = tf . nn . relu ( conv2d ( h_pool1 , W_conv2 ) + b_conv2 ) h_pool2 = max_pool_2x2 ( h_conv2 ) ### \u5168\u7d50\u5408\u5c64 W_fc1 = weight_variable ([ 7 * 7 * 64 , 1024 ]) b_fc1 = bias_variable ([ 1024 ]) h_pool2_flat = tf . reshape ( h_pool2 , [ - 1 , 7 * 7 * 64 ]) h_fc1 = tf . nn . relu ( tf . matmul ( h_pool2_flat , W_fc1 ) + b_fc1 ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 keep_prob = tf . placeholder ( tf . float32 ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc1_drop = tf . nn . dropout ( h_fc1 , keep_prob ) ### \u51fa\u529b\u5c64 W_fc2 = weight_variable ([ 1024 , 10 ]) b_fc2 = bias_variable ([ 10 ]) y_conv = tf . matmul ( h_fc1_drop , W_fc2 ) + b_fc2 ### \u6b63\u5247\u5316\u9805 \u91cd\u307f\u6e1b\u8870 norm_term = tf . nn . l2_loss ( W_conv1 ) + tf . nn . l2_loss ( W_conv2 ) + tf . nn . l2_loss ( W_fc1 ) + tf . nn . l2_loss ( W_fc2 ) # \u6b63\u5247\u5316\u30d1\u30e9\u30e1\u30bf lambda_ = 0.001 ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 cross_entropy = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( logits = y_conv , labels = t )) # \u30b3\u30b9\u30c8\u95a2\u6570 loss = cross_entropy + lambda_ * norm_term ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y_conv , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 40000 ): i += 1 batch = mnist . train . next_batch ( 50 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 0.5 }) if i % 1000 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 test_acc , test_loss = sess . run ([ accuracy , loss ], feed_dict = { X : mnist . test . images , t : mnist . test . labels , keep_prob : 1.0 }) print \"[Test] loss: %f , acc: %f \" % ( test_loss , test_acc ) row = \" %d , %f , %f , %f , %f \\n \" % ( i , train_loss , train_acc , test_loss , test_acc ) with open ( \"evaluation.csv\" , \"a\" ) as fout : fout . write ( row ) sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [ Train ] step : 1000 , loss : 9.564764 , acc : 0.960000 [ Test ] loss : 9.496043 , acc : 0.962800 [ Train ] step : 2000 , loss : 7.332607 , acc : 0.960000 [ Test ] loss : 7.343444 , acc : 0.973200 [ Train ] step : 3000 , loss : 5.587842 , acc : 0.980000 [ Test ] loss : 5.605972 , acc : 0.981300 [ Train ] step : 4000 , loss : 4.164217 , acc : 0.960000 [ Test ] loss : 4.161895 , acc : 0.984500 [ Train ] step : 5000 , loss : 3.013694 , acc : 0.960000 [ Test ] loss : 3.012430 , acc : 0.984300 [ Train ] step : 6000 , loss : 2.119012 , acc : 1.000000 [ Test ] loss : 2.136513 , acc : 0.986900 [ Train ] step : 7000 , loss : 1.490582 , acc : 1.000000 [ Test ] loss : 1.522875 , acc : 0.986200 [ Train ] step : 8000 , loss : 1.118178 , acc : 0.980000 [ Test ] loss : 1.097470 , acc : 0.987600 [ Train ] step : 9000 , loss : 0.854345 , acc : 0.980000 [ Test ] loss : 0.810288 , acc : 0.988400 [ Train ] step : 10000 , loss : 0.598223 , acc : 1.000000 [ Test ] loss : 0.611443 , acc : 0.990100 [ Train ] step : 11000 , loss : 0.462429 , acc : 1.000000 [ Test ] loss : 0.483126 , acc : 0.989000 [ Train ] step : 12000 , loss : 0.362479 , acc : 1.000000 [ Test ] loss : 0.387866 , acc : 0.990300 [ Train ] step : 13000 , loss : 0.298344 , acc : 1.000000 [ Test ] loss : 0.322925 , acc : 0.989400 [ Train ] step : 14000 , loss : 0.259400 , acc : 1.000000 [ Test ] loss : 0.271788 , acc : 0.990300 [ Train ] step : 15000 , loss : 0.217047 , acc : 1.000000 [ Test ] loss : 0.235201 , acc : 0.991100 [ Train ] step : 16000 , loss : 0.190560 , acc : 1.000000 [ Test ] loss : 0.211423 , acc : 0.988800 [ Train ] step : 17000 , loss : 0.177036 , acc : 1.000000 [ Test ] loss : 0.184945 , acc : 0.990000 [ Train ] step : 18000 , loss : 0.145949 , acc : 1.000000 [ Test ] loss : 0.170616 , acc : 0.989400 [ Train ] step : 19000 , loss : 0.139694 , acc : 1.000000 [ Test ] loss : 0.153126 , acc : 0.990900 [ Train ] step : 20000 , loss : 0.130082 , acc : 1.000000 \u91cd\u307f\u6e1b\u8870\u3092\u4f7f\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3068\u306e\u6bd4\u8f03 : \u91cd\u307f\u6e1b\u8870\u3092\u7528\u3044\u3066\u3044\u306a\u3044\u5834\u5408\u3068\u6bd4\u3079\u308b\u3068\u3001\u5b66\u7fd2\u304c\u505c\u6ede\u3057\u306a\u3044\u3053\u3068\u304c\u5206\u304b\u308b\u3002","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u91cd\u307f\u6e1b\u8870"},{"location":"model_mnist/tensorflow_cnn_mnist_04/","text":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af TensorBoard\u7de8 \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092TensorBoard\u306b\u3088\u308a\u53ef\u8996\u5316\u3059\u308b\u3002\u306a\u304a\u5404\u5c64\u306f\u30af\u30e9\u30b9\u3068\u3057\u3066\u5b9f\u88c5\u3057\u305f\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) def weight_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial , name = name ) def bias_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial , name = name ) class InputLayer : \"\"\"\u5165\u529b\u5c64\"\"\" def __init__ ( self , X , image_shape = None , name = None ): with tf . name_scope ( name ): shape = tf . shape ( X ) output = tf . reshape ( X , [ - 1 , image_shape [ 0 ], image_shape [ 1 ], 1 ]) self . output = output class ConvPoolLayer : \"\"\" \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 \"\"\" def __init__ ( self , layer , shape = None , name = None ): \"\"\" input_layer: \u5165\u529b\u5c64(\u753b\u50cf\u30c7\u30fc\u30bf\u7fa4) shape: \u30d5\u30a3\u30eb\u30bf\u306e\u7e26\u30b5\u30a4\u30ba, \u30d5\u30a3\u30eb\u30bf\u306e\u6a2a\u30b5\u30a4\u30ba, \u5165\u529b\u30ec\u30a4\u30e4\u6570, \u51fa\u529b\u30ec\u30a4\u30e4\u6570 \"\"\" with tf . name_scope ( name ): ### \u7573\u8fbc\u307f\u5c64 with tf . name_scope ( \"convolutional_layer\" ): W = weight_variable ( shape , \"conv_layer_weights\" ) b = bias_variable ([ shape [ 3 ]], \"conv_layer_biases\" ) conv = self . conv2d ( layer . output , W , \"conv_layer\" ) + b ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 with tf . name_scope ( \"max_pooling_layer\" ): h_conv = tf . nn . relu ( conv ) h_pool = self . max_pool_2x2 ( h_conv , \"max_pooling_layer\" ) self . output = h_pool # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \" %s _weights\" % name , W ) tf . summary . histogram ( \" %s _biases\" % name , b ) def conv2d ( self , X , W , name ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' , name = name ) def max_pool_2x2 ( self , X , name ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' , name = name ) class FullyConnectedLayer : \"\"\"\u5168\u7d50\u5408\u5c64\"\"\" def __init__ ( self , layer , shape = None , dropout_rate = None , name = None ): \"\"\" shape: \u5165\u529b\u30ec\u30a4\u30e4\u306e\u5f62\u5f0f dropout_rate: \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 \"\"\" with tf . name_scope ( name ): ### \u5168\u7d50\u5408\u5c64 with tf . name_scope ( \"fully_connected_layer\" ): W = weight_variable ( shape , \"fully_connected_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"fully_connected_layer_biases\" ) h_pool_flat = tf . reshape ( layer . output , [ - 1 , shape [ 0 ]]) h_fc = tf . nn . relu ( tf . matmul ( h_pool_flat , W ) + b ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 with tf . name_scope ( \"dropout\" ): # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc_drop = tf . nn . dropout ( h_fc , dropout_rate , name = \"dropout\" ) self . output = h_fc_drop # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \" %s _weights\" % name , W ) tf . summary . histogram ( \" %s _biases\" % name , b ) class OutputLayer : \"\"\"\u51fa\u529b\u5c64\"\"\" def __init__ ( self , layer , shape = None , name = None ): with tf . name_scope ( name ): with tf . name_scope ( \"output_layer\" ): W = weight_variable ( shape , \"output_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"output_layer_biases\" ) y = tf . matmul ( layer . output , W ) + b self . output = y # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \" %s _weights\" % name , W ) tf . summary . histogram ( \" %s _biases\" % name , b ) class Optimizer : \"\"\"\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( name ): ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( logits = output_layer . output , labels = t ), name = \"loss\" ) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) self . loss = loss self . train_step = optimizer . minimize ( loss ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . scalar ( \"loss\" , loss ) class Evaluator : \"\"\"\u8a55\u4fa1\u5668\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( \"evaluator\" ): # \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( output_layer . output , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 ), name = \"accuracy\" ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . scalar ( \"accuracy\" , accuracy ) self . accuracy = accuracy if __name__ == \"__main__\" : # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) # \u5165\u529b\u30c7\u30fc\u30bf X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 ), name = \"input_images\" ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 10 ), name = \"labels\" ) # \u5165\u529b\u5c64 input_layer = InputLayer ( X , image_shape = ( 28 , 28 ), name = \"input_layer\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer1 = ConvPoolLayer ( input_layer , shape = ( 5 , 5 , 1 , 32 ), name = \"cnv_pool_layer1\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer2 = ConvPoolLayer ( cnv_pool_layer1 , shape = ( 5 , 5 , 32 , 64 ), name = \"cnv_pool_layer2\" ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 keep_prob = tf . placeholder ( tf . float32 ) # \u5168\u7d50\u5408\u5c64 fully_connected_layer = FullyConnectedLayer ( cnv_pool_layer2 , shape = ( 7 * 7 * 64 , 1024 ), dropout_rate = keep_prob , name = \"fully_connected_layer\" ) # \u51fa\u529b\u5c64 output_layer = OutputLayer ( fully_connected_layer , shape = ( 1024 , 10 ), name = \"output_layer\" ) # \u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 optimizer = Optimizer ( output_layer , t , name = \"Optimizer\" ) # \u8a55\u4fa1 evaluator = Evaluator ( output_layer , t , name = \"Evaluator\" ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u884c sess = tf . InteractiveSession () sess . run ( tf . global_variables_initializer ()) summary = tf . merge_all_summaries () writer = tf . train . SummaryWriter ( \"./mnist_cnn_log\" , sess . graph ) i = 0 for _ in range ( 20000 ): i += 1 # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u53d6\u5f97\u3059\u308b batch = mnist . train . next_batch ( 50 ) # \u5b66\u7fd2\u306e\u5b9f\u884c \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387:0.5 sess . run ( optimizer . train_step , feed_dict = { X : batch_x , t : batch_t , keep_prob : 0.5 }) if i % 1000 == 0 : summary_ , train_acc , train_loss = sess . run ([ summary , evaluator . accuracy , optimizer . loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) writer . add_summary ( summary_ , i ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u9069\u7528\u3057\u306a\u3044\u305f\u3081\u3001keep_prob:1.0 test_x , test_t = mnist . test . images , mnist . test . labels test_acc , test_loss = sess . run ([ evaluator . accuracy , optimizer . loss ], feed_dict = { X : test_x , t : test_t , keep_prob : 1.0 }) print \"[Test] step: %d , loss: %f , acc: %f \" % ( i , test_loss , test_acc ) sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ Train ] step : 1000 , loss : 0.154021 , acc : 0.960000 [ Test ] step : 1000 , loss : 0.127709 , acc : 0.962900 [ Train ] step : 2000 , loss : 0.073485 , acc : 0.960000 [ Test ] step : 2000 , loss : 0.079148 , acc : 0.974900 [ Train ] step : 3000 , loss : 0.036493 , acc : 0.980000 [ Test ] step : 3000 , loss : 0.054819 , acc : 0.983000 [ Train ] step : 4000 , loss : 0.057755 , acc : 0.980000 [ Test ] step : 4000 , loss : 0.047340 , acc : 0.985100 [ Train ] step : 5000 , loss : 0.053140 , acc : 0.960000 [ Test ] step : 5000 , loss : 0.041212 , acc : 0.985700 [ Train ] step : 6000 , loss : 0.020518 , acc : 0.980000 [ Test ] step : 6000 , loss : 0.037913 , acc : 0.986600 [ Train ] step : 7000 , loss : 0.003017 , acc : 1.000000 [ Test ] step : 7000 , loss : 0.035466 , acc : 0.988700 [ Train ] step : 8000 , loss : 0.049886 , acc : 0.980000 [ Test ] step : 8000 , loss : 0.032213 , acc : 0.989100 [ Train ] step : 9000 , loss : 0.048381 , acc : 0.980000 [ Test ] step : 9000 , loss : 0.034304 , acc : 0.988400 [ Train ] step : 10000 , loss : 0.002774 , acc : 1.000000 [ Test ] step : 10000 , loss : 0.027083 , acc : 0.990200 [ Train ] step : 11000 , loss : 0.002015 , acc : 1.000000 [ Test ] step : 11000 , loss : 0.025643 , acc : 0.990300 [ Train ] step : 12000 , loss : 0.001177 , acc : 1.000000 [ Test ] step : 12000 , loss : 0.026513 , acc : 0.990200 [ Train ] step : 13000 , loss : 0.003036 , acc : 1.000000 [ Test ] step : 13000 , loss : 0.028380 , acc : 0.990800 [ Train ] step : 14000 , loss : 0.004232 , acc : 1.000000 [ Test ] step : 14000 , loss : 0.026199 , acc : 0.991200 [ Train ] step : 15000 , loss : 0.003418 , acc : 1.000000 [ Test ] step : 15000 , loss : 0.027537 , acc : 0.991500 [ Train ] step : 16000 , loss : 0.000270 , acc : 1.000000 [ Test ] step : 16000 , loss : 0.024676 , acc : 0.991900 [ Train ] step : 17000 , loss : 0.001024 , acc : 1.000000 [ Test ] step : 17000 , loss : 0.028792 , acc : 0.991100 [ Train ] step : 18000 , loss : 0.000457 , acc : 1.000000 [ Test ] step : 18000 , loss : 0.026204 , acc : 0.992200 [ Train ] step : 19000 , loss : 0.001267 , acc : 1.000000 [ Test ] step : 19000 , loss : 0.025982 , acc : 0.991700 [ Train ] step : 20000 , loss : 0.000333 , acc : 1.000000 [ Test ] step : 20000 , loss : 0.023550 , acc : 0.992600 Tensorboard\u306e\u5b9f\u884c 1 $ tensorboard --logdir = ./mnist_cnn_log EVENTS GRAPHS GRAPHS DISTRIBUTIONS HISTOGRAMS","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af TensorBoard\u7de8"},{"location":"model_mnist/tensorflow_cnn_mnist_04/#mnist-tensorboard","text":"\u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092TensorBoard\u306b\u3088\u308a\u53ef\u8996\u5316\u3059\u308b\u3002\u306a\u304a\u5404\u5c64\u306f\u30af\u30e9\u30b9\u3068\u3057\u3066\u5b9f\u88c5\u3057\u305f\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) def weight_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial , name = name ) def bias_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial , name = name ) class InputLayer : \"\"\"\u5165\u529b\u5c64\"\"\" def __init__ ( self , X , image_shape = None , name = None ): with tf . name_scope ( name ): shape = tf . shape ( X ) output = tf . reshape ( X , [ - 1 , image_shape [ 0 ], image_shape [ 1 ], 1 ]) self . output = output class ConvPoolLayer : \"\"\" \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 \"\"\" def __init__ ( self , layer , shape = None , name = None ): \"\"\" input_layer: \u5165\u529b\u5c64(\u753b\u50cf\u30c7\u30fc\u30bf\u7fa4) shape: \u30d5\u30a3\u30eb\u30bf\u306e\u7e26\u30b5\u30a4\u30ba, \u30d5\u30a3\u30eb\u30bf\u306e\u6a2a\u30b5\u30a4\u30ba, \u5165\u529b\u30ec\u30a4\u30e4\u6570, \u51fa\u529b\u30ec\u30a4\u30e4\u6570 \"\"\" with tf . name_scope ( name ): ### \u7573\u8fbc\u307f\u5c64 with tf . name_scope ( \"convolutional_layer\" ): W = weight_variable ( shape , \"conv_layer_weights\" ) b = bias_variable ([ shape [ 3 ]], \"conv_layer_biases\" ) conv = self . conv2d ( layer . output , W , \"conv_layer\" ) + b ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 with tf . name_scope ( \"max_pooling_layer\" ): h_conv = tf . nn . relu ( conv ) h_pool = self . max_pool_2x2 ( h_conv , \"max_pooling_layer\" ) self . output = h_pool # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \" %s _weights\" % name , W ) tf . summary . histogram ( \" %s _biases\" % name , b ) def conv2d ( self , X , W , name ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' , name = name ) def max_pool_2x2 ( self , X , name ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' , name = name ) class FullyConnectedLayer : \"\"\"\u5168\u7d50\u5408\u5c64\"\"\" def __init__ ( self , layer , shape = None , dropout_rate = None , name = None ): \"\"\" shape: \u5165\u529b\u30ec\u30a4\u30e4\u306e\u5f62\u5f0f dropout_rate: \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 \"\"\" with tf . name_scope ( name ): ### \u5168\u7d50\u5408\u5c64 with tf . name_scope ( \"fully_connected_layer\" ): W = weight_variable ( shape , \"fully_connected_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"fully_connected_layer_biases\" ) h_pool_flat = tf . reshape ( layer . output , [ - 1 , shape [ 0 ]]) h_fc = tf . nn . relu ( tf . matmul ( h_pool_flat , W ) + b ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 with tf . name_scope ( \"dropout\" ): # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc_drop = tf . nn . dropout ( h_fc , dropout_rate , name = \"dropout\" ) self . output = h_fc_drop # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \" %s _weights\" % name , W ) tf . summary . histogram ( \" %s _biases\" % name , b ) class OutputLayer : \"\"\"\u51fa\u529b\u5c64\"\"\" def __init__ ( self , layer , shape = None , name = None ): with tf . name_scope ( name ): with tf . name_scope ( \"output_layer\" ): W = weight_variable ( shape , \"output_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"output_layer_biases\" ) y = tf . matmul ( layer . output , W ) + b self . output = y # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . histogram ( \" %s _weights\" % name , W ) tf . summary . histogram ( \" %s _biases\" % name , b ) class Optimizer : \"\"\"\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( name ): ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( logits = output_layer . output , labels = t ), name = \"loss\" ) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) self . loss = loss self . train_step = optimizer . minimize ( loss ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . scalar ( \"loss\" , loss ) class Evaluator : \"\"\"\u8a55\u4fa1\u5668\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( \"evaluator\" ): # \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( output_layer . output , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 ), name = \"accuracy\" ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . summary . scalar ( \"accuracy\" , accuracy ) self . accuracy = accuracy if __name__ == \"__main__\" : # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) # \u5165\u529b\u30c7\u30fc\u30bf X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 ), name = \"input_images\" ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 10 ), name = \"labels\" ) # \u5165\u529b\u5c64 input_layer = InputLayer ( X , image_shape = ( 28 , 28 ), name = \"input_layer\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer1 = ConvPoolLayer ( input_layer , shape = ( 5 , 5 , 1 , 32 ), name = \"cnv_pool_layer1\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer2 = ConvPoolLayer ( cnv_pool_layer1 , shape = ( 5 , 5 , 32 , 64 ), name = \"cnv_pool_layer2\" ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 keep_prob = tf . placeholder ( tf . float32 ) # \u5168\u7d50\u5408\u5c64 fully_connected_layer = FullyConnectedLayer ( cnv_pool_layer2 , shape = ( 7 * 7 * 64 , 1024 ), dropout_rate = keep_prob , name = \"fully_connected_layer\" ) # \u51fa\u529b\u5c64 output_layer = OutputLayer ( fully_connected_layer , shape = ( 1024 , 10 ), name = \"output_layer\" ) # \u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 optimizer = Optimizer ( output_layer , t , name = \"Optimizer\" ) # \u8a55\u4fa1 evaluator = Evaluator ( output_layer , t , name = \"Evaluator\" ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u884c sess = tf . InteractiveSession () sess . run ( tf . global_variables_initializer ()) summary = tf . merge_all_summaries () writer = tf . train . SummaryWriter ( \"./mnist_cnn_log\" , sess . graph ) i = 0 for _ in range ( 20000 ): i += 1 # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u53d6\u5f97\u3059\u308b batch = mnist . train . next_batch ( 50 ) # \u5b66\u7fd2\u306e\u5b9f\u884c \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387:0.5 sess . run ( optimizer . train_step , feed_dict = { X : batch_x , t : batch_t , keep_prob : 0.5 }) if i % 1000 == 0 : summary_ , train_acc , train_loss = sess . run ([ summary , evaluator . accuracy , optimizer . loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) writer . add_summary ( summary_ , i ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u9069\u7528\u3057\u306a\u3044\u305f\u3081\u3001keep_prob:1.0 test_x , test_t = mnist . test . images , mnist . test . labels test_acc , test_loss = sess . run ([ evaluator . accuracy , optimizer . loss ], feed_dict = { X : test_x , t : test_t , keep_prob : 1.0 }) print \"[Test] step: %d , loss: %f , acc: %f \" % ( i , test_loss , test_acc ) sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ Train ] step : 1000 , loss : 0.154021 , acc : 0.960000 [ Test ] step : 1000 , loss : 0.127709 , acc : 0.962900 [ Train ] step : 2000 , loss : 0.073485 , acc : 0.960000 [ Test ] step : 2000 , loss : 0.079148 , acc : 0.974900 [ Train ] step : 3000 , loss : 0.036493 , acc : 0.980000 [ Test ] step : 3000 , loss : 0.054819 , acc : 0.983000 [ Train ] step : 4000 , loss : 0.057755 , acc : 0.980000 [ Test ] step : 4000 , loss : 0.047340 , acc : 0.985100 [ Train ] step : 5000 , loss : 0.053140 , acc : 0.960000 [ Test ] step : 5000 , loss : 0.041212 , acc : 0.985700 [ Train ] step : 6000 , loss : 0.020518 , acc : 0.980000 [ Test ] step : 6000 , loss : 0.037913 , acc : 0.986600 [ Train ] step : 7000 , loss : 0.003017 , acc : 1.000000 [ Test ] step : 7000 , loss : 0.035466 , acc : 0.988700 [ Train ] step : 8000 , loss : 0.049886 , acc : 0.980000 [ Test ] step : 8000 , loss : 0.032213 , acc : 0.989100 [ Train ] step : 9000 , loss : 0.048381 , acc : 0.980000 [ Test ] step : 9000 , loss : 0.034304 , acc : 0.988400 [ Train ] step : 10000 , loss : 0.002774 , acc : 1.000000 [ Test ] step : 10000 , loss : 0.027083 , acc : 0.990200 [ Train ] step : 11000 , loss : 0.002015 , acc : 1.000000 [ Test ] step : 11000 , loss : 0.025643 , acc : 0.990300 [ Train ] step : 12000 , loss : 0.001177 , acc : 1.000000 [ Test ] step : 12000 , loss : 0.026513 , acc : 0.990200 [ Train ] step : 13000 , loss : 0.003036 , acc : 1.000000 [ Test ] step : 13000 , loss : 0.028380 , acc : 0.990800 [ Train ] step : 14000 , loss : 0.004232 , acc : 1.000000 [ Test ] step : 14000 , loss : 0.026199 , acc : 0.991200 [ Train ] step : 15000 , loss : 0.003418 , acc : 1.000000 [ Test ] step : 15000 , loss : 0.027537 , acc : 0.991500 [ Train ] step : 16000 , loss : 0.000270 , acc : 1.000000 [ Test ] step : 16000 , loss : 0.024676 , acc : 0.991900 [ Train ] step : 17000 , loss : 0.001024 , acc : 1.000000 [ Test ] step : 17000 , loss : 0.028792 , acc : 0.991100 [ Train ] step : 18000 , loss : 0.000457 , acc : 1.000000 [ Test ] step : 18000 , loss : 0.026204 , acc : 0.992200 [ Train ] step : 19000 , loss : 0.001267 , acc : 1.000000 [ Test ] step : 19000 , loss : 0.025982 , acc : 0.991700 [ Train ] step : 20000 , loss : 0.000333 , acc : 1.000000 [ Test ] step : 20000 , loss : 0.023550 , acc : 0.992600 Tensorboard\u306e\u5b9f\u884c 1 $ tensorboard --logdir = ./mnist_cnn_log EVENTS GRAPHS GRAPHS DISTRIBUTIONS HISTOGRAMS","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af TensorBoard\u7de8"},{"location":"model_mnist/tensorflow_cnn_mnist_05/","text":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u66f8\u8fbc\u307f tf.app.flags \u3092\u4f7f\u3044\u3001TensorBoard\u306e\u30b5\u30de\u30ea\u30fc\u3068\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u5148\u3092\u6307\u5b9a\u3059\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 # coding:utf-8 \"\"\" filename: mnist_cnn.py \"\"\" import re import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570 FLAGS = tf . app . flags . FLAGS tf . app . flags . DEFINE_string ( \"restore_session_path\" , None , \"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u8aad\u307f\u8fbc\u307f\u5148\" ) tf . app . flags . DEFINE_string ( \"summary_path\" , \"./mnist_cnn_log\" , \"TensorBoard\u7528\u30ed\u30b0\u306e\u4fdd\u5b58\u5148\" ) tf . app . flags . DEFINE_string ( \"session_path\" , \"./mnist_cnn_session\" , \"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u5148\" ) def weight_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial , name = name ) def bias_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial , name = name ) class InputLayer : \"\"\"\u5165\u529b\u5c64\"\"\" def __init__ ( self , X , image_shape = None , name = None ): with tf . name_scope ( name ): shape = tf . shape ( X ) output = tf . reshape ( X , [ - 1 , image_shape [ 0 ], image_shape [ 1 ], 1 ]) self . output = output class ConvPoolLayer : \"\"\" \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 \"\"\" def __init__ ( self , layer , shape = None , name = None ): \"\"\" input_layer: \u5165\u529b\u5c64(\u753b\u50cf\u30c7\u30fc\u30bf\u7fa4) shape: \u30d5\u30a3\u30eb\u30bf\u306e\u7e26\u30b5\u30a4\u30ba, \u30d5\u30a3\u30eb\u30bf\u306e\u6a2a\u30b5\u30a4\u30ba, \u5165\u529b\u30ec\u30a4\u30e4\u6570, \u51fa\u529b\u30ec\u30a4\u30e4\u6570 \"\"\" with tf . name_scope ( name ): ### \u7573\u8fbc\u307f\u5c64 with tf . name_scope ( \"convolutional_layer\" ): W = weight_variable ( shape , \"conv_layer_weights\" ) b = bias_variable ([ shape [ 3 ]], \"conv_layer_biases\" ) conv = self . conv2d ( layer . output , W , \"conv_layer\" ) + b ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 with tf . name_scope ( \"max_pooling_layer\" ): h_conv = tf . nn . relu ( conv ) h_pool = self . max_pool_2x2 ( h_conv , \"max_pooling_layer\" ) self . output = h_pool # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . histogram_summary ( \" %s _weights\" % name , W ) tf . histogram_summary ( \" %s _biases\" % name , b ) def conv2d ( self , X , W , name ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' , name = name ) def max_pool_2x2 ( self , X , name ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' , name = name ) class FullyConnectedLayer : \"\"\"\u5168\u7d50\u5408\u5c64\"\"\" def __init__ ( self , layer , shape = None , dropout_rate = None , name = None ): \"\"\" shape: \u5165\u529b\u30ec\u30a4\u30e4\u306e\u5f62\u5f0f dropout_rate: \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 \"\"\" with tf . name_scope ( name ): ### \u5168\u7d50\u5408\u5c64 with tf . name_scope ( \"fully_connected_layer\" ): W = weight_variable ( shape , \"fully_connected_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"fully_connected_layer_biases\" ) h_pool_flat = tf . reshape ( layer . output , [ - 1 , shape [ 0 ]]) h_fc = tf . nn . relu ( tf . matmul ( h_pool_flat , W ) + b ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 with tf . name_scope ( \"dropout\" ): # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc_drop = tf . nn . dropout ( h_fc , dropout_rate , name = \"dropout\" ) self . output = h_fc_drop # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . histogram_summary ( \" %s _weights\" % name , W ) tf . histogram_summary ( \" %s _biases\" % name , b ) class OutputLayer : \"\"\"\u51fa\u529b\u5c64\"\"\" def __init__ ( self , layer , shape = None , name = None ): with tf . name_scope ( name ): with tf . name_scope ( \"output_layer\" ): W = weight_variable ( shape , \"output_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"output_layer_biases\" ) y = tf . matmul ( layer . output , W ) + b self . output = y # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . histogram_summary ( \" %s _weights\" % name , W ) tf . histogram_summary ( \" %s _biases\" % name , b ) class Optimizer : \"\"\"\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( name ): ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( output_layer . output , t ), name = \"loss\" ) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) self . loss = loss self . train_step = optimizer . minimize ( loss ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . scalar_summary ( \"loss\" , loss ) class Evaluator : \"\"\"\u8a55\u4fa1\u5668\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( \"evaluator\" ): # \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( output_layer . output , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 ), name = \"accuracy\" ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . scalar_summary ( \"accuracy\" , accuracy ) self . accuracy = accuracy if __name__ == \"__main__\" : # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) # \u5165\u529b\u30c7\u30fc\u30bf X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 ), name = \"input_images\" ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 10 ), name = \"labels\" ) # \u5165\u529b\u5c64 input_layer = InputLayer ( X , image_shape = ( 28 , 28 ), name = \"input_layer\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer1 = ConvPoolLayer ( input_layer , shape = ( 5 , 5 , 1 , 32 ), name = \"cnv_pool_layer1\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer2 = ConvPoolLayer ( cnv_pool_layer1 , shape = ( 5 , 5 , 32 , 64 ), name = \"cnv_pool_layer2\" ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 keep_prob = tf . placeholder ( tf . float32 ) # \u5168\u7d50\u5408\u5c64 fully_connected_layer = FullyConnectedLayer ( cnv_pool_layer2 , shape = ( 7 * 7 * 64 , 1024 ), dropout_rate = keep_prob , name = \"fully_connected_layer\" ) # \u51fa\u529b\u5c64 output_layer = OutputLayer ( fully_connected_layer , shape = ( 1024 , 10 ), name = \"output_layer\" ) # \u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 optimizer = Optimizer ( output_layer , t , name = \"Optimizer\" ) # \u8a55\u4fa1 evaluator = Evaluator ( output_layer , t , name = \"Evaluator\" ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u884c sess = tf . InteractiveSession () sess . run ( tf . initialize_all_variables ()) summary = tf . merge_all_summaries () writer = tf . train . SummaryWriter ( FLAGS . summary_path , sess . graph ) # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f\u3092\u884c\u3046\u30aa\u30d6\u30b8\u30a7\u30af\u30c8 saver = tf . train . Saver () i = 0 # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u306e\u51e6\u7406 if FLAGS . restore_session_path : saver . restore ( sess , FLAGS . restore_session_path ) ret = re . search ( \"-(\\d+)\" , FLAGS . restore_session_path ) i = int ( ret . group ( 1 )) for _ in range ( 20000 ): i += 1 # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u53d6\u5f97\u3059\u308b batch_x , batch_t = mnist . train . next_batch ( 50 ) # \u5b66\u7fd2\u306e\u5b9f\u884c \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387:0.5 sess . run ( optimizer . train_step , feed_dict = { X : batch_x , t : batch_t , keep_prob : 0.5 }) if i % 1000 == 0 : summary_ , train_acc , train_loss = sess . run ([ summary , evaluator . accuracy , optimizer . loss ], feed_dict = { X : batch_x , t : batch_t , keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) writer . add_summary ( summary_ , i ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u9069\u7528\u3057\u306a\u3044\u305f\u3081\u3001keep_prob:1.0 test_x , test_t = mnist . test . images , mnist . test . labels test_acc , test_loss = sess . run ([ evaluator . accuracy , optimizer . loss ], feed_dict = { X : test_x , t : test_t , keep_prob : 1.0 }) print \"[Test] step: %d , loss: %f , acc: %f \" % ( i , test_loss , test_acc ) # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58 saver . save ( sess , FLAGS . summary_path , global_step = i ) sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 $ python mnist_cnn . py --summary_path ./summary_log --session_path ./session_log [ Train ] step : 1000 , loss : 0.153830 , acc : 0.960000 [ Test ] step : 1000 , loss : 0.127999 , acc : 0.963500 [ Train ] step : 2000 , loss : 0.071363 , acc : 0.960000 [ Test ] step : 2000 , loss : 0.079683 , acc : 0.975100 ... \u7565 ... [ Train ] step : 19000 , loss : 0.000496 , acc : 1.000000 [ Test ] step : 19000 , loss : 0.023757 , acc : 0.992700 [ Train ] step : 20000 , loss : 0.000214 , acc : 1.000000 [ Test ] step : 20000 , loss : 0.024871 , acc : 0.992600 \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u8aad\u307f\u8fbc\u307f\u3093\u3067\u3001\u5b66\u7fd2\u3092\u518d\u958b\u3059\u308b 1 2 3 4 5 6 7 8 9 10 $ python mnist_cnn . py --summary_path ./summary_log --session_path ./session_log --restore_session_path ./summary_log-20000 [ Train ] step : 21000 , loss : 0.000479 , acc : 1.000000 [ Test ] step : 21000 , loss : 0.027168 , acc : 0.991400 [ Train ] step : 22000 , loss : 0.002611 , acc : 1.000000 [ Test ] step : 22000 , loss : 0.028818 , acc : 0.993100 ... \u7565 ... [ Train ] step : 39000 , loss : 0.000082 , acc : 1.000000 [ Test ] step : 39000 , loss : 0.030697 , acc : 0.992600 [ Train ] step : 40000 , loss : 0.000004 , acc : 1.000000 [ Test ] step : 40000 , loss : 0.028255 , acc : 0.993100","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u66f8\u8fbc\u307f"},{"location":"model_mnist/tensorflow_cnn_mnist_05/#mnist","text":"tf.app.flags \u3092\u4f7f\u3044\u3001TensorBoard\u306e\u30b5\u30de\u30ea\u30fc\u3068\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u5148\u3092\u6307\u5b9a\u3059\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 # coding:utf-8 \"\"\" filename: mnist_cnn.py \"\"\" import re import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570 FLAGS = tf . app . flags . FLAGS tf . app . flags . DEFINE_string ( \"restore_session_path\" , None , \"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u8aad\u307f\u8fbc\u307f\u5148\" ) tf . app . flags . DEFINE_string ( \"summary_path\" , \"./mnist_cnn_log\" , \"TensorBoard\u7528\u30ed\u30b0\u306e\u4fdd\u5b58\u5148\" ) tf . app . flags . DEFINE_string ( \"session_path\" , \"./mnist_cnn_session\" , \"\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u5148\" ) def weight_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial , name = name ) def bias_variable ( shape , name ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial , name = name ) class InputLayer : \"\"\"\u5165\u529b\u5c64\"\"\" def __init__ ( self , X , image_shape = None , name = None ): with tf . name_scope ( name ): shape = tf . shape ( X ) output = tf . reshape ( X , [ - 1 , image_shape [ 0 ], image_shape [ 1 ], 1 ]) self . output = output class ConvPoolLayer : \"\"\" \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 \"\"\" def __init__ ( self , layer , shape = None , name = None ): \"\"\" input_layer: \u5165\u529b\u5c64(\u753b\u50cf\u30c7\u30fc\u30bf\u7fa4) shape: \u30d5\u30a3\u30eb\u30bf\u306e\u7e26\u30b5\u30a4\u30ba, \u30d5\u30a3\u30eb\u30bf\u306e\u6a2a\u30b5\u30a4\u30ba, \u5165\u529b\u30ec\u30a4\u30e4\u6570, \u51fa\u529b\u30ec\u30a4\u30e4\u6570 \"\"\" with tf . name_scope ( name ): ### \u7573\u8fbc\u307f\u5c64 with tf . name_scope ( \"convolutional_layer\" ): W = weight_variable ( shape , \"conv_layer_weights\" ) b = bias_variable ([ shape [ 3 ]], \"conv_layer_biases\" ) conv = self . conv2d ( layer . output , W , \"conv_layer\" ) + b ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 with tf . name_scope ( \"max_pooling_layer\" ): h_conv = tf . nn . relu ( conv ) h_pool = self . max_pool_2x2 ( h_conv , \"max_pooling_layer\" ) self . output = h_pool # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . histogram_summary ( \" %s _weights\" % name , W ) tf . histogram_summary ( \" %s _biases\" % name , b ) def conv2d ( self , X , W , name ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' , name = name ) def max_pool_2x2 ( self , X , name ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' , name = name ) class FullyConnectedLayer : \"\"\"\u5168\u7d50\u5408\u5c64\"\"\" def __init__ ( self , layer , shape = None , dropout_rate = None , name = None ): \"\"\" shape: \u5165\u529b\u30ec\u30a4\u30e4\u306e\u5f62\u5f0f dropout_rate: \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 \"\"\" with tf . name_scope ( name ): ### \u5168\u7d50\u5408\u5c64 with tf . name_scope ( \"fully_connected_layer\" ): W = weight_variable ( shape , \"fully_connected_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"fully_connected_layer_biases\" ) h_pool_flat = tf . reshape ( layer . output , [ - 1 , shape [ 0 ]]) h_fc = tf . nn . relu ( tf . matmul ( h_pool_flat , W ) + b ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 with tf . name_scope ( \"dropout\" ): # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc_drop = tf . nn . dropout ( h_fc , dropout_rate , name = \"dropout\" ) self . output = h_fc_drop # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . histogram_summary ( \" %s _weights\" % name , W ) tf . histogram_summary ( \" %s _biases\" % name , b ) class OutputLayer : \"\"\"\u51fa\u529b\u5c64\"\"\" def __init__ ( self , layer , shape = None , name = None ): with tf . name_scope ( name ): with tf . name_scope ( \"output_layer\" ): W = weight_variable ( shape , \"output_layer_weights\" ) b = bias_variable ([ shape [ 1 ]], \"output_layer_biases\" ) y = tf . matmul ( layer . output , W ) + b self . output = y # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . histogram_summary ( \" %s _weights\" % name , W ) tf . histogram_summary ( \" %s _biases\" % name , b ) class Optimizer : \"\"\"\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( name ): ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( output_layer . output , t ), name = \"loss\" ) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam \u5b66\u7fd2\u7387:0.0001 optimizer = tf . train . AdamOptimizer ( 1e-4 ) self . loss = loss self . train_step = optimizer . minimize ( loss ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . scalar_summary ( \"loss\" , loss ) class Evaluator : \"\"\"\u8a55\u4fa1\u5668\"\"\" def __init__ ( self , output_layer , t , name = None ): with tf . name_scope ( \"evaluator\" ): # \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( output_layer . output , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 ), name = \"accuracy\" ) # \u30ed\u30b0\u306e\u8a2d\u5b9a tf . scalar_summary ( \"accuracy\" , accuracy ) self . accuracy = accuracy if __name__ == \"__main__\" : # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) # \u5165\u529b\u30c7\u30fc\u30bf X = tf . placeholder ( tf . float32 , shape = ( None , 28 * 28 ), name = \"input_images\" ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 10 ), name = \"labels\" ) # \u5165\u529b\u5c64 input_layer = InputLayer ( X , image_shape = ( 28 , 28 ), name = \"input_layer\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer1 = ConvPoolLayer ( input_layer , shape = ( 5 , 5 , 1 , 32 ), name = \"cnv_pool_layer1\" ) # \u7573\u8fbc\u307f\u5c64 + \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 cnv_pool_layer2 = ConvPoolLayer ( cnv_pool_layer1 , shape = ( 5 , 5 , 32 , 64 ), name = \"cnv_pool_layer2\" ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387 keep_prob = tf . placeholder ( tf . float32 ) # \u5168\u7d50\u5408\u5c64 fully_connected_layer = FullyConnectedLayer ( cnv_pool_layer2 , shape = ( 7 * 7 * 64 , 1024 ), dropout_rate = keep_prob , name = \"fully_connected_layer\" ) # \u51fa\u529b\u5c64 output_layer = OutputLayer ( fully_connected_layer , shape = ( 1024 , 10 ), name = \"output_layer\" ) # \u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 optimizer = Optimizer ( output_layer , t , name = \"Optimizer\" ) # \u8a55\u4fa1 evaluator = Evaluator ( output_layer , t , name = \"Evaluator\" ) # \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u884c sess = tf . InteractiveSession () sess . run ( tf . initialize_all_variables ()) summary = tf . merge_all_summaries () writer = tf . train . SummaryWriter ( FLAGS . summary_path , sess . graph ) # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f\u3092\u884c\u3046\u30aa\u30d6\u30b8\u30a7\u30af\u30c8 saver = tf . train . Saver () i = 0 # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u306e\u51e6\u7406 if FLAGS . restore_session_path : saver . restore ( sess , FLAGS . restore_session_path ) ret = re . search ( \"-(\\d+)\" , FLAGS . restore_session_path ) i = int ( ret . group ( 1 )) for _ in range ( 20000 ): i += 1 # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u53d6\u5f97\u3059\u308b batch_x , batch_t = mnist . train . next_batch ( 50 ) # \u5b66\u7fd2\u306e\u5b9f\u884c \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387:0.5 sess . run ( optimizer . train_step , feed_dict = { X : batch_x , t : batch_t , keep_prob : 0.5 }) if i % 1000 == 0 : summary_ , train_acc , train_loss = sess . run ([ summary , evaluator . accuracy , optimizer . loss ], feed_dict = { X : batch_x , t : batch_t , keep_prob : 1.0 }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) writer . add_summary ( summary_ , i ) # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u3088\u308b\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u9069\u7528\u3057\u306a\u3044\u305f\u3081\u3001keep_prob:1.0 test_x , test_t = mnist . test . images , mnist . test . labels test_acc , test_loss = sess . run ([ evaluator . accuracy , optimizer . loss ], feed_dict = { X : test_x , t : test_t , keep_prob : 1.0 }) print \"[Test] step: %d , loss: %f , acc: %f \" % ( i , test_loss , test_acc ) # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58 saver . save ( sess , FLAGS . summary_path , global_step = i ) sess . close () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 $ python mnist_cnn . py --summary_path ./summary_log --session_path ./session_log [ Train ] step : 1000 , loss : 0.153830 , acc : 0.960000 [ Test ] step : 1000 , loss : 0.127999 , acc : 0.963500 [ Train ] step : 2000 , loss : 0.071363 , acc : 0.960000 [ Test ] step : 2000 , loss : 0.079683 , acc : 0.975100 ... \u7565 ... [ Train ] step : 19000 , loss : 0.000496 , acc : 1.000000 [ Test ] step : 19000 , loss : 0.023757 , acc : 0.992700 [ Train ] step : 20000 , loss : 0.000214 , acc : 1.000000 [ Test ] step : 20000 , loss : 0.024871 , acc : 0.992600 \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u8aad\u307f\u8fbc\u307f\u3093\u3067\u3001\u5b66\u7fd2\u3092\u518d\u958b\u3059\u308b 1 2 3 4 5 6 7 8 9 10 $ python mnist_cnn . py --summary_path ./summary_log --session_path ./session_log --restore_session_path ./summary_log-20000 [ Train ] step : 21000 , loss : 0.000479 , acc : 1.000000 [ Test ] step : 21000 , loss : 0.027168 , acc : 0.991400 [ Train ] step : 22000 , loss : 0.002611 , acc : 1.000000 [ Test ] step : 22000 , loss : 0.028818 , acc : 0.993100 ... \u7565 ... [ Train ] step : 39000 , loss : 0.000082 , acc : 1.000000 [ Test ] step : 39000 , loss : 0.030697 , acc : 0.992600 [ Train ] step : 40000 , loss : 0.000004 , acc : 1.000000 [ Test ] step : 40000 , loss : 0.028255 , acc : 0.993100","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4fdd\u5b58\u30fb\u66f8\u8fbc\u307f"},{"location":"model_mnist/tensorflow_cnn_mnist_06/","text":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data file_name = \"mnist_cnn_bn.csv\" # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) # \u5b66\u7fd2\u4e2d\u304b\u5426\u304b is_training = tf . placeholder ( tf . bool ) def input_image ( X ): \"\"\"\u5165\u529b\u5c64\"\"\" return tf . reshape ( X , [ - 1 , 28 , 28 , 1 ]) def weight_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial ) def bias_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial ) def conv2d ( X , W ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) def max_pool_2x2 ( X ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) def batch_norm ( X , axes , shape , is_training ): \"\"\" \u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u5e73\u5747\u3068\u5206\u6563\u306b\u3088\u308b\u5404\u30ec\u30a4\u30e4\u306e\u5165\u529b\u3092\u6b63\u898f\u5316(\u767d\u8272\u5316)\u3059\u308b \"\"\" if is_training is False : return X epsilon = 1e-5 # \u5e73\u5747\u3068\u5206\u6563 mean , variance = tf . nn . moments ( X , axes ) # scale\u3068offset\u3082\u5b66\u7fd2\u5bfe\u8c61 scale = tf . Variable ( tf . ones ([ shape ])) offset = tf . Variable ( tf . zeros ([ shape ])) return tf . nn . batch_normalization ( X , mean , variance , offset , scale , epsilon ) ### \u5165\u529b\u5c64 input_layer = input_image ( X ) ### \u7573\u8fbc\u307f\u5c64 1 W_conv1 = weight_variable ([ 5 , 5 , 1 , 32 ]) conv1 = conv2d ( input_layer , W_conv1 ) conv1_bn = batch_norm ( conv1 , [ 0 , 1 , 2 ], 32 , is_training ) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 1 h_conv1 = tf . nn . relu ( conv1_bn ) h_pool1 = max_pool_2x2 ( h_conv1 ) ### \u7573\u8fbc\u307f\u5c64 2 W_conv2 = weight_variable ([ 5 , 5 , 32 , 64 ]) conv2 = conv2d ( h_pool1 , W_conv2 ) conv2_bn = batch_norm ( conv2 , [ 0 , 1 , 2 ], 64 , is_training ) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 2 h_conv2 = tf . nn . relu ( conv2_bn ) h_pool2 = max_pool_2x2 ( h_conv2 ) ### \u5168\u7d50\u5408\u5c64 W_fc1 = weight_variable ([ 7 * 7 * 64 , 1024 ]) h_pool2_flat = tf . reshape ( h_pool2 , [ - 1 , 7 * 7 * 64 ]) h_pool2 = tf . matmul ( h_pool2_flat , W_fc1 ) h_pool2_bn = batch_norm ( h_pool2 , [ 0 ], 1024 , is_training ) h_fc1 = tf . nn . relu ( h_pool2_bn ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 keep_prob = tf . placeholder ( tf . float32 ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc1_drop = tf . nn . dropout ( h_fc1 , keep_prob ) ### \u51fa\u529b\u5c64 W_fc2 = weight_variable ([ 1024 , 10 ]) b_fc2 = bias_variable ([ 10 ]) y_conv = tf . matmul ( h_fc1_drop , W_fc2 ) + b_fc2 ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( y_conv , t )) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam optimizer = tf . train . AdamOptimizer () train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y_conv , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 20000 ): i += 1 batch = mnist . train . next_batch ( 50 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 , is_training : True }) if i % 100 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 , is_training : True }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) test_acc , test_loss = sess . run ([ accuracy , loss ], feed_dict = { X : mnist . test . images , t : mnist . test . labels , keep_prob : 1.0 , is_training : False }) print \"[Test] loss: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : DO : \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u306e\u307f BN : \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306e\u307f DO+BN : \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3068\u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u8a13\u7df4\u8aa4\u5dee\u306e\u30b0\u30e9\u30d5 \u65e9\u3044\u6bb5\u968e\u3067\u3001 DO \u3088\u308a\u3082 BN \u306e\u307b\u3046\u304c\u8aa4\u5dee\u304c\u5c0f\u3055\u304f\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 \u30c6\u30b9\u30c8\u7cbe\u5ea6\u306e\u30b0\u30e9\u30d5 \u5168\u4f53\u7684\u306b BN \u304c DO \u3068\u540c\u7a0b\u5ea6\u306e\u7cbe\u5ea6\u3068\u306a\u3063\u3066\u3044\u308b\u3002","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30d0\u30c3\u30c1\u6b63\u898f\u5316"},{"location":"model_mnist/tensorflow_cnn_mnist_06/#mnist","text":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 # coding:utf-8 import numpy as np import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data file_name = \"mnist_cnn_bn.csv\" # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) ### \u30e2\u30c7\u30eb\u306e\u5b9f\u88c5 # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) Nx10\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 10\u306f\u30af\u30e9\u30b9\u6570 t = tf . placeholder ( tf . float32 , shape = ( None , 10 )) # \u5165\u529b\u30c7\u30fc\u30bf Nx784\u884c\u5217 # N\u306f\u30c7\u30fc\u30bf\u6570, 28x28=784 X = tf . placeholder ( tf . float32 , shape = ( None , 784 )) # \u5b66\u7fd2\u4e2d\u304b\u5426\u304b is_training = tf . placeholder ( tf . bool ) def input_image ( X ): \"\"\"\u5165\u529b\u5c64\"\"\" return tf . reshape ( X , [ - 1 , 28 , 28 , 1 ]) def weight_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u91cd\u307f\"\"\" initial = tf . truncated_normal ( shape , stddev = 0.1 ) return tf . Variable ( initial ) def bias_variable ( shape ): \"\"\"\u521d\u671f\u5316\u6e08\u307f\u306e\u30d0\u30a4\u30a2\u30b9\"\"\" initial = tf . constant ( 0.1 , shape = shape ) return tf . Variable ( initial ) def conv2d ( X , W ): \"\"\"\u7573\u8fbc\u307f\u5c64\"\"\" return tf . nn . conv2d ( X , W , strides = [ 1 , 1 , 1 , 1 ], padding = 'SAME' ) def max_pool_2x2 ( X ): \"\"\"\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\"\"\" return tf . nn . max_pool ( X , ksize = [ 1 , 2 , 2 , 1 ], strides = [ 1 , 2 , 2 , 1 ], padding = 'SAME' ) def batch_norm ( X , axes , shape , is_training ): \"\"\" \u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u5e73\u5747\u3068\u5206\u6563\u306b\u3088\u308b\u5404\u30ec\u30a4\u30e4\u306e\u5165\u529b\u3092\u6b63\u898f\u5316(\u767d\u8272\u5316)\u3059\u308b \"\"\" if is_training is False : return X epsilon = 1e-5 # \u5e73\u5747\u3068\u5206\u6563 mean , variance = tf . nn . moments ( X , axes ) # scale\u3068offset\u3082\u5b66\u7fd2\u5bfe\u8c61 scale = tf . Variable ( tf . ones ([ shape ])) offset = tf . Variable ( tf . zeros ([ shape ])) return tf . nn . batch_normalization ( X , mean , variance , offset , scale , epsilon ) ### \u5165\u529b\u5c64 input_layer = input_image ( X ) ### \u7573\u8fbc\u307f\u5c64 1 W_conv1 = weight_variable ([ 5 , 5 , 1 , 32 ]) conv1 = conv2d ( input_layer , W_conv1 ) conv1_bn = batch_norm ( conv1 , [ 0 , 1 , 2 ], 32 , is_training ) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 1 h_conv1 = tf . nn . relu ( conv1_bn ) h_pool1 = max_pool_2x2 ( h_conv1 ) ### \u7573\u8fbc\u307f\u5c64 2 W_conv2 = weight_variable ([ 5 , 5 , 32 , 64 ]) conv2 = conv2d ( h_pool1 , W_conv2 ) conv2_bn = batch_norm ( conv2 , [ 0 , 1 , 2 ], 64 , is_training ) ### \u30d7\u30fc\u30ea\u30f3\u30b0\u5c64 2 h_conv2 = tf . nn . relu ( conv2_bn ) h_pool2 = max_pool_2x2 ( h_conv2 ) ### \u5168\u7d50\u5408\u5c64 W_fc1 = weight_variable ([ 7 * 7 * 64 , 1024 ]) h_pool2_flat = tf . reshape ( h_pool2 , [ - 1 , 7 * 7 * 64 ]) h_pool2 = tf . matmul ( h_pool2_flat , W_fc1 ) h_pool2_bn = batch_norm ( h_pool2 , [ 0 ], 1024 , is_training ) h_fc1 = tf . nn . relu ( h_pool2_bn ) ### \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8 keep_prob = tf . placeholder ( tf . float32 ) # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3059\u308b\u5272\u5408 h_fc1_drop = tf . nn . dropout ( h_fc1 , keep_prob ) ### \u51fa\u529b\u5c64 W_fc2 = weight_variable ([ 1024 , 10 ]) b_fc2 = bias_variable ([ 10 ]) y_conv = tf . matmul ( h_fc1_drop , W_fc2 ) + b_fc2 ### \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30b3\u30b9\u30c8\u95a2\u6570 loss = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( y_conv , t )) ### \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 # Adam optimizer = tf . train . AdamOptimizer () train_step = optimizer . minimize ( loss ) ### \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1 correct_prediction = tf . equal ( tf . argmax ( y_conv , 1 ), tf . argmax ( t , 1 )) # \u7cbe\u5ea6 accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) ### \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 20000 ): i += 1 batch = mnist . train . next_batch ( 50 ) sess . run ( train_step , feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 , is_training : True }) if i % 100 == 0 : train_acc , train_loss = sess . run ([ accuracy , loss ], feed_dict = { X : batch [ 0 ], t : batch [ 1 ], keep_prob : 1.0 , is_training : True }) print \"[Train] step: %d , loss: %f , acc: %f \" % ( i , train_loss , train_acc ) test_acc , test_loss = sess . run ([ accuracy , loss ], feed_dict = { X : mnist . test . images , t : mnist . test . labels , keep_prob : 1.0 , is_training : False }) print \"[Test] loss: %f , acc: %f \" % ( test_loss , test_acc ) \u5b9f\u884c\u7d50\u679c : DO : \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u306e\u307f BN : \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306e\u307f DO+BN : \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3068\u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u8a13\u7df4\u8aa4\u5dee\u306e\u30b0\u30e9\u30d5 \u65e9\u3044\u6bb5\u968e\u3067\u3001 DO \u3088\u308a\u3082 BN \u306e\u307b\u3046\u304c\u8aa4\u5dee\u304c\u5c0f\u3055\u304f\u306a\u3063\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 \u30c6\u30b9\u30c8\u7cbe\u5ea6\u306e\u30b0\u30e9\u30d5 \u5168\u4f53\u7684\u306b BN \u304c DO \u3068\u540c\u7a0b\u5ea6\u306e\u7cbe\u5ea6\u3068\u306a\u3063\u3066\u3044\u308b\u3002","title":"MNIST \u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af \u30d0\u30c3\u30c1\u6b63\u898f\u5316"},{"location":"model_mnist/tensorflow_deep_dream/","text":"DeepDream GoogLeNet\u306e\u5404\u30ec\u30a4\u30e4\u30fc\u3092\u53ef\u8996\u5316\u3059\u308b \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570 : --layer_list : GoogLeNet\u306e\u30ec\u30a4\u30e4\u30fc\u4e00\u89a7 --random_noise : \u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\u753b\u50cf\u306b\u5bfe\u3057\u3066DeepDream\u3092\u9069\u7528\u3059\u308b --layer : \u53ef\u8996\u5316\u3059\u308b\u30ec\u30a4\u30e4\u30fc --channel : \u53ef\u8996\u5316\u3059\u308b\u30ec\u30a4\u30e4\u30fc\u306e\u30c1\u30e3\u30cd\u30eb --in_img : \u5165\u529b\u753b\u50cf --out_img : \u753b\u50cf\u306e\u51fa\u529b\u5148(\u62e1\u5f35\u5b50\u306f\u7701\u7565\u3059\u308b) \u30e2\u30c7\u30eb\u304a\u3088\u3073\u30bb\u30c3\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 : 1 2 $ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip $ unzip inception5h.zip \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 # coding:utf-8 \"\"\" filename: tensorflow_deep_dream.py \"\"\" import numpy as np import PIL.Image import tensorflow as tf # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u306e\u5b9a\u7fa9 FLAGS = tf . app . flags . FLAGS tf . app . flags . DEFINE_bool ( \"layer_list\" , False , \"GoogLeNet\u306e\u30ec\u30a4\u30e4\u30fc\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b\" ) tf . app . flags . DEFINE_bool ( \"random_noise\" , False , \"\u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\u753b\u50cf\u3092\u4f7f\u7528\u3059\u308b\" ) tf . app . flags . DEFINE_string ( \"layer\" , None , \"\u753b\u50cf\u751f\u6210\u306b\u4f7f\u3046\u30ec\u30a4\u30e4\u30fc\u540d\" ) tf . app . flags . DEFINE_integer ( \"channel\" , None , \"\u753b\u50cf\u751f\u6210\u306b\u4f7f\u3046\u30ec\u30a4\u30e4\u30fc\u306e\u30c1\u30e3\u30cd\u30eb\" ) tf . app . flags . DEFINE_string ( \"in_img\" , None , \"\u5165\u529b\u753b\u50cf\u306e\u30d1\u30b9\" ) tf . app . flags . DEFINE_string ( \"out_img\" , \"deep_dream.jpg\" , \"\u51fa\u529b\u753b\u50cf\u306e\u30d1\u30b9 \u753b\u50cf\u306e\u62e1\u5f35\u5b50\u306f\u8981\u7701\u7565\" ) # \u30e2\u30c7\u30eb\u304a\u3088\u3073\u30bb\u30c3\u30b7\u30e7\u30f3\u60c5\u5831\u3092\u683c\u7d0d\u3057\u305f\u30d5\u30a1\u30a4\u30eb model_fn = 'tensorflow_inception_graph.pb' # \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u3068\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u8aad\u307f\u8fbc\u307f graph = tf . Graph () sess = tf . InteractiveSession ( graph = graph ) # pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 with tf . gfile . FastGFile ( model_fn , 'rb' ) as f : graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) # \u753b\u50cf\u3092\u683c\u7d0d\u3059\u308bTensor t_input = tf . placeholder ( np . float32 , name = 'input' ) # define the input tensor imagenet_mean = 117.0 t_preprocessed = tf . expand_dims ( t_input - imagenet_mean , 0 ) tf . import_graph_def ( graph_def , { 'input' : t_preprocessed }) # \u5404\u5c64\u306e\u540d\u524d\u3092\u51fa\u529b\u3059\u308b def show_layer_list (): \"\"\"\u30ec\u30a4\u30e4\u30fc\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b\"\"\" for op in graph . get_operations (): if op . type == 'Conv2D' and 'import/' in op . name : print op . name def read_image ( name ): \"\"\"\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u307f\u3001nparray\u306b\u5909\u63db\u3059\u308b\"\"\" img_arr = PIL . Image . open ( name ) return np . array ( img_arr , dtype = np . float32 ) def save_image ( a , name ): \"\"\"nparray\u3092\u753b\u50cf\u3068\u3057\u3066\u4fdd\u5b58\u3059\u308b\"\"\" x = np . uint8 ( np . clip ( a , 0.0 , 1.0 ) * 255.0 ) img = PIL . Image . fromarray ( x ) img . save ( name ) def visstd ( a , s = 0.1 ): \"\"\"\u753b\u50cf\u306e\u6b63\u898f\u5316\u3059\u308b\"\"\" return ( a - a . mean ()) / max ( a . std (), 1e-4 ) * s + 0.5 def T ( layer ): \"\"\"\u6307\u5b9a\u3057\u305f\u5c64\u306e\u51fa\u529bTensor\u3092\u53d6\u5f97\u3059\u308b\"\"\" return graph . get_tensor_by_name ( \"import/ %s :0\" % layer ) def render_naive ( t_obj , iter_n = 20 , step = 1.0 , name = \"noise_dream.jpg\" ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\u753b\u50cf\u306b\u5bfe\u3057\u3066Deepdream\u3092\u9069\u7528\u3059\u308b\"\"\" img0 = np . random . uniform ( size = ( 224 , 224 , 3 )) + 100.0 # \u6700\u9069\u5316\u3092\u884c\u3046Tensor t_score = tf . reduce_mean ( t_obj ) # \u81ea\u52d5\u5fae\u5206\u306e\u7d2f\u4e57 t_grad = tf . gradients ( t_score , t_input )[ 0 ] img = img0 . copy () for i in range ( iter_n ): g , score = sess . run ([ t_grad , t_score ], { t_input : img }) # \u52fe\u914d\u3092\u6b63\u898f\u5316\u3059\u308b g /= g . std () + 1e-8 img += g * step save_image ( visstd ( img ), name ) def tffunc ( * argtypes ): \"\"\"TF\u30b0\u30e9\u30d5\u95a2\u6570\u306e\u30d8\u30eb\u30d1\u30fc\u95a2\u6570\"\"\" placeholders = list ( map ( tf . placeholder , argtypes )) def wrap ( f ): out = f ( * placeholders ) def wrapper ( * args , ** kw ): return out . eval ( dict ( zip ( placeholders , args )), session = kw . get ( 'session' )) return wrapper return wrap def resize ( img , size ): \"\"\"TensorFlow\u306b\u3088\u3063\u3066\u753b\u50cf\u3092\u30ea\u30b5\u30a4\u30ba\u3059\u308b\"\"\" img = tf . expand_dims ( img , 0 ) return tf . image . resize_bilinear ( img , size )[ 0 ,:,:,:] # tf\u95a2\u6570\u5316 resize = tffunc ( np . float32 , np . int32 )( resize ) def calc_grad_tiled ( img , t_grad , tile_size = 512 ): \"\"\"\u5165\u529b\u753b\u50cf\u306b\u5bfe\u3059\u308b\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\"\"\" sz = tile_size h , w = img . shape [: 2 ] sx , sy = np . random . randint ( sz , size = 2 ) # \u30e9\u30f3\u30c0\u30e0\u306b\u914d\u5217\u306e\u8981\u7d20\u3092\u56de\u8ee2\u3055\u305b\u308b img_shift = np . roll ( np . roll ( img , sx , 1 ), sy , 0 ) grad = np . zeros_like ( img ) for y in range ( 0 , max ( h - sz // 2 , sz ), sz ): for x in range ( 0 , max ( w - sz // 2 , sz ), sz ): sub = img_shift [ y : y + sz , x : x + sz ] g = sess . run ( t_grad , { t_input : sub }) grad [ y : y + sz , x : x + sz ] = g return np . roll ( np . roll ( grad , - sx , 1 ), - sy , 0 ) def render_deepdream ( t_obj , img , iter_n = 10 , step = 1.5 , octave_n = 4 , octave_scale = 1.4 , name = \"deep_dream\" ): \"\"\"\u30ce\u30a4\u30ba\u753b\u50cf\u306b\u5bfe\u3057\u3066 Deep dream\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\"\"\" # \u6700\u9069\u5316\u3092\u884c\u3046Tensor t_score = tf . reduce_mean ( t_obj ) # Tensor\u306e\u52fe\u914d t_grad = tf . gradients ( t_score , t_input )[ 0 ] octaves = [] for i in range ( octave_n - 1 ): hw = img . shape [: 2 ] lo = resize ( img , np . int32 ( np . float32 ( hw ) / octave_scale )) hi = img - resize ( lo , hw ) img = lo octaves . append ( hi ) # \u30aa\u30af\u30bf\u30fc\u30d6\u3054\u3068\u306b\u753b\u50cf\u3092\u751f\u6210\u3059\u308b for octave in range ( octave_n ): if octave > 0 : hi = octaves [ - octave ] img = resize ( img , hi . shape [: 2 ]) + hi for i in range ( iter_n ): g = calc_grad_tiled ( img , t_grad ) img += g * ( step / ( np . abs ( g ) . mean () + 1e-7 )) save_image ( img / 255.0 , \" %s _ %i .jpg\" % ( name , octave )) if __name__ == \"__main__\" : output_path = FLAGS . out_img if FLAGS . layer_list : show_layer_list () quit () if FLAGS . random_noise : layer = \"mixed4d_3x3_bottleneck_pre_relu\" tensor = T ( layer )[:,:,:, 139 ] render_naive ( tensor , name = output_path ) quit () if FLAGS . in_img : # \u5165\u529b\u753b\u50cf img = read_image ( FLAGS . in_img ) tensor = tf . square ( T ( 'mixed4c' )) if FLAGS . layer and FLAGS . channel : tensor = T ( FLAGS . layer )[:,:,:, FLAGS . channel ] render_deepdream ( tensor , img , name = output_path ) python tensorflow_deep_dream.py --layer_list \u306e\u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import/conv2d0_pre_relu/conv import/conv2d1_pre_relu/conv import/conv2d2_pre_relu/conv import/mixed3a_1x1_pre_relu/conv import/mixed3a_3x3_bottleneck_pre_relu/conv import/mixed3a_3x3_pre_relu/conv import/mixed3a_5x5_bottleneck_pre_relu/conv import/mixed3a_5x5_pre_relu/conv ...\u7565... import/mixed5b_1x1_pre_relu/conv import/mixed5b_3x3_bottleneck_pre_relu/conv import/mixed5b_3x3_pre_relu/conv import/mixed5b_5x5_bottleneck_pre_relu/conv import/mixed5b_5x5_pre_relu/conv import/mixed5b_pool_reduce_pre_relu/conv import/head0_bottleneck_pre_relu/conv import/head1_bottleneck_pre_relu/conv python tensorflow_deep_dream.py --random_noise \u306e\u5b9f\u884c\u7d50\u679c : \u5143\u753b\u50cf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 : 1 $ wget https://github.com/FaBoPlatform/TensorFlow/raw/master/img/scenery.jpgg python tensorflow_deep_dream.py --in_img scenery.jpg --out_img scenery_deep \u306e\u5b9f\u884c\u7d50\u679c : \u5143\u753b\u50cf Deepdream\u3092\u9069\u7528\u3057\u305f\u753b\u50cf python tensorflow_deep_dream.py --in_img scenery.jpg --out_img scenery_deep --layer mixed3b_1x1_pre_relu --channel 101 \u306e\u5b9f\u884c\u7d50\u679c : \u53c2\u8003 DeepDreaming with TensorFlow","title":"DeepDream"},{"location":"model_mnist/tensorflow_deep_dream/#deepdream","text":"GoogLeNet\u306e\u5404\u30ec\u30a4\u30e4\u30fc\u3092\u53ef\u8996\u5316\u3059\u308b \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570 : --layer_list : GoogLeNet\u306e\u30ec\u30a4\u30e4\u30fc\u4e00\u89a7 --random_noise : \u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\u753b\u50cf\u306b\u5bfe\u3057\u3066DeepDream\u3092\u9069\u7528\u3059\u308b --layer : \u53ef\u8996\u5316\u3059\u308b\u30ec\u30a4\u30e4\u30fc --channel : \u53ef\u8996\u5316\u3059\u308b\u30ec\u30a4\u30e4\u30fc\u306e\u30c1\u30e3\u30cd\u30eb --in_img : \u5165\u529b\u753b\u50cf --out_img : \u753b\u50cf\u306e\u51fa\u529b\u5148(\u62e1\u5f35\u5b50\u306f\u7701\u7565\u3059\u308b) \u30e2\u30c7\u30eb\u304a\u3088\u3073\u30bb\u30c3\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 : 1 2 $ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip $ unzip inception5h.zip \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 # coding:utf-8 \"\"\" filename: tensorflow_deep_dream.py \"\"\" import numpy as np import PIL.Image import tensorflow as tf # \u7d50\u679c\u304c\u540c\u3058\u306b\u306a\u308b\u3088\u3046\u306b\u3001\u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b tf . set_random_seed ( 20200724 ) np . random . seed ( 20200724 ) # \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u306e\u5b9a\u7fa9 FLAGS = tf . app . flags . FLAGS tf . app . flags . DEFINE_bool ( \"layer_list\" , False , \"GoogLeNet\u306e\u30ec\u30a4\u30e4\u30fc\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b\" ) tf . app . flags . DEFINE_bool ( \"random_noise\" , False , \"\u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\u753b\u50cf\u3092\u4f7f\u7528\u3059\u308b\" ) tf . app . flags . DEFINE_string ( \"layer\" , None , \"\u753b\u50cf\u751f\u6210\u306b\u4f7f\u3046\u30ec\u30a4\u30e4\u30fc\u540d\" ) tf . app . flags . DEFINE_integer ( \"channel\" , None , \"\u753b\u50cf\u751f\u6210\u306b\u4f7f\u3046\u30ec\u30a4\u30e4\u30fc\u306e\u30c1\u30e3\u30cd\u30eb\" ) tf . app . flags . DEFINE_string ( \"in_img\" , None , \"\u5165\u529b\u753b\u50cf\u306e\u30d1\u30b9\" ) tf . app . flags . DEFINE_string ( \"out_img\" , \"deep_dream.jpg\" , \"\u51fa\u529b\u753b\u50cf\u306e\u30d1\u30b9 \u753b\u50cf\u306e\u62e1\u5f35\u5b50\u306f\u8981\u7701\u7565\" ) # \u30e2\u30c7\u30eb\u304a\u3088\u3073\u30bb\u30c3\u30b7\u30e7\u30f3\u60c5\u5831\u3092\u683c\u7d0d\u3057\u305f\u30d5\u30a1\u30a4\u30eb model_fn = 'tensorflow_inception_graph.pb' # \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u3068\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u8aad\u307f\u8fbc\u307f graph = tf . Graph () sess = tf . InteractiveSession ( graph = graph ) # pb\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080 with tf . gfile . FastGFile ( model_fn , 'rb' ) as f : graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) # \u753b\u50cf\u3092\u683c\u7d0d\u3059\u308bTensor t_input = tf . placeholder ( np . float32 , name = 'input' ) # define the input tensor imagenet_mean = 117.0 t_preprocessed = tf . expand_dims ( t_input - imagenet_mean , 0 ) tf . import_graph_def ( graph_def , { 'input' : t_preprocessed }) # \u5404\u5c64\u306e\u540d\u524d\u3092\u51fa\u529b\u3059\u308b def show_layer_list (): \"\"\"\u30ec\u30a4\u30e4\u30fc\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b\"\"\" for op in graph . get_operations (): if op . type == 'Conv2D' and 'import/' in op . name : print op . name def read_image ( name ): \"\"\"\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u307f\u3001nparray\u306b\u5909\u63db\u3059\u308b\"\"\" img_arr = PIL . Image . open ( name ) return np . array ( img_arr , dtype = np . float32 ) def save_image ( a , name ): \"\"\"nparray\u3092\u753b\u50cf\u3068\u3057\u3066\u4fdd\u5b58\u3059\u308b\"\"\" x = np . uint8 ( np . clip ( a , 0.0 , 1.0 ) * 255.0 ) img = PIL . Image . fromarray ( x ) img . save ( name ) def visstd ( a , s = 0.1 ): \"\"\"\u753b\u50cf\u306e\u6b63\u898f\u5316\u3059\u308b\"\"\" return ( a - a . mean ()) / max ( a . std (), 1e-4 ) * s + 0.5 def T ( layer ): \"\"\"\u6307\u5b9a\u3057\u305f\u5c64\u306e\u51fa\u529bTensor\u3092\u53d6\u5f97\u3059\u308b\"\"\" return graph . get_tensor_by_name ( \"import/ %s :0\" % layer ) def render_naive ( t_obj , iter_n = 20 , step = 1.0 , name = \"noise_dream.jpg\" ): \"\"\"\u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\u753b\u50cf\u306b\u5bfe\u3057\u3066Deepdream\u3092\u9069\u7528\u3059\u308b\"\"\" img0 = np . random . uniform ( size = ( 224 , 224 , 3 )) + 100.0 # \u6700\u9069\u5316\u3092\u884c\u3046Tensor t_score = tf . reduce_mean ( t_obj ) # \u81ea\u52d5\u5fae\u5206\u306e\u7d2f\u4e57 t_grad = tf . gradients ( t_score , t_input )[ 0 ] img = img0 . copy () for i in range ( iter_n ): g , score = sess . run ([ t_grad , t_score ], { t_input : img }) # \u52fe\u914d\u3092\u6b63\u898f\u5316\u3059\u308b g /= g . std () + 1e-8 img += g * step save_image ( visstd ( img ), name ) def tffunc ( * argtypes ): \"\"\"TF\u30b0\u30e9\u30d5\u95a2\u6570\u306e\u30d8\u30eb\u30d1\u30fc\u95a2\u6570\"\"\" placeholders = list ( map ( tf . placeholder , argtypes )) def wrap ( f ): out = f ( * placeholders ) def wrapper ( * args , ** kw ): return out . eval ( dict ( zip ( placeholders , args )), session = kw . get ( 'session' )) return wrapper return wrap def resize ( img , size ): \"\"\"TensorFlow\u306b\u3088\u3063\u3066\u753b\u50cf\u3092\u30ea\u30b5\u30a4\u30ba\u3059\u308b\"\"\" img = tf . expand_dims ( img , 0 ) return tf . image . resize_bilinear ( img , size )[ 0 ,:,:,:] # tf\u95a2\u6570\u5316 resize = tffunc ( np . float32 , np . int32 )( resize ) def calc_grad_tiled ( img , t_grad , tile_size = 512 ): \"\"\"\u5165\u529b\u753b\u50cf\u306b\u5bfe\u3059\u308b\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\"\"\" sz = tile_size h , w = img . shape [: 2 ] sx , sy = np . random . randint ( sz , size = 2 ) # \u30e9\u30f3\u30c0\u30e0\u306b\u914d\u5217\u306e\u8981\u7d20\u3092\u56de\u8ee2\u3055\u305b\u308b img_shift = np . roll ( np . roll ( img , sx , 1 ), sy , 0 ) grad = np . zeros_like ( img ) for y in range ( 0 , max ( h - sz // 2 , sz ), sz ): for x in range ( 0 , max ( w - sz // 2 , sz ), sz ): sub = img_shift [ y : y + sz , x : x + sz ] g = sess . run ( t_grad , { t_input : sub }) grad [ y : y + sz , x : x + sz ] = g return np . roll ( np . roll ( grad , - sx , 1 ), - sy , 0 ) def render_deepdream ( t_obj , img , iter_n = 10 , step = 1.5 , octave_n = 4 , octave_scale = 1.4 , name = \"deep_dream\" ): \"\"\"\u30ce\u30a4\u30ba\u753b\u50cf\u306b\u5bfe\u3057\u3066 Deep dream\u753b\u50cf\u3092\u751f\u6210\u3059\u308b\"\"\" # \u6700\u9069\u5316\u3092\u884c\u3046Tensor t_score = tf . reduce_mean ( t_obj ) # Tensor\u306e\u52fe\u914d t_grad = tf . gradients ( t_score , t_input )[ 0 ] octaves = [] for i in range ( octave_n - 1 ): hw = img . shape [: 2 ] lo = resize ( img , np . int32 ( np . float32 ( hw ) / octave_scale )) hi = img - resize ( lo , hw ) img = lo octaves . append ( hi ) # \u30aa\u30af\u30bf\u30fc\u30d6\u3054\u3068\u306b\u753b\u50cf\u3092\u751f\u6210\u3059\u308b for octave in range ( octave_n ): if octave > 0 : hi = octaves [ - octave ] img = resize ( img , hi . shape [: 2 ]) + hi for i in range ( iter_n ): g = calc_grad_tiled ( img , t_grad ) img += g * ( step / ( np . abs ( g ) . mean () + 1e-7 )) save_image ( img / 255.0 , \" %s _ %i .jpg\" % ( name , octave )) if __name__ == \"__main__\" : output_path = FLAGS . out_img if FLAGS . layer_list : show_layer_list () quit () if FLAGS . random_noise : layer = \"mixed4d_3x3_bottleneck_pre_relu\" tensor = T ( layer )[:,:,:, 139 ] render_naive ( tensor , name = output_path ) quit () if FLAGS . in_img : # \u5165\u529b\u753b\u50cf img = read_image ( FLAGS . in_img ) tensor = tf . square ( T ( 'mixed4c' )) if FLAGS . layer and FLAGS . channel : tensor = T ( FLAGS . layer )[:,:,:, FLAGS . channel ] render_deepdream ( tensor , img , name = output_path ) python tensorflow_deep_dream.py --layer_list \u306e\u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import/conv2d0_pre_relu/conv import/conv2d1_pre_relu/conv import/conv2d2_pre_relu/conv import/mixed3a_1x1_pre_relu/conv import/mixed3a_3x3_bottleneck_pre_relu/conv import/mixed3a_3x3_pre_relu/conv import/mixed3a_5x5_bottleneck_pre_relu/conv import/mixed3a_5x5_pre_relu/conv ...\u7565... import/mixed5b_1x1_pre_relu/conv import/mixed5b_3x3_bottleneck_pre_relu/conv import/mixed5b_3x3_pre_relu/conv import/mixed5b_5x5_bottleneck_pre_relu/conv import/mixed5b_5x5_pre_relu/conv import/mixed5b_pool_reduce_pre_relu/conv import/head0_bottleneck_pre_relu/conv import/head1_bottleneck_pre_relu/conv python tensorflow_deep_dream.py --random_noise \u306e\u5b9f\u884c\u7d50\u679c : \u5143\u753b\u50cf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 : 1 $ wget https://github.com/FaBoPlatform/TensorFlow/raw/master/img/scenery.jpgg python tensorflow_deep_dream.py --in_img scenery.jpg --out_img scenery_deep \u306e\u5b9f\u884c\u7d50\u679c : \u5143\u753b\u50cf Deepdream\u3092\u9069\u7528\u3057\u305f\u753b\u50cf python tensorflow_deep_dream.py --in_img scenery.jpg --out_img scenery_deep --layer mixed3b_1x1_pre_relu --channel 101 \u306e\u5b9f\u884c\u7d50\u679c :","title":"DeepDream"},{"location":"model_mnist/tensorflow_deep_dream/#_1","text":"DeepDreaming with TensorFlow","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_fitting/","text":"\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0 \u7bc4\u56f2-1\u301c1\u306ecos\u95a2\u6570\u306e\u5024\u306b\u6b63\u898f\u5206\u5e03\u306e\u4e71\u6570\u306b\u3088\u308b\u30ce\u30a4\u30ba\u3092\u6df7\u305c\u305f\u30c7\u30fc\u30bf\u3092\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf def multiplier_list ( v , n ): \"\"\" \u5f15\u6570\u30920\u4e57\u304b\u3089n\u4e57\u3057\u305f\u30ea\u30b9\u30c8\u3092\u8fd4\u3059 \u4f8b: multiplier_list(2, 4) [1, 2, 4, 8] \"\"\" return [ v ** i for i in range ( 0 , n )] # 9\u6b21\u591a\u9805\u5f0f\u306b\u3088\u308b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0 dim = 9 x_data = np . arange ( - 1.0 , 1.0 , 0.01 ) formed_x_data = np . array ([ multiplier_list ( x , dim ) for x in x_data ]) # \u30ce\u30a4\u30ba bias = np . random . normal ( scale = 0.2 , size = x_data . shape ) # cos\u95a2\u6570\u306b\u30ce\u30a4\u30ba\u3092\u306e\u305b\u308b y_data = y = np . cos ( 2. * np . pi * x_data ) + bias # \u7e26\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db formed_y_data = y_data [:, np . newaxis ] # N x dim \u884c\u5217(N\u306f\u30c7\u30fc\u30bf\u6570) X = tf . placeholder ( tf . float32 , shape = ( None , dim )) # dim x 1 \u884c\u5217 w = tf . Variable ( tf . zeros ([ dim , 1 ])) # 9\u6b21\u591a\u9805\u5f0f y = tf . matmul ( X , w ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # \u8aa4\u5dee\u95a2\u6570(\u4e8c\u4e57\u8aa4\u5dee) loss = tf . reduce_mean ( tf . square ( y - t )) # \u52fe\u914d\u964d\u4e0b\u6cd5(\u5b66\u7fd2\u7387:0.15) optimizer = tf . train . GradientDescentOptimizer ( 0.15 ) train_step = optimizer . minimize ( loss ) with tf . Session () as sess : sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 1000 ): i += 1 sess . run ( train_step , feed_dict = { X : formed_x_data , t : formed_y_data }) if i % 200 == 0 : train_loss = sess . run ( loss , feed_dict = { X : formed_x_data , t : formed_y_data }) print \"[Train] step: %d , loss: %f \" % ( i , train_loss ) # \u4e88\u6e2c\u5024\u306e\u30d7\u30ed\u30c3\u30c8 predict_y = sess . run ( y , feed_dict = { X : formed_x_data }) plt . plot ( x_data , predict_y , label = \"STEP %d \" % i ) # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8 plt . scatter ( x_data , y_data ) plt . legend () plt . show () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 [ Train ] step : 200 , loss : 0.302734 [ Train ] step : 400 , loss : 0.265032 [ Train ] step : 600 , loss : 0.250256 [ Train ] step : 800 , loss : 0.240955 [ Train ] step : 1000 , loss : 0.233153 \u30b0\u30e9\u30d5 : \u5b66\u7fd2\u3092\u7e70\u308a\u8fd4\u3059\u3054\u3068\u306b\u5143\u306e\u30c7\u30fc\u30bf\u306b\u8fd1\u3065\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b \u53c2\u8003 \u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306e\u884c\u5217\u5f62\u5f0f","title":"\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0"},{"location":"model_mnist/tensorflow_fitting/#_1","text":"\u7bc4\u56f2-1\u301c1\u306ecos\u95a2\u6570\u306e\u5024\u306b\u6b63\u898f\u5206\u5e03\u306e\u4e71\u6570\u306b\u3088\u308b\u30ce\u30a4\u30ba\u3092\u6df7\u305c\u305f\u30c7\u30fc\u30bf\u3092\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf def multiplier_list ( v , n ): \"\"\" \u5f15\u6570\u30920\u4e57\u304b\u3089n\u4e57\u3057\u305f\u30ea\u30b9\u30c8\u3092\u8fd4\u3059 \u4f8b: multiplier_list(2, 4) [1, 2, 4, 8] \"\"\" return [ v ** i for i in range ( 0 , n )] # 9\u6b21\u591a\u9805\u5f0f\u306b\u3088\u308b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0 dim = 9 x_data = np . arange ( - 1.0 , 1.0 , 0.01 ) formed_x_data = np . array ([ multiplier_list ( x , dim ) for x in x_data ]) # \u30ce\u30a4\u30ba bias = np . random . normal ( scale = 0.2 , size = x_data . shape ) # cos\u95a2\u6570\u306b\u30ce\u30a4\u30ba\u3092\u306e\u305b\u308b y_data = y = np . cos ( 2. * np . pi * x_data ) + bias # \u7e26\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db formed_y_data = y_data [:, np . newaxis ] # N x dim \u884c\u5217(N\u306f\u30c7\u30fc\u30bf\u6570) X = tf . placeholder ( tf . float32 , shape = ( None , dim )) # dim x 1 \u884c\u5217 w = tf . Variable ( tf . zeros ([ dim , 1 ])) # 9\u6b21\u591a\u9805\u5f0f y = tf . matmul ( X , w ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 1 )) # \u8aa4\u5dee\u95a2\u6570(\u4e8c\u4e57\u8aa4\u5dee) loss = tf . reduce_mean ( tf . square ( y - t )) # \u52fe\u914d\u964d\u4e0b\u6cd5(\u5b66\u7fd2\u7387:0.15) optimizer = tf . train . GradientDescentOptimizer ( 0.15 ) train_step = optimizer . minimize ( loss ) with tf . Session () as sess : sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 1000 ): i += 1 sess . run ( train_step , feed_dict = { X : formed_x_data , t : formed_y_data }) if i % 200 == 0 : train_loss = sess . run ( loss , feed_dict = { X : formed_x_data , t : formed_y_data }) print \"[Train] step: %d , loss: %f \" % ( i , train_loss ) # \u4e88\u6e2c\u5024\u306e\u30d7\u30ed\u30c3\u30c8 predict_y = sess . run ( y , feed_dict = { X : formed_x_data }) plt . plot ( x_data , predict_y , label = \"STEP %d \" % i ) # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8 plt . scatter ( x_data , y_data ) plt . legend () plt . show () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 [ Train ] step : 200 , loss : 0.302734 [ Train ] step : 400 , loss : 0.265032 [ Train ] step : 600 , loss : 0.250256 [ Train ] step : 800 , loss : 0.240955 [ Train ] step : 1000 , loss : 0.233153 \u30b0\u30e9\u30d5 : \u5b66\u7fd2\u3092\u7e70\u308a\u8fd4\u3059\u3054\u3068\u306b\u5143\u306e\u30c7\u30fc\u30bf\u306b\u8fd1\u3065\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b","title":"\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0"},{"location":"model_mnist/tensorflow_fitting/#_2","text":"\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306e\u884c\u5217\u5f62\u5f0f","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_logistic_regression_last2/","text":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30 2\u3064\u306e\u6b63\u898f\u5206\u5e03\u3092\u4f7f\u3044\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u304a\u3053\u306a\u3046\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding:utf-8 import tensorflow as tf import numpy as np x1 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , - 2 ]) x2 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , 2 ]) X = np . vstack ([ x1 , x2 ]) . astype ( np . float32 ) Y = np . array ([ 0 ] * 500 + [ 1 ] * 500 ) N = len ( Y ) T = np . zeros (( N , 2 )) for i in xrange ( N ): T [ i , Y [ i ]] = 1 tfX = tf . placeholder ( tf . float32 , [ None , 2 ]) tfY = tf . placeholder ( tf . float32 , [ None , 2 ]) W = tf . Variable ( tf . random_normal ([ 2 , 2 ], stddev = 0.01 ), name = \"weight\" ) b = tf . Variable ( tf . random_normal ([ 2 ], stddev = 0.01 ), name = \"bias\" ) learning_rate = 0.01 activation = tf . nn . softmax ( tf . matmul ( tfX , W ) + b ) init_op = tf . initialize_all_variables () cost = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( activation , T )) train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) correct_prediction = tf . equal ( tf . argmax ( activation , 1 ), tf . argmax ( T , 1 )) accuracy_op = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) with tf . Session () as sess : sess . run ( init_op ) for i in xrange ( 1000 ): sess . run ( train_op , feed_dict = { tfX : X , tfY : T }) if i % 100 == 0 : a , c = sess . run ([ accuracy_op , cost ], feed_dict = { tfX : X , tfY : T }) print \"step %d , cost %f , accuracy %f \" % ( i , c , a ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 step 0, cost 1380.991089, predict 0.685000, accuracy 0.685000 step 100, cost 992.892090, predict 0.980000, accuracy 0.980000 step 200, cost 769.331665, predict 0.982000, accuracy 0.982000 step 300, cost 634.025146, predict 0.983000, accuracy 0.983000 step 400, cost 545.197266, predict 0.983000, accuracy 0.983000 step 500, cost 482.741455, predict 0.983000, accuracy 0.983000 step 600, cost 436.451721, predict 0.983000, accuracy 0.983000 step 700, cost 400.731537, predict 0.983000, accuracy 0.983000 step 800, cost 372.287750, predict 0.983000, accuracy 0.983000 step 900, cost 349.065369, predict 0.983000, accuracy 0.983000 \u53c2\u8003 Breast Cancer Wisconsin (Prognostic) Data Set","title":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30"},{"location":"model_mnist/tensorflow_logistic_regression_last2/#_1","text":"2\u3064\u306e\u6b63\u898f\u5206\u5e03\u3092\u4f7f\u3044\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u3092\u304a\u3053\u306a\u3046\u3002","title":"\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30"},{"location":"model_mnist/tensorflow_logistic_regression_last2/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding:utf-8 import tensorflow as tf import numpy as np x1 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , - 2 ]) x2 = np . random . randn ( 500 , 2 ) + np . array ([ 0 , 2 ]) X = np . vstack ([ x1 , x2 ]) . astype ( np . float32 ) Y = np . array ([ 0 ] * 500 + [ 1 ] * 500 ) N = len ( Y ) T = np . zeros (( N , 2 )) for i in xrange ( N ): T [ i , Y [ i ]] = 1 tfX = tf . placeholder ( tf . float32 , [ None , 2 ]) tfY = tf . placeholder ( tf . float32 , [ None , 2 ]) W = tf . Variable ( tf . random_normal ([ 2 , 2 ], stddev = 0.01 ), name = \"weight\" ) b = tf . Variable ( tf . random_normal ([ 2 ], stddev = 0.01 ), name = \"bias\" ) learning_rate = 0.01 activation = tf . nn . softmax ( tf . matmul ( tfX , W ) + b ) init_op = tf . initialize_all_variables () cost = tf . reduce_mean ( tf . nn . softmax_cross_entropy_with_logits ( activation , T )) train_op = tf . train . GradientDescentOptimizer ( learning_rate ) . minimize ( cost ) correct_prediction = tf . equal ( tf . argmax ( activation , 1 ), tf . argmax ( T , 1 )) accuracy_op = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) with tf . Session () as sess : sess . run ( init_op ) for i in xrange ( 1000 ): sess . run ( train_op , feed_dict = { tfX : X , tfY : T }) if i % 100 == 0 : a , c = sess . run ([ accuracy_op , cost ], feed_dict = { tfX : X , tfY : T }) print \"step %d , cost %f , accuracy %f \" % ( i , c , a ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 step 0, cost 1380.991089, predict 0.685000, accuracy 0.685000 step 100, cost 992.892090, predict 0.980000, accuracy 0.980000 step 200, cost 769.331665, predict 0.982000, accuracy 0.982000 step 300, cost 634.025146, predict 0.983000, accuracy 0.983000 step 400, cost 545.197266, predict 0.983000, accuracy 0.983000 step 500, cost 482.741455, predict 0.983000, accuracy 0.983000 step 600, cost 436.451721, predict 0.983000, accuracy 0.983000 step 700, cost 400.731537, predict 0.983000, accuracy 0.983000 step 800, cost 372.287750, predict 0.983000, accuracy 0.983000 step 900, cost 349.065369, predict 0.983000, accuracy 0.983000","title":"Sample"},{"location":"model_mnist/tensorflow_logistic_regression_last2/#_2","text":"Breast Cancer Wisconsin (Prognostic) Data Set","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_math_func/","text":"\u57fa\u672c\u7684\u306a\u6570\u5b66\u95a2\u6570 \u57fa\u672c\u7684\u306a\u6570\u5b66\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u4ed6\u306e\u95a2\u6570\u306f\u3001 Tensorflow\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306eBasic Math Functions \u3092\u53c2\u8003\u3002 \u95a2\u6570\u540d \u8aac\u660e tf.abs(x) \u7d76\u5bfe\u5024 tf.sign(x) \u7b26\u53f7\u95a2\u6570 tf.pow(x, y) n\u4e57\u95a2\u6570 tf.exp(x) \u6307\u6570\u95a2\u6570 tf.log(x) \u5bfe\u6570\u95a2\u6570 tf.sin(x) \u6b63\u5f26\u95a2\u6570 tf.maximum(x, y) \u6bd4\u8f03\u3057\u3066\u5927\u304d\u3044\u5024\u3092\u8fd4\u3059 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # coding:utf-8 import numpy as np import tensorflow as tf x_data = np . arange ( - 4.0 , 5.0 ) . reshape ( 3 , 3 ) x = tf . constant ( x_data , tf . float32 , shape = ( 3 , 3 )) # [[-4. -3. -2.] # [-1. 0. 1.] # [ 2. 3. 4.]] with tf . Session () as sess : print sess . run ( x ) # \u7d76\u5bfe\u5024 print sess . run ( tf . abs ( x )) # \u7b26\u53f7\u95a2\u6570 sign function print sess . run ( tf . sign ( x )) # n\u4e57 print sess . run ( tf . pow ( x , 1 )) # \u6307\u6570\u95a2\u6570 print sess . run ( tf . exp ( x )) # \u5bfe\u6570\u95a2\u6570 print sess . run ( tf . log ( x )) # \u6b63\u5f26\u95a2\u6570 print sess . run ( tf . sin ( x )) # \u5024\u3092\u6bd4\u8f03\u3057\u3066\u5927\u304d\u3044\u65b9\u306e\u5024\u3092\u8fd4\u3059 print sess . run ( tf . maximum ( x , 2 )) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [[-4. -3. -2.] [-1. 0. 1.] [ 2. 3. 4.]] [[ 4. 3. 2.] [ 1. 0. 1.] [ 2. 3. 4.]] [[-1. -1. -1.] [-1. 0. 1.] [ 1. 1. 1.]] [[ 16. 9. 4.] [ 1. 0. 1.] [ 4. 9. 16.]] [[ 1.83156393e-02 4.97870669e-02 1.35335281e-01] [ 3.67879450e-01 1.00000000e+00 2.71828175e+00] [ 7.38905621e+00 2.00855370e+01 5.45981483e+01]] [[ nan nan nan] [ nan -inf 0. ] [ 0.69314718 1.09861231 1.38629436]] [[ 0.7568025 -0.14112 -0.90929741] [-0.84147096 0. 0.84147096] [ 0.90929741 0.14112 -0.7568025 ]] [[ 1. 1. 1.] [ 1. 1. 1.] [ 2. 3. 4.] \u53c2\u8003 https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html","title":"\u57fa\u672c\u7684\u306a\u6570\u5b66\u95a2\u6570"},{"location":"model_mnist/tensorflow_math_func/#_1","text":"\u57fa\u672c\u7684\u306a\u6570\u5b66\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u4ed6\u306e\u95a2\u6570\u306f\u3001 Tensorflow\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306eBasic Math Functions \u3092\u53c2\u8003\u3002 \u95a2\u6570\u540d \u8aac\u660e tf.abs(x) \u7d76\u5bfe\u5024 tf.sign(x) \u7b26\u53f7\u95a2\u6570 tf.pow(x, y) n\u4e57\u95a2\u6570 tf.exp(x) \u6307\u6570\u95a2\u6570 tf.log(x) \u5bfe\u6570\u95a2\u6570 tf.sin(x) \u6b63\u5f26\u95a2\u6570 tf.maximum(x, y) \u6bd4\u8f03\u3057\u3066\u5927\u304d\u3044\u5024\u3092\u8fd4\u3059 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # coding:utf-8 import numpy as np import tensorflow as tf x_data = np . arange ( - 4.0 , 5.0 ) . reshape ( 3 , 3 ) x = tf . constant ( x_data , tf . float32 , shape = ( 3 , 3 )) # [[-4. -3. -2.] # [-1. 0. 1.] # [ 2. 3. 4.]] with tf . Session () as sess : print sess . run ( x ) # \u7d76\u5bfe\u5024 print sess . run ( tf . abs ( x )) # \u7b26\u53f7\u95a2\u6570 sign function print sess . run ( tf . sign ( x )) # n\u4e57 print sess . run ( tf . pow ( x , 1 )) # \u6307\u6570\u95a2\u6570 print sess . run ( tf . exp ( x )) # \u5bfe\u6570\u95a2\u6570 print sess . run ( tf . log ( x )) # \u6b63\u5f26\u95a2\u6570 print sess . run ( tf . sin ( x )) # \u5024\u3092\u6bd4\u8f03\u3057\u3066\u5927\u304d\u3044\u65b9\u306e\u5024\u3092\u8fd4\u3059 print sess . run ( tf . maximum ( x , 2 )) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [[-4. -3. -2.] [-1. 0. 1.] [ 2. 3. 4.]] [[ 4. 3. 2.] [ 1. 0. 1.] [ 2. 3. 4.]] [[-1. -1. -1.] [-1. 0. 1.] [ 1. 1. 1.]] [[ 16. 9. 4.] [ 1. 0. 1.] [ 4. 9. 16.]] [[ 1.83156393e-02 4.97870669e-02 1.35335281e-01] [ 3.67879450e-01 1.00000000e+00 2.71828175e+00] [ 7.38905621e+00 2.00855370e+01 5.45981483e+01]] [[ nan nan nan] [ nan -inf 0. ] [ 0.69314718 1.09861231 1.38629436]] [[ 0.7568025 -0.14112 -0.90929741] [-0.84147096 0. 0.84147096] [ 0.90929741 0.14112 -0.7568025 ]] [[ 1. 1. 1.] [ 1. 1. 1.] [ 2. 3. 4.]","title":"\u57fa\u672c\u7684\u306a\u6570\u5b66\u95a2\u6570"},{"location":"model_mnist/tensorflow_math_func/#_2","text":"https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_matrix_func/","text":"\u884c\u5217\u95a2\u6570 \u884c\u5217\u306e\u8a08\u7b97\u3092\u884c\u3046\u6570\u5b66\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u8a73\u3057\u304f\u306f Matrix Math Functions \u3092\u53c2\u8003\u3002 \u95a2\u6570\u540d \u8aac\u660e tf.matrix_diag(x) \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u3059\u308b tf.matrix_diag_part(x) \u5bfe\u89d2\u884c\u5217\u3092\u30d9\u30af\u30c8\u30eb\u306b\u3059\u308b tf.matrix_inverse(x) \u9006\u884c\u5217 tf.matrix_determinant(x) \u884c\u5217\u5f0f tf.trace(x) \u5bfe\u89d2\u548c tf.matrix_transpose(x) \u8ee2\u7f6e\u884c\u5217 tf.eye(x) \u5358\u4f4d\u884c\u5217 tf.matrix_solve(x, y) \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # coding:utf-8 import numpy as np import tensorflow as tf # \u30d9\u30af\u30c8\u30eb x_data = np . arange ( 1.0 , 4.0 ) x = tf . constant ( x_data , tf . float32 ) # 3x3\u884c\u5217 y_data = np . arange ( 1.0 , 10.0 ) . reshape ( 3 , 3 ) y = tf . constant ( y_data , tf . float32 , shape = ( 3 , 3 )) # 2x3\u884c\u5217 z_data = np . arange ( 1.0 , 7.0 ) . reshape ( 2 , 3 ) z = tf . constant ( z_data , tf . float32 , shape = ( 2 , 3 )) with tf . Session () as sess : print sess . run ( x ) # \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u5909\u63db diag = sess . run ( tf . matrix_diag ( x )) print diag # \u884c\u5217\u5f0f # \u6ce8\u610f: \u5bfe\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u306b\u306a\u308b print sess . run ( tf . matrix_determinant ( diag )) # \u9006\u884c\u5217 print sess . run ( tf . matrix_inverse ( diag )) # \u5bfe\u89d2\u884c\u5217\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db vec = sess . run ( tf . matrix_diag_part ( diag )) print vec # \u5bfe\u89d2\u548c \u8de1(\u5bfe\u89d2\u6210\u5206\u306e\u548c) print sess . run ( tf . trace ( diag )) # \u8ee2\u7f6e\u884c\u5217 print sess . run ( y ) print sess . run ( tf . matrix_transpose ( y )) # 2x3\u884c\u5217\u306e\u5834\u5408\u30013x2\u884c\u5217\u306b\u5909\u5f62\u3055\u308c\u308b print sess . run ( z ) print sess . run ( tf . matrix_transpose ( z )) # 3x3\u5358\u4f4d\u884c\u5217 print sess . run ( tf . eye ( 3 )) # \u9023\u7acb\u65b9\u7a0b\u5f0f : # 2x+y+z = 15 # 4x+6y+3z = 41 # 8x+8y+9z = 83 # \u89e3 : x=5,y=2,z=3 # \u85a9\u6469\u9806\u5409, \u56db\u30c4\u8c37\u6676\u4e8c, \"\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u7dda\u5f62\u4ee3\u6570\" p.2\u3088\u308a a = tf . constant ([[ 2 , 1 , 1 ],[ 4 , 6 , 3 ],[ 8 , 8 , 9 ]] , tf . float32 , shape = ( 3 , 3 )) b = tf . constant ([[ 15 ],[ 41 ],[ 83 ]] , tf . float32 , shape = ( 3 , 1 )) with tf . Session () as sess : # \u9023\u7acb\u65b9\u7a0b\u5f0f print sess . run ( tf . matrix_solve ( a , b )) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [ 1. 2. 3.] [[ 1. 0. 0.] [ 0. 2. 0.] [ 0. 0. 3.]] 6.0 [[ 1. 0. 0. ] [ 0. 0.5 0. ] [ 0. 0. 0.33333334]] [ 1. 2. 3.] 6.0 [[ 1. 2. 3.] [ 4. 5. 6.] [ 7. 8. 9.]] [[ 1. 4. 7.] [ 2. 5. 8.] [ 3. 6. 9.]] [[ 1. 2. 3.] [ 4. 5. 6.]] [[ 1. 4.] [ 2. 5.] [ 3. 6.]] [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.]] [[ 5.] [ 2.] [ 3.]] \u53c2\u8003 https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#matrix-math-functions trace \u8de1 (\u7dda\u578b\u4ee3\u6570\u5b66) https://ja.wikipedia.org/wiki/%E8%B7%A1_(%E7%B7%9A%E5%9E%8B%E4%BB%A3%E6%95%B0%E5%AD%A6) \u8ee2\u7f6e\u884c\u5217\u306e\u89e3\u8aac https://ja.wikipedia.org/wiki/%E8%BB%A2%E7%BD%AE%E8%A1%8C%E5%88%97 \u884c\u5217\u5f0f\u306e\u89e3\u8aac https://ja.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E5%BC%8F","title":"\u884c\u5217\u95a2\u6570"},{"location":"model_mnist/tensorflow_matrix_func/#_1","text":"\u884c\u5217\u306e\u8a08\u7b97\u3092\u884c\u3046\u6570\u5b66\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u8a73\u3057\u304f\u306f Matrix Math Functions \u3092\u53c2\u8003\u3002 \u95a2\u6570\u540d \u8aac\u660e tf.matrix_diag(x) \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u3059\u308b tf.matrix_diag_part(x) \u5bfe\u89d2\u884c\u5217\u3092\u30d9\u30af\u30c8\u30eb\u306b\u3059\u308b tf.matrix_inverse(x) \u9006\u884c\u5217 tf.matrix_determinant(x) \u884c\u5217\u5f0f tf.trace(x) \u5bfe\u89d2\u548c tf.matrix_transpose(x) \u8ee2\u7f6e\u884c\u5217 tf.eye(x) \u5358\u4f4d\u884c\u5217 tf.matrix_solve(x, y) \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # coding:utf-8 import numpy as np import tensorflow as tf # \u30d9\u30af\u30c8\u30eb x_data = np . arange ( 1.0 , 4.0 ) x = tf . constant ( x_data , tf . float32 ) # 3x3\u884c\u5217 y_data = np . arange ( 1.0 , 10.0 ) . reshape ( 3 , 3 ) y = tf . constant ( y_data , tf . float32 , shape = ( 3 , 3 )) # 2x3\u884c\u5217 z_data = np . arange ( 1.0 , 7.0 ) . reshape ( 2 , 3 ) z = tf . constant ( z_data , tf . float32 , shape = ( 2 , 3 )) with tf . Session () as sess : print sess . run ( x ) # \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u5909\u63db diag = sess . run ( tf . matrix_diag ( x )) print diag # \u884c\u5217\u5f0f # \u6ce8\u610f: \u5bfe\u89d2\u884c\u5217\u306e\u884c\u5217\u5f0f\u306f\u5bfe\u89d2\u6210\u5206\u306e\u7a4d\u306b\u306a\u308b print sess . run ( tf . matrix_determinant ( diag )) # \u9006\u884c\u5217 print sess . run ( tf . matrix_inverse ( diag )) # \u5bfe\u89d2\u884c\u5217\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db vec = sess . run ( tf . matrix_diag_part ( diag )) print vec # \u5bfe\u89d2\u548c \u8de1(\u5bfe\u89d2\u6210\u5206\u306e\u548c) print sess . run ( tf . trace ( diag )) # \u8ee2\u7f6e\u884c\u5217 print sess . run ( y ) print sess . run ( tf . matrix_transpose ( y )) # 2x3\u884c\u5217\u306e\u5834\u5408\u30013x2\u884c\u5217\u306b\u5909\u5f62\u3055\u308c\u308b print sess . run ( z ) print sess . run ( tf . matrix_transpose ( z )) # 3x3\u5358\u4f4d\u884c\u5217 print sess . run ( tf . eye ( 3 )) # \u9023\u7acb\u65b9\u7a0b\u5f0f : # 2x+y+z = 15 # 4x+6y+3z = 41 # 8x+8y+9z = 83 # \u89e3 : x=5,y=2,z=3 # \u85a9\u6469\u9806\u5409, \u56db\u30c4\u8c37\u6676\u4e8c, \"\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u7dda\u5f62\u4ee3\u6570\" p.2\u3088\u308a a = tf . constant ([[ 2 , 1 , 1 ],[ 4 , 6 , 3 ],[ 8 , 8 , 9 ]] , tf . float32 , shape = ( 3 , 3 )) b = tf . constant ([[ 15 ],[ 41 ],[ 83 ]] , tf . float32 , shape = ( 3 , 1 )) with tf . Session () as sess : # \u9023\u7acb\u65b9\u7a0b\u5f0f print sess . run ( tf . matrix_solve ( a , b )) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [ 1. 2. 3.] [[ 1. 0. 0.] [ 0. 2. 0.] [ 0. 0. 3.]] 6.0 [[ 1. 0. 0. ] [ 0. 0.5 0. ] [ 0. 0. 0.33333334]] [ 1. 2. 3.] 6.0 [[ 1. 2. 3.] [ 4. 5. 6.] [ 7. 8. 9.]] [[ 1. 4. 7.] [ 2. 5. 8.] [ 3. 6. 9.]] [[ 1. 2. 3.] [ 4. 5. 6.]] [[ 1. 4.] [ 2. 5.] [ 3. 6.]] [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.]] [[ 5.] [ 2.] [ 3.]]","title":"\u884c\u5217\u95a2\u6570"},{"location":"model_mnist/tensorflow_matrix_func/#_2","text":"https://www.tensorflow.org/versions/master/api_docs/python/math_ops.html#matrix-math-functions trace \u8de1 (\u7dda\u578b\u4ee3\u6570\u5b66) https://ja.wikipedia.org/wiki/%E8%B7%A1_(%E7%B7%9A%E5%9E%8B%E4%BB%A3%E6%95%B0%E5%AD%A6) \u8ee2\u7f6e\u884c\u5217\u306e\u89e3\u8aac https://ja.wikipedia.org/wiki/%E8%BB%A2%E7%BD%AE%E8%A1%8C%E5%88%97 \u884c\u5217\u5f0f\u306e\u89e3\u8aac https://ja.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E5%BC%8F","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_mnist_batch/","text":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d0\u30c3\u30c1\u8aad\u307f\u8fbc\u307f 1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) for i in range ( 5 ): # 20\u500b\u5206\u306e\u30c7\u30fc\u30bf(\u30d0\u30c3\u30c1)\u3092\u53d6\u5f97\u3059\u308b batch = mnist . train . next_batch ( 20 ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u3068\u753b\u50cf\u30c7\u30fc\u30bf labels , images = batch # \u30c7\u30fc\u30bf\u6570\u3092\u8868\u793a\u3059\u308b print len ( labels ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 Extracting ./MNIST_data/train-images-idx3-ubyte.gz Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz 20 20 20 20 20","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d0\u30c3\u30c1\u8aad\u307f\u8fbc\u307f"},{"location":"model_mnist/tensorflow_mnist_batch/#mnist","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) for i in range ( 5 ): # 20\u500b\u5206\u306e\u30c7\u30fc\u30bf(\u30d0\u30c3\u30c1)\u3092\u53d6\u5f97\u3059\u308b batch = mnist . train . next_batch ( 20 ) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u3068\u753b\u50cf\u30c7\u30fc\u30bf labels , images = batch # \u30c7\u30fc\u30bf\u6570\u3092\u8868\u793a\u3059\u308b print len ( labels ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 Extracting ./MNIST_data/train-images-idx3-ubyte.gz Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz 20 20 20 20 20","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30d0\u30c3\u30c1\u8aad\u307f\u8fbc\u307f"},{"location":"model_mnist/tensorflow_mnist_download/","text":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 MNIST(\u624b\u66f8\u304d\u6570\u5b57)\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b 1 2 3 4 5 6 # coding:utf-8 from tensorflow.examples.tutorials.mnist import input_data # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f # \u6307\u5b9a\u30c7\u30a3\u30ec\u30af\u30c8\u306b\u30c7\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u306f\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes. Extracting ./MNIST_data/train-images-idx3-ubyte.gz Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes. Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes. Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes. Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz \u30ed\u30fc\u30ab\u30eb\u306b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u308b 1 2 3 $ ls ./MNIST_data t10k-images-idx3-ubyte.gz train-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz train-labels-idx1-ubyte.gz \u53c2\u8003 MNIST For ML Beginners","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"},{"location":"model_mnist/tensorflow_mnist_download/#mnist","text":"MNIST(\u624b\u66f8\u304d\u6570\u5b57)\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b 1 2 3 4 5 6 # coding:utf-8 from tensorflow.examples.tutorials.mnist import input_data # MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f # \u6307\u5b9a\u30c7\u30a3\u30ec\u30af\u30c8\u306b\u30c7\u30fc\u30bf\u304c\u306a\u3044\u5834\u5408\u306f\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes. Extracting ./MNIST_data/train-images-idx3-ubyte.gz Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes. Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes. Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes. Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz \u30ed\u30fc\u30ab\u30eb\u306b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u4fdd\u5b58\u3055\u308c\u3066\u3044\u308b 1 2 3 $ ls ./MNIST_data t10k-images-idx3-ubyte.gz train-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz train-labels-idx1-ubyte.gz","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"},{"location":"model_mnist/tensorflow_mnist_download/#_1","text":"MNIST For ML Beginners","title":"\u53c2\u8003"},{"location":"model_mnist/tensorflow_mnist_info/","text":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u60c5\u5831 mnist.train.images \u306f\u753b\u50cf\u30c7\u30fc\u30bf\u3067784(28\u306e2\u4e57)\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u3002\u30e2\u30ce\u30af\u30ed\u753b\u50cf\u306a\u306e\u30670\u301c1\u306e\u6fc3\u5ea6\u60c5\u5831\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u3002 mnist.train.labels \u306f\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u306710\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u3002\u6570\u5b57\u306e7\u304c\u6b63\u89e3\u306a\u3089\u3070\u30017\u756a\u76ee\u306e\u8981\u7d20\u306e\u5024\u306e\u307f\u304c1\u306b\u306a\u3063\u3066\u3044\u308b\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308b\u3002 1 2 3 4 5 6 7 8 9 # coding:utf-8 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) # \u753b\u50cf\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u6570 print mnist . train . images [ 0 ] . shape # \u30e9\u30d9\u30eb\u306e\u6b21\u5143\u6570 print mnist . train . labels [ 0 ] . shape \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 Extracting ./MNIST_data/train-images-idx3-ubyte.gz Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz (784,) (10,)","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u60c5\u5831"},{"location":"model_mnist/tensorflow_mnist_info/#mnist","text":"mnist.train.images \u306f\u753b\u50cf\u30c7\u30fc\u30bf\u3067784(28\u306e2\u4e57)\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u3002\u30e2\u30ce\u30af\u30ed\u753b\u50cf\u306a\u306e\u30670\u301c1\u306e\u6fc3\u5ea6\u60c5\u5831\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u3002 mnist.train.labels \u306f\u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf)\u306710\u6b21\u5143\u306e\u30d9\u30af\u30c8\u30eb\u3002\u6570\u5b57\u306e7\u304c\u6b63\u89e3\u306a\u3089\u3070\u30017\u756a\u76ee\u306e\u8981\u7d20\u306e\u5024\u306e\u307f\u304c1\u306b\u306a\u3063\u3066\u3044\u308b\u30d9\u30af\u30c8\u30eb\u3068\u306a\u308b\u3002 1 2 3 4 5 6 7 8 9 # coding:utf-8 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) # \u753b\u50cf\u30c7\u30fc\u30bf\u306e\u6b21\u5143\u6570 print mnist . train . images [ 0 ] . shape # \u30e9\u30d9\u30eb\u306e\u6b21\u5143\u6570 print mnist . train . labels [ 0 ] . shape \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 Extracting ./MNIST_data/train-images-idx3-ubyte.gz Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz (784,) (10,)","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u60c5\u5831"},{"location":"model_mnist/tensorflow_mnist_load/","text":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3001\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # coding:utf-8 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # \u6b63\u89e3\u30c7\u30fc\u30bf # mnist.train.labels # \u753b\u50cf\u30c7\u30fc\u30bf # mnist.train.images # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570 print mnist . train . num_examples # \u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # mnist.validation.labels # mnist.validation.images # \u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570 print mnist . validation . num_examples # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # mnist.test.labels # mnist.test.images # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570 print mnist . test . num_examples \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 Extracting ./MNIST_data/train-images-idx3-ubyte.gz Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz 55000 5000 10000","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"model_mnist/tensorflow_mnist_load/#mnist","text":"\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3001\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # coding:utf-8 import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( \"./MNIST_data/\" , one_hot = True ) # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # \u6b63\u89e3\u30c7\u30fc\u30bf # mnist.train.labels # \u753b\u50cf\u30c7\u30fc\u30bf # mnist.train.images # \u8a13\u7df4\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570 print mnist . train . num_examples # \u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # mnist.validation.labels # mnist.validation.images # \u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570 print mnist . validation . num_examples # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # mnist.test.labels # mnist.test.images # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570 print mnist . test . num_examples \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 Extracting ./MNIST_data/train-images-idx3-ubyte.gz Extracting ./MNIST_data/train-labels-idx1-ubyte.gz Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz 55000 5000 10000","title":"MNIST\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"model_mnist/tensorflow_perceptron_fitting/","text":"\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0 \u7bc4\u56f2-1\u301c1\u306ecos\u95a2\u6570\u306e\u5024\u306b\u6b63\u898f\u5206\u5e03\u306e\u4e71\u6570\u306b\u3088\u308b\u30ce\u30a4\u30ba\u3092\u6df7\u305c\u305f\u30c7\u30fc\u30bf\u3092\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308a\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf def multiplier_list ( v , n ): \"\"\" \u5f15\u6570\u30920\u4e57\u304b\u3089n\u4e57\u3057\u305f\u30ea\u30b9\u30c8\u3092\u8fd4\u3059 \u4f8b: multiplier_list(2, 4) [1, 2, 4, 8] \"\"\" return [ v ** i for i in range ( 0 , n )] # 9\u6b21\u591a\u9805\u5f0f\u306b\u3088\u308b\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0(\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u7248) dim = 9 x_data = np . arange ( - 1.0 , 1.0 , 0.01 ) formed_x_data = np . array ([ multiplier_list ( x , dim ) for x in x_data ]) # \u30ce\u30a4\u30ba bias = np . random . normal ( scale = 0.2 , size = x_data . shape ) # cos\u95a2\u6570\u306b\u30ce\u30a4\u30ba\u3092\u306e\u305b\u308b y_data = y = np . cos ( 2. * np . pi * x_data ) + bias # \u7e26\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db formed_y_data = y_data [:, np . newaxis ] # N x dim \u884c\u5217(N\u306f\u30c7\u30fc\u30bf\u6570) X = tf . placeholder ( tf . float32 , shape = ( None , dim )) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 1 )) def single_layer ( input_layer , node_num = 1024 ): \"\"\"\u96a0\u308c\u5c64\"\"\" w = tf . Variable ( tf . truncated_normal ([ dim , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( input_layer , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer , node_num = 1024 ): \"\"\"\u51fa\u529b\u5c64\"\"\" w = tf . Variable ( tf . zeros ([ node_num , 1 ])) b = tf . Variable ( tf . zeros ([ 1 ])) y = tf . matmul ( layer , w ) + b return y # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X , node_num = 64 ) # \u51fa\u529b\u5c64 y = output_layer ( hidden_layer , node_num = 64 ) # \u8aa4\u5dee\u95a2\u6570(\u4e8c\u4e57\u8aa4\u5dee) loss = tf . reduce_mean ( tf . square ( y - t )) # Adam optimizer = tf . train . AdamOptimizer () train_step = optimizer . minimize ( loss ) with tf . Session () as sess : sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 1000 ): i += 1 sess . run ( train_step , feed_dict = { X : formed_x_data , t : formed_y_data }) if i % 200 == 0 : train_loss = sess . run ( loss , feed_dict = { X : formed_x_data , t : formed_y_data }) print \"[Train] step: %d , loss: %f \" % ( i , train_loss ) # \u4e88\u6e2c\u5024\u306e\u30d7\u30ed\u30c3\u30c8 predict_y = sess . run ( y , feed_dict = { X : formed_x_data }) plt . plot ( x_data , predict_y , label = \"STEP %d \" % i ) # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8 plt . scatter ( x_data , y_data ) plt . legend () plt . show () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 [ Train ] step : 200 , loss : 0.133793 [ Train ] step : 400 , loss : 0.049397 [ Train ] step : 600 , loss : 0.034507 [ Train ] step : 800 , loss : 0.031959 [ Train ] step : 1000 , loss : 0.031614 \u30b0\u30e9\u30d5 : \u53c2\u8003","title":"\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0"},{"location":"model_mnist/tensorflow_perceptron_fitting/#_1","text":"\u7bc4\u56f2-1\u301c1\u306ecos\u95a2\u6570\u306e\u5024\u306b\u6b63\u898f\u5206\u5e03\u306e\u4e71\u6570\u306b\u3088\u308b\u30ce\u30a4\u30ba\u3092\u6df7\u305c\u305f\u30c7\u30fc\u30bf\u3092\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308a\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3059\u308b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # coding:utf-8 import numpy as np import matplotlib.pyplot as plt import tensorflow as tf def multiplier_list ( v , n ): \"\"\" \u5f15\u6570\u30920\u4e57\u304b\u3089n\u4e57\u3057\u305f\u30ea\u30b9\u30c8\u3092\u8fd4\u3059 \u4f8b: multiplier_list(2, 4) [1, 2, 4, 8] \"\"\" return [ v ** i for i in range ( 0 , n )] # 9\u6b21\u591a\u9805\u5f0f\u306b\u3088\u308b\u591a\u9805\u5f0f\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0(\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u7248) dim = 9 x_data = np . arange ( - 1.0 , 1.0 , 0.01 ) formed_x_data = np . array ([ multiplier_list ( x , dim ) for x in x_data ]) # \u30ce\u30a4\u30ba bias = np . random . normal ( scale = 0.2 , size = x_data . shape ) # cos\u95a2\u6570\u306b\u30ce\u30a4\u30ba\u3092\u306e\u305b\u308b y_data = y = np . cos ( 2. * np . pi * x_data ) + bias # \u7e26\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db formed_y_data = y_data [:, np . newaxis ] # N x dim \u884c\u5217(N\u306f\u30c7\u30fc\u30bf\u6570) X = tf . placeholder ( tf . float32 , shape = ( None , dim )) # \u30e9\u30d9\u30eb(\u6b63\u89e3\u30c7\u30fc\u30bf) t = tf . placeholder ( tf . float32 , shape = ( None , 1 )) def single_layer ( input_layer , node_num = 1024 ): \"\"\"\u96a0\u308c\u5c64\"\"\" w = tf . Variable ( tf . truncated_normal ([ dim , node_num ])) b = tf . Variable ( tf . zeros ([ node_num ])) f = tf . matmul ( input_layer , w ) + b layer = tf . nn . relu ( f ) return layer def output_layer ( layer , node_num = 1024 ): \"\"\"\u51fa\u529b\u5c64\"\"\" w = tf . Variable ( tf . zeros ([ node_num , 1 ])) b = tf . Variable ( tf . zeros ([ 1 ])) y = tf . matmul ( layer , w ) + b return y # \u96a0\u308c\u5c64 hidden_layer = single_layer ( X , node_num = 64 ) # \u51fa\u529b\u5c64 y = output_layer ( hidden_layer , node_num = 64 ) # \u8aa4\u5dee\u95a2\u6570(\u4e8c\u4e57\u8aa4\u5dee) loss = tf . reduce_mean ( tf . square ( y - t )) # Adam optimizer = tf . train . AdamOptimizer () train_step = optimizer . minimize ( loss ) with tf . Session () as sess : sess . run ( tf . initialize_all_variables ()) i = 0 for _ in range ( 1000 ): i += 1 sess . run ( train_step , feed_dict = { X : formed_x_data , t : formed_y_data }) if i % 200 == 0 : train_loss = sess . run ( loss , feed_dict = { X : formed_x_data , t : formed_y_data }) print \"[Train] step: %d , loss: %f \" % ( i , train_loss ) # \u4e88\u6e2c\u5024\u306e\u30d7\u30ed\u30c3\u30c8 predict_y = sess . run ( y , feed_dict = { X : formed_x_data }) plt . plot ( x_data , predict_y , label = \"STEP %d \" % i ) # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30d7\u30ed\u30c3\u30c8 plt . scatter ( x_data , y_data ) plt . legend () plt . show () \u5b9f\u884c\u7d50\u679c : 1 2 3 4 5 [ Train ] step : 200 , loss : 0.133793 [ Train ] step : 400 , loss : 0.049397 [ Train ] step : 600 , loss : 0.034507 [ Train ] step : 800 , loss : 0.031959 [ Train ] step : 1000 , loss : 0.031614 \u30b0\u30e9\u30d5 :","title":"\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\u306b\u3088\u308b\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0"},{"location":"model_mnist/tensorflow_perceptron_fitting/#_2","text":"","title":"\u53c2\u8003"},{"location":"model_sequence/rnn_basic/","text":"RNN\u306b\u3088\u308b\u30af\u30e9\u30b9\u5206\u985e RNN\u306f\u4e2d\u9593\u5c64\u306b\u518d\u5e30\u69cb\u9020\u3092\u6301\u3064\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3001\u81ea\u7136\u8a00\u8a9e\u3084\u97f3\u58f0\u3001\u52d5\u753b\u3068\u3044\u3063\u305f\u6642\u7cfb\u5217\u3092\u6271\u3046\u3053\u3068\u3092\u5f97\u610f\u3068\u3057\u3066\u3044\u307e\u3059\u3002 RNN\u306e\u69cb\u9020\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u305f\u3060\u3057\u3001x\u306f\u5165\u529b\u3001z\u306f\u4e2d\u9593\u5c64\u306e\u51fa\u529b\u3001W\u306f\u5c64\u9593\u306e\u91cd\u307f\u3001t\u306f\u7cfb\u5217\u9577\u3092\u8868\u3057\u3066\u3044\u308b\u3002 \u4ee5\u4e0a\u306e\u56f3\u3092\u6642\u9593\u3067\u5c55\u958b\u3057\u305f\u3082\u306e\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308b\u30021\u3064\u524d\u306e\u6642\u9593\u306e\u91cd\u307f\u304c\u6b21\u306e\u6642\u9593\u306b\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 TensorFlow\u306b\u304a\u3044\u3066z\u306e\u90e8\u5206\u306f\u30e6\u30cb\u30c3\u30c8\u30bb\u30eb\u3068\u3057\u3066\u6271\u308f\u308c\u3001LSTM\u3084GRU\u3068\u547c\u3070\u308c\u308b\u69cb\u9020\u3092\u30bb\u30eb\u3068\u3057\u3066\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 \u6b21\u306e\u3088\u3046\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u6ce2\u5f62\u3092\u30af\u30e9\u30b9\u5206\u985e\u3059\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3002 \u30af\u30e9\u30b91\u306e\u4f8b \u30af\u30e9\u30b92\u306e\u4f8b \u30af\u30e9\u30b93\u306e\u4f8b TensorFlow\u306b\u304a\u3044\u3066\u5229\u7528\u3067\u304d\u308bRNN\u306e\u30bb\u30eb\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002 * BasicRNNCell * BasicLSTMCell * LSTMCell * GRUCell \u4eca\u56de\u306f BasicRNNCell \u3092\u4f7f\u7528\u3059\u308b\u3002\u307e\u305f\u3001\u8aa4\u5dee\u95a2\u6570\u306b\u306f\u4e2d\u9593\u5c64\u306e\u6700\u7d42\u6642\u9593\u306b\u304a\u3051\u308b\u51fa\u529b(\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u3044\u3046 outputs[-1] )\u3092\u7528\u3044\u3066\u51fa\u529b\u5c64\u306e\u5b9a\u7fa9\u3092\u884c\u3046\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 #!/usr/bin/env python # coding: utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function import random import numpy as np import tensorflow as tf from tensorflow.contrib import rnn random . seed ( 777 ) np . random . seed ( 777 ) tf . set_random_seed ( 777 ) # \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc N_CLASSES = 3 # \u30af\u30e9\u30b9\u6570 N_INPUTS = 1 # 1\u30b9\u30c6\u30c3\u30d7\u306b\u5165\u529b\u3055\u308c\u308b\u30c7\u30fc\u30bf\u6570 N_STEPS = 200 # \u5b66\u7fd2\u30b9\u30c6\u30c3\u30d7\u6570 LEN_SEQ = 10 # \u7cfb\u5217\u9577 N_NODES = 64 # \u30ce\u30fc\u30c9\u6570 N_DATA = 1000 # \u5404\u30af\u30e9\u30b9\u306e\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u6570 N_TEST = 1000 # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u6570 BATCH_SIZE = 20 # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba # \u30c7\u30fc\u30bf\u306e\u6e96\u5099 def gen_non_pulse ( len_seq ): \"\"\"\u6ce2\u3092\u6301\u305f\u306a\u3044\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3059\u308b\"\"\" ret = np . random . rand ( len_seq ) ret = np . append ( ret , 0 ) return ret . reshape ( - 1 , 1 ) def gen_pulse ( len_seq , positive = True ): \"\"\"\u6ce2\u3092\u6301\u3064\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3059\u308b\"\"\" seq = np . zeros ( len_seq ) i = random . randint ( 0 , len_seq - 3 ) # \u6ce2\u3092\u7acb\u3066\u308b\u4f4d\u7f6e w = random . randint ( 1 , 4 ) w = w if positive else w * ( - 1. ) e = 3 if positive else - 3 l = 1 if positive else 2 # \u30e9\u30d9\u30eb seq [ i ], seq [ i + 1 ], seq [ i + 2 ] = w , w + e , w noise = np . random . rand ( len_seq ) ret = seq + noise ret = np . append ( ret , l ) # \u30e9\u30d9\u30eb\u3092\u52a0\u3048\u308b return ret . reshape ( - 1 , 1 ) def gen_dataset ( len_seq , n_data ): class_01_data = [ gen_non_pulse ( len_seq ) for _ in range ( n_data )] class_02_data = [ gen_pulse ( len_seq , positive = True ) for _ in range ( n_data )] class_03_data = [ gen_pulse ( len_seq , positive = False ) for _ in range ( n_data )] dataset = np . r_ [ class_01_data , class_02_data , class_03_data ] np . random . shuffle ( dataset ) x_ = dataset [:,: 10 ] t_ = dataset [:, 10 ] . reshape ( - 1 ) return x_ , t_ x_train , t_train = gen_dataset ( LEN_SEQ , N_DATA ) # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 x_test , t_test = gen_dataset ( LEN_SEQ , N_DATA ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9 x = tf . placeholder ( tf . float32 , [ None , LEN_SEQ , N_INPUTS ]) # \u5165\u529b\u30c7\u30fc\u30bf t = tf . placeholder ( tf . int32 , [ None ]) # \u6559\u5e2b\u30c7\u30fc\u30bf t_on_hot = tf . one_hot ( t , depth = N_CLASSES , dtype = tf . float32 ) # 1-of-K\u30d9\u30af\u30c8\u30eb cell = rnn . BasicRNNCell ( num_units = N_NODES , activation = tf . nn . tanh ) # \u4e2d\u9593\u5c64\u306e\u30bb\u30eb # RNN\u306b\u5165\u529b\u304a\u3088\u3073\u30bb\u30eb\u8a2d\u5b9a\u3059\u308b outputs , states = tf . nn . dynamic_rnn ( cell = cell , inputs = x , dtype = tf . float32 , time_major = False ) # [\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba,\u7cfb\u5217\u9577,\u51fa\u529b\u6570]\u2192[\u7cfb\u5217\u9577,\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba,\u51fa\u529b\u6570] outputs = tf . transpose ( outputs , perm = [ 1 , 0 , 2 ]) w = tf . Variable ( tf . random_normal ([ N_NODES , N_CLASSES ], stddev = 0.01 )) b = tf . Variable ( tf . zeros ([ N_CLASSES ])) logits = tf . matmul ( outputs [ - 1 ], w ) + b # \u51fa\u529b\u5c64 pred = tf . nn . softmax ( logits ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9 cross_entropy = tf . nn . softmax_cross_entropy_with_logits ( labels = t_on_hot , logits = logits ) loss = tf . reduce_mean ( cross_entropy ) # \u8aa4\u5dee\u95a2\u6570 train_step = tf . train . AdamOptimizer () . minimize ( loss ) # \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 correct_prediction = tf . equal ( tf . argmax ( pred , 1 ), tf . argmax ( t_on_hot , 1 )) accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) # \u7cbe\u5ea6 # \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( N_STEPS ): cycle = int ( N_DATA * 3 / BATCH_SIZE ) begin = int ( BATCH_SIZE * ( i % cycle )) end = begin + BATCH_SIZE x_batch , t_batch = x_train [ begin : end ], t_train [ begin : end ] sess . run ( train_step , feed_dict = { x : x_batch , t : t_batch }) i += 1 if i % 10 == 0 : loss_ , acc_ = sess . run ([ loss , accuracy ], feed_dict = { x : x_batch , t : t_batch }) loss_test_ , acc_test_ = sess . run ([ loss , accuracy ], feed_dict = { x : x_test , t : t_test }) print ( \"[TRAIN] loss : %f , accuracy : %f \" % ( loss_ , acc_ )) print ( \"[TEST loss : %f , accuracy : %f \" % ( loss_test_ , acc_test_ )) sess . close () \u5b9f\u884c\u7d50\u679c TensorFlow 1.0.0 Python 3.6.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ TRAIN ] loss : 0.923734 , accuracy : 0.600000 [ TEST loss : 0.941685, accuracy : 0.585000 [TRAIN ] loss : 0.763349 , accuracy : 0.800000 [ TEST loss : 0.758919, accuracy : 0.830333 [TRAIN ] loss : 0.634125 , accuracy : 0.950000 [ TEST loss : 0.596680, accuracy : 0.820333 [TRAIN ] loss : 0.443795 , accuracy : 1.000000 [ TEST loss : 0.487444, accuracy : 0.961000 [TRAIN ] loss : 0.488549 , accuracy : 0.900000 [ TEST loss : 0.406113, accuracy : 0.961667 [TRAIN ] loss : 0.286011 , accuracy : 1.000000 [ TEST loss : 0.295983, accuracy : 0.981000 [TRAIN ] loss : 0.249263 , accuracy : 1.000000 [ TEST loss : 0.236212, accuracy : 0.977000 [TRAIN ] loss : 0.182674 , accuracy : 0.950000 [ TEST loss : 0.185231, accuracy : 0.977000 [TRAIN ] loss : 0.163595 , accuracy : 1.000000 [ TEST loss : 0.173578, accuracy : 0.963667 [TRAIN ] loss : 0.196658 , accuracy : 0.950000 [ TEST loss : 0.145268, accuracy : 0.983000 [TRAIN ] loss : 0.086850 , accuracy : 1.000000 [ TEST loss : 0.120967, accuracy : 0.990000 [TRAIN ] loss : 0.108360 , accuracy : 1.000000 [ TEST loss : 0.098970, accuracy : 0.987667 [TRAIN ] loss : 0.090482 , accuracy : 1.000000 [ TEST loss : 0.081187, accuracy : 0.991667 [TRAIN ] loss : 0.072524 , accuracy : 1.000000 [ TEST loss : 0.075302, accuracy : 0.994667 [TRAIN ] loss : 0.073820 , accuracy : 1.000000 [ TEST loss : 0.065370, accuracy : 0.996000 [TRAIN ] loss : 0.062461 , accuracy : 1.000000 [ TEST loss : 0.055922, accuracy : 0.997000 [TRAIN ] loss : 0.055428 , accuracy : 1.000000 [ TEST loss : 0.050875, accuracy : 0.998333 [TRAIN ] loss : 0.033651 , accuracy : 1.000000 [ TEST loss : 0.042620, accuracy : 0.998333 [TRAIN ] loss : 0.024268 , accuracy : 1.000000 [ TEST loss : 0.040751, accuracy : 0.994667 [TRAIN ] loss : 0.077236 , accuracy : 1.000000 [ TEST loss : 0.048991 , accuracy : 0.992000 \u8aa4\u5dee\u306e\u63a8\u79fb \u7cbe\u5ea6\u306e\u63a8\u79fb \u53c2\u8003 \u5ca1\u8c37\u8cb4\u4e4b,\"\u6df1\u5c64\u5b66\u7fd2\",\u8b1b\u8ac7\u793e,2015 RNN\u306e\u56f3\u306f\u4e0a\u306e\u672c\u3092\u53c2\u8003\u306b\u4f5c\u6210\u3057\u305f https://www.tensorflow.org/api_guides/python/contrib.rnn https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn","title":"RNN\u306b\u3088\u308b\u30af\u30e9\u30b9\u5206\u985e"},{"location":"model_sequence/rnn_basic/#rnn","text":"RNN\u306f\u4e2d\u9593\u5c64\u306b\u518d\u5e30\u69cb\u9020\u3092\u6301\u3064\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3067\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u7279\u5fb4\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u305f\u3081\u3001\u81ea\u7136\u8a00\u8a9e\u3084\u97f3\u58f0\u3001\u52d5\u753b\u3068\u3044\u3063\u305f\u6642\u7cfb\u5217\u3092\u6271\u3046\u3053\u3068\u3092\u5f97\u610f\u3068\u3057\u3066\u3044\u307e\u3059\u3002 RNN\u306e\u69cb\u9020\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002\u305f\u3060\u3057\u3001x\u306f\u5165\u529b\u3001z\u306f\u4e2d\u9593\u5c64\u306e\u51fa\u529b\u3001W\u306f\u5c64\u9593\u306e\u91cd\u307f\u3001t\u306f\u7cfb\u5217\u9577\u3092\u8868\u3057\u3066\u3044\u308b\u3002 \u4ee5\u4e0a\u306e\u56f3\u3092\u6642\u9593\u3067\u5c55\u958b\u3057\u305f\u3082\u306e\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u308b\u30021\u3064\u524d\u306e\u6642\u9593\u306e\u91cd\u307f\u304c\u6b21\u306e\u6642\u9593\u306b\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u5206\u304b\u308b\u3002 TensorFlow\u306b\u304a\u3044\u3066z\u306e\u90e8\u5206\u306f\u30e6\u30cb\u30c3\u30c8\u30bb\u30eb\u3068\u3057\u3066\u6271\u308f\u308c\u3001LSTM\u3084GRU\u3068\u547c\u3070\u308c\u308b\u69cb\u9020\u3092\u30bb\u30eb\u3068\u3057\u3066\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002","title":"RNN\u306b\u3088\u308b\u30af\u30e9\u30b9\u5206\u985e"},{"location":"model_sequence/rnn_basic/#_1","text":"\u6b21\u306e\u3088\u3046\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u6ce2\u5f62\u3092\u30af\u30e9\u30b9\u5206\u985e\u3059\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3002 \u30af\u30e9\u30b91\u306e\u4f8b \u30af\u30e9\u30b92\u306e\u4f8b \u30af\u30e9\u30b93\u306e\u4f8b TensorFlow\u306b\u304a\u3044\u3066\u5229\u7528\u3067\u304d\u308bRNN\u306e\u30bb\u30eb\u306f\u6b21\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002 * BasicRNNCell * BasicLSTMCell * LSTMCell * GRUCell \u4eca\u56de\u306f BasicRNNCell \u3092\u4f7f\u7528\u3059\u308b\u3002\u307e\u305f\u3001\u8aa4\u5dee\u95a2\u6570\u306b\u306f\u4e2d\u9593\u5c64\u306e\u6700\u7d42\u6642\u9593\u306b\u304a\u3051\u308b\u51fa\u529b(\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3067\u3044\u3046 outputs[-1] )\u3092\u7528\u3044\u3066\u51fa\u529b\u5c64\u306e\u5b9a\u7fa9\u3092\u884c\u3046\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 #!/usr/bin/env python # coding: utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function import random import numpy as np import tensorflow as tf from tensorflow.contrib import rnn random . seed ( 777 ) np . random . seed ( 777 ) tf . set_random_seed ( 777 ) # \u30d1\u30e9\u30e1\u30fc\u30bf\u30fc N_CLASSES = 3 # \u30af\u30e9\u30b9\u6570 N_INPUTS = 1 # 1\u30b9\u30c6\u30c3\u30d7\u306b\u5165\u529b\u3055\u308c\u308b\u30c7\u30fc\u30bf\u6570 N_STEPS = 200 # \u5b66\u7fd2\u30b9\u30c6\u30c3\u30d7\u6570 LEN_SEQ = 10 # \u7cfb\u5217\u9577 N_NODES = 64 # \u30ce\u30fc\u30c9\u6570 N_DATA = 1000 # \u5404\u30af\u30e9\u30b9\u306e\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u6570 N_TEST = 1000 # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u6570 BATCH_SIZE = 20 # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba # \u30c7\u30fc\u30bf\u306e\u6e96\u5099 def gen_non_pulse ( len_seq ): \"\"\"\u6ce2\u3092\u6301\u305f\u306a\u3044\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3059\u308b\"\"\" ret = np . random . rand ( len_seq ) ret = np . append ( ret , 0 ) return ret . reshape ( - 1 , 1 ) def gen_pulse ( len_seq , positive = True ): \"\"\"\u6ce2\u3092\u6301\u3064\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3059\u308b\"\"\" seq = np . zeros ( len_seq ) i = random . randint ( 0 , len_seq - 3 ) # \u6ce2\u3092\u7acb\u3066\u308b\u4f4d\u7f6e w = random . randint ( 1 , 4 ) w = w if positive else w * ( - 1. ) e = 3 if positive else - 3 l = 1 if positive else 2 # \u30e9\u30d9\u30eb seq [ i ], seq [ i + 1 ], seq [ i + 2 ] = w , w + e , w noise = np . random . rand ( len_seq ) ret = seq + noise ret = np . append ( ret , l ) # \u30e9\u30d9\u30eb\u3092\u52a0\u3048\u308b return ret . reshape ( - 1 , 1 ) def gen_dataset ( len_seq , n_data ): class_01_data = [ gen_non_pulse ( len_seq ) for _ in range ( n_data )] class_02_data = [ gen_pulse ( len_seq , positive = True ) for _ in range ( n_data )] class_03_data = [ gen_pulse ( len_seq , positive = False ) for _ in range ( n_data )] dataset = np . r_ [ class_01_data , class_02_data , class_03_data ] np . random . shuffle ( dataset ) x_ = dataset [:,: 10 ] t_ = dataset [:, 10 ] . reshape ( - 1 ) return x_ , t_ x_train , t_train = gen_dataset ( LEN_SEQ , N_DATA ) # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 x_test , t_test = gen_dataset ( LEN_SEQ , N_DATA ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 # \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9 x = tf . placeholder ( tf . float32 , [ None , LEN_SEQ , N_INPUTS ]) # \u5165\u529b\u30c7\u30fc\u30bf t = tf . placeholder ( tf . int32 , [ None ]) # \u6559\u5e2b\u30c7\u30fc\u30bf t_on_hot = tf . one_hot ( t , depth = N_CLASSES , dtype = tf . float32 ) # 1-of-K\u30d9\u30af\u30c8\u30eb cell = rnn . BasicRNNCell ( num_units = N_NODES , activation = tf . nn . tanh ) # \u4e2d\u9593\u5c64\u306e\u30bb\u30eb # RNN\u306b\u5165\u529b\u304a\u3088\u3073\u30bb\u30eb\u8a2d\u5b9a\u3059\u308b outputs , states = tf . nn . dynamic_rnn ( cell = cell , inputs = x , dtype = tf . float32 , time_major = False ) # [\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba,\u7cfb\u5217\u9577,\u51fa\u529b\u6570]\u2192[\u7cfb\u5217\u9577,\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba,\u51fa\u529b\u6570] outputs = tf . transpose ( outputs , perm = [ 1 , 0 , 2 ]) w = tf . Variable ( tf . random_normal ([ N_NODES , N_CLASSES ], stddev = 0.01 )) b = tf . Variable ( tf . zeros ([ N_CLASSES ])) logits = tf . matmul ( outputs [ - 1 ], w ) + b # \u51fa\u529b\u5c64 pred = tf . nn . softmax ( logits ) # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9 cross_entropy = tf . nn . softmax_cross_entropy_with_logits ( labels = t_on_hot , logits = logits ) loss = tf . reduce_mean ( cross_entropy ) # \u8aa4\u5dee\u95a2\u6570 train_step = tf . train . AdamOptimizer () . minimize ( loss ) # \u5b66\u7fd2\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 correct_prediction = tf . equal ( tf . argmax ( pred , 1 ), tf . argmax ( t_on_hot , 1 )) accuracy = tf . reduce_mean ( tf . cast ( correct_prediction , tf . float32 )) # \u7cbe\u5ea6 # \u5b66\u7fd2\u306e\u5b9f\u884c sess = tf . Session () sess . run ( tf . global_variables_initializer ()) i = 0 for _ in range ( N_STEPS ): cycle = int ( N_DATA * 3 / BATCH_SIZE ) begin = int ( BATCH_SIZE * ( i % cycle )) end = begin + BATCH_SIZE x_batch , t_batch = x_train [ begin : end ], t_train [ begin : end ] sess . run ( train_step , feed_dict = { x : x_batch , t : t_batch }) i += 1 if i % 10 == 0 : loss_ , acc_ = sess . run ([ loss , accuracy ], feed_dict = { x : x_batch , t : t_batch }) loss_test_ , acc_test_ = sess . run ([ loss , accuracy ], feed_dict = { x : x_test , t : t_test }) print ( \"[TRAIN] loss : %f , accuracy : %f \" % ( loss_ , acc_ )) print ( \"[TEST loss : %f , accuracy : %f \" % ( loss_test_ , acc_test_ )) sess . close ()","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"model_sequence/rnn_basic/#_2","text":"TensorFlow 1.0.0 Python 3.6.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [ TRAIN ] loss : 0.923734 , accuracy : 0.600000 [ TEST loss : 0.941685, accuracy : 0.585000 [TRAIN ] loss : 0.763349 , accuracy : 0.800000 [ TEST loss : 0.758919, accuracy : 0.830333 [TRAIN ] loss : 0.634125 , accuracy : 0.950000 [ TEST loss : 0.596680, accuracy : 0.820333 [TRAIN ] loss : 0.443795 , accuracy : 1.000000 [ TEST loss : 0.487444, accuracy : 0.961000 [TRAIN ] loss : 0.488549 , accuracy : 0.900000 [ TEST loss : 0.406113, accuracy : 0.961667 [TRAIN ] loss : 0.286011 , accuracy : 1.000000 [ TEST loss : 0.295983, accuracy : 0.981000 [TRAIN ] loss : 0.249263 , accuracy : 1.000000 [ TEST loss : 0.236212, accuracy : 0.977000 [TRAIN ] loss : 0.182674 , accuracy : 0.950000 [ TEST loss : 0.185231, accuracy : 0.977000 [TRAIN ] loss : 0.163595 , accuracy : 1.000000 [ TEST loss : 0.173578, accuracy : 0.963667 [TRAIN ] loss : 0.196658 , accuracy : 0.950000 [ TEST loss : 0.145268, accuracy : 0.983000 [TRAIN ] loss : 0.086850 , accuracy : 1.000000 [ TEST loss : 0.120967, accuracy : 0.990000 [TRAIN ] loss : 0.108360 , accuracy : 1.000000 [ TEST loss : 0.098970, accuracy : 0.987667 [TRAIN ] loss : 0.090482 , accuracy : 1.000000 [ TEST loss : 0.081187, accuracy : 0.991667 [TRAIN ] loss : 0.072524 , accuracy : 1.000000 [ TEST loss : 0.075302, accuracy : 0.994667 [TRAIN ] loss : 0.073820 , accuracy : 1.000000 [ TEST loss : 0.065370, accuracy : 0.996000 [TRAIN ] loss : 0.062461 , accuracy : 1.000000 [ TEST loss : 0.055922, accuracy : 0.997000 [TRAIN ] loss : 0.055428 , accuracy : 1.000000 [ TEST loss : 0.050875, accuracy : 0.998333 [TRAIN ] loss : 0.033651 , accuracy : 1.000000 [ TEST loss : 0.042620, accuracy : 0.998333 [TRAIN ] loss : 0.024268 , accuracy : 1.000000 [ TEST loss : 0.040751, accuracy : 0.994667 [TRAIN ] loss : 0.077236 , accuracy : 1.000000 [ TEST loss : 0.048991 , accuracy : 0.992000 \u8aa4\u5dee\u306e\u63a8\u79fb \u7cbe\u5ea6\u306e\u63a8\u79fb","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"model_sequence/rnn_basic/#_3","text":"\u5ca1\u8c37\u8cb4\u4e4b,\"\u6df1\u5c64\u5b66\u7fd2\",\u8b1b\u8ac7\u793e,2015 RNN\u306e\u56f3\u306f\u4e0a\u306e\u672c\u3092\u53c2\u8003\u306b\u4f5c\u6210\u3057\u305f https://www.tensorflow.org/api_guides/python/contrib.rnn https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn","title":"\u53c2\u8003"},{"location":"numpy/numpy006/","text":"\u884c\u5217\u306e\u6f14\u7b97 Sample \u6f14\u7b97 \u8aac\u660e + \u884c\u5217\u306e\u548c - \u884c\u5217\u306e\u5dee np.dot(a, b) \u307e\u305f\u306f a.dot(b) \u884c\u5217\u306e\u7a4d * \u884c\u5217\u306e \u8981\u7d20 \u540c\u58eb\u306e\u7a4d / \u884c\u5217\u306e \u8981\u7d20 \u540c\u58eb\u306e\u5546 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding:utf-8 import numpy as np # 2x2\u884c\u5217a a = np . array ([[ 2.0 , 1.0 ], [ 4.0 , 2.0 ]]) # 2x2\u884c\u5217b b = np . array ([[ 1.0 , 1.0 ], [ 6.0 , 3.0 ]]) # \u884c\u5217\u306e\u548c print a + b # \u884c\u5217\u306e\u5dee print a - b # \u884c\u5217\u306e\u7a4d print np . dot ( a , b ) # \u4ee5\u4e0b\u3082\u540c\u69d8 print a . dot ( b ) # \u884c\u5217\u306e\u8981\u7d20\u540c\u58eb\u306e\u7a4d print a * b # \u884c\u5217\u306e\u8981\u7d20\u540c\u58eb\u306e\u5546 print a / b \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 [[ 3 . 2 . ] [ 10 . 5 . ]] [[ 1 . 0 . ] [ -2. -1. ]] [[ 8 . 5 . ] [ 16 . 10 . ]] [[ 8 . 5 . ] [ 16 . 10 . ]] [[ 2 . 1 . ] [ 24 . 6 . ]] [[ 2 . 1 . ] [ 0 .66666667 0 .66666667 ]] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy006.ipynb","title":"\u884c\u5217\u306e\u6f14\u7b97"},{"location":"numpy/numpy006/#_1","text":"","title":"\u884c\u5217\u306e\u6f14\u7b97"},{"location":"numpy/numpy006/#sample","text":"\u6f14\u7b97 \u8aac\u660e + \u884c\u5217\u306e\u548c - \u884c\u5217\u306e\u5dee np.dot(a, b) \u307e\u305f\u306f a.dot(b) \u884c\u5217\u306e\u7a4d * \u884c\u5217\u306e \u8981\u7d20 \u540c\u58eb\u306e\u7a4d / \u884c\u5217\u306e \u8981\u7d20 \u540c\u58eb\u306e\u5546 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding:utf-8 import numpy as np # 2x2\u884c\u5217a a = np . array ([[ 2.0 , 1.0 ], [ 4.0 , 2.0 ]]) # 2x2\u884c\u5217b b = np . array ([[ 1.0 , 1.0 ], [ 6.0 , 3.0 ]]) # \u884c\u5217\u306e\u548c print a + b # \u884c\u5217\u306e\u5dee print a - b # \u884c\u5217\u306e\u7a4d print np . dot ( a , b ) # \u4ee5\u4e0b\u3082\u540c\u69d8 print a . dot ( b ) # \u884c\u5217\u306e\u8981\u7d20\u540c\u58eb\u306e\u7a4d print a * b # \u884c\u5217\u306e\u8981\u7d20\u540c\u58eb\u306e\u5546 print a / b \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 [[ 3 . 2 . ] [ 10 . 5 . ]] [[ 1 . 0 . ] [ -2. -1. ]] [[ 8 . 5 . ] [ 16 . 10 . ]] [[ 8 . 5 . ] [ 16 . 10 . ]] [[ 2 . 1 . ] [ 24 . 6 . ]] [[ 2 . 1 . ] [ 0 .66666667 0 .66666667 ]]","title":"Sample"},{"location":"numpy/numpy006/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy006.ipynb","title":"Notebook"},{"location":"numpy/numpy007/","text":"\u4e71\u6570\u306e\u30b7\u30fc\u30c9 \u8a08\u7b97\u7d50\u679c\u3092\u518d\u73fe\u3055\u305b\u308b\u5834\u5408\u306b\u5229\u7528\u3059\u308b\u3002 Sample 1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b np . random . seed ( 20200724 ) X = np . random . randn ( 10 , 2 ) print X \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 [[ 0 .061722 -0.57419489 ] [ -0.22935131 0 .96591975 ] [ 0 .26623205 0 .43955315 ] [ 0 .29570751 1 .38297576 ] [ -0.48006958 -0.25572763 ] [ 0 .53592532 -0.15575423 ] [ -1.35626278 -0.78728757 ] [ 0 .19937479 1 .08232194 ] [ -1.27044623 0 .30786437 ] [ -0.1061809 0 .57807686 ]]","title":"\u4e71\u6570\u306e\u30b7\u30fc\u30c9"},{"location":"numpy/numpy007/#_1","text":"\u8a08\u7b97\u7d50\u679c\u3092\u518d\u73fe\u3055\u305b\u308b\u5834\u5408\u306b\u5229\u7528\u3059\u308b\u3002 Sample 1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b np . random . seed ( 20200724 ) X = np . random . randn ( 10 , 2 ) print X \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 [[ 0 .061722 -0.57419489 ] [ -0.22935131 0 .96591975 ] [ 0 .26623205 0 .43955315 ] [ 0 .29570751 1 .38297576 ] [ -0.48006958 -0.25572763 ] [ 0 .53592532 -0.15575423 ] [ -1.35626278 -0.78728757 ] [ 0 .19937479 1 .08232194 ] [ -1.27044623 0 .30786437 ] [ -0.1061809 0 .57807686 ]]","title":"\u4e71\u6570\u306e\u30b7\u30fc\u30c9"},{"location":"numpy/numpy008/","text":"\u30b9\u30e9\u30a4\u30b7\u30f3\u30b0 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # coding:utf-8 import numpy as np # 0\u304b\u308910\u307e\u3067\u306e\u914d\u5217 x = np . arange ( 10 ) # => [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # \u5168\u90e8\u306e\u5024 print x [:] # \u5168\u90e8\u306e\u5024\u3092\u9006\u9806\u3067\u53d6\u5f97\u3059\u308b print x [:: - 1 ] # 4\u756a\u76ee\u4ee5\u964d\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print x [ 4 :] # 7\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print x [: 7 ] # 4\u756a\u76ee\u304b\u30897\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print x [ 4 : 7 ] # 1\u756a\u76ee\u304b\u30898\u756a\u76ee\u307e\u3067\u306e\u5024\u30921\u3064\u98db\u3070\u3057\u3067\u53d6\u5f97\u3059\u308b print x [ 1 : 8 : 2 ] # 8\u756a\u76ee\u304b\u30891\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u9006\u9806\u3067\u53d6\u5f97\u3059\u308b print x [ 8 : 1 : - 1 ] #\u30008\u756a\u76ee\u304b\u30891\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u9006\u9806\u3067\u304b\u3064\u4e00\u3064\u98db\u3070\u3057\u3067\u53d6\u5f97\u3059\u308b #\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3088\u3046\u306bx[8:1:-1]\u3067\u5f97\u305f\u914d\u5217\u3092\u6b21\u306b[::2]\u3067\u4e00\u3064\u98db\u3070\u3057\u3067\u53d6\u5f97\u3059\u308b\u3001\u3068\u3044\u3046\u898b\u65b9 print ( x [ 8 : 1 : - 1 ][:: 2 ]) #\u3000\u3055\u3089\u306b\u4e00\u3064\u98db\u3070\u3057\u3057\u3066\u53d6\u5f97\u3059\u308b print ( x [ 8 : 1 : - 1 ][:: 2 ][:: 2 ]) # \u884c\u5217(\u591a\u6b21\u5143\u914d\u5217)\u306b\u95a2\u3057\u3066\u3082\u540c\u69d8 # 0\u304b\u308916\u307e\u3067\u306e4x4\u884c\u5217 # => [[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11], # [12, 13, 14, 15]] y = np . arange ( 16 ) . reshape (( 4 , 4 )) # \u5168\u90e8\u306e\u5024 print y [:] # 2\u884c3\u5217\u306e\u8981\u7d20 print y [ 2 , 3 ] # \u4ee5\u4e0b\u3082\u540c\u69d8 print y [ 2 ][ 3 ] # 2\u884c\u76ee print y [ 2 ] # 3\u5217\u76ee print y [:, 3 ] # \u7e26\u30d9\u30af\u30c8\u30eb\u3068\u3057\u30661\u884c\u76ee\u3092\u53d6\u308a\u51fa\u3059 print y [:,: 1 ] # \u7e26\u30d9\u30af\u30c8\u30eb\u3068\u3057\u30662\u884c\u76ee\u3092\u53d6\u308a\u51fa\u3059 print y [:, 1 : 2 ] # 2\u884c\u76ee\u3092\u7e26\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u53d6\u308a\u51fa\u3059(\u5f8c\u306e\u6b21\u5143\u6570\u306e\u5897\u52a0\u306e\u8a18\u4e8b\u3092\u53c2\u7167) print ( y [ 2 ][:, np . newaxis ]) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [ 0 1 2 3 4 5 6 7 8 9 ] [ 9 8 7 6 5 4 3 2 1 0 ] [ 4 5 6 7 8 9 ] [ 0 1 2 3 4 5 6 ] [ 4 5 6 ] [ 1 3 5 7 ] [ 8 7 6 5 4 3 2 ] [ 8 6 4 2 ] [ 8 4 ] [[ 0 1 2 3 ] [ 4 5 6 7 ] [ 8 9 10 11 ] [ 12 13 14 15 ]] 11 11 [ 8 9 10 11 ] [ 3 7 11 15 ] [[ 0 ] [ 4 ] [ 8 ] [ 12 ]] [[ 1 ] [ 5 ] [ 9 ] [ 13 ]] [[ 8 ] [ 9 ] [ 10 ] [ 11 ]] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy008.ipynb","title":"\u30b9\u30e9\u30a4\u30b7\u30f3\u30b0"},{"location":"numpy/numpy008/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # coding:utf-8 import numpy as np # 0\u304b\u308910\u307e\u3067\u306e\u914d\u5217 x = np . arange ( 10 ) # => [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # \u5168\u90e8\u306e\u5024 print x [:] # \u5168\u90e8\u306e\u5024\u3092\u9006\u9806\u3067\u53d6\u5f97\u3059\u308b print x [:: - 1 ] # 4\u756a\u76ee\u4ee5\u964d\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print x [ 4 :] # 7\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print x [: 7 ] # 4\u756a\u76ee\u304b\u30897\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b print x [ 4 : 7 ] # 1\u756a\u76ee\u304b\u30898\u756a\u76ee\u307e\u3067\u306e\u5024\u30921\u3064\u98db\u3070\u3057\u3067\u53d6\u5f97\u3059\u308b print x [ 1 : 8 : 2 ] # 8\u756a\u76ee\u304b\u30891\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u9006\u9806\u3067\u53d6\u5f97\u3059\u308b print x [ 8 : 1 : - 1 ] #\u30008\u756a\u76ee\u304b\u30891\u756a\u76ee\u307e\u3067\u306e\u5024\u3092\u9006\u9806\u3067\u304b\u3064\u4e00\u3064\u98db\u3070\u3057\u3067\u53d6\u5f97\u3059\u308b #\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3088\u3046\u306bx[8:1:-1]\u3067\u5f97\u305f\u914d\u5217\u3092\u6b21\u306b[::2]\u3067\u4e00\u3064\u98db\u3070\u3057\u3067\u53d6\u5f97\u3059\u308b\u3001\u3068\u3044\u3046\u898b\u65b9 print ( x [ 8 : 1 : - 1 ][:: 2 ]) #\u3000\u3055\u3089\u306b\u4e00\u3064\u98db\u3070\u3057\u3057\u3066\u53d6\u5f97\u3059\u308b print ( x [ 8 : 1 : - 1 ][:: 2 ][:: 2 ]) # \u884c\u5217(\u591a\u6b21\u5143\u914d\u5217)\u306b\u95a2\u3057\u3066\u3082\u540c\u69d8 # 0\u304b\u308916\u307e\u3067\u306e4x4\u884c\u5217 # => [[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11], # [12, 13, 14, 15]] y = np . arange ( 16 ) . reshape (( 4 , 4 )) # \u5168\u90e8\u306e\u5024 print y [:] # 2\u884c3\u5217\u306e\u8981\u7d20 print y [ 2 , 3 ] # \u4ee5\u4e0b\u3082\u540c\u69d8 print y [ 2 ][ 3 ] # 2\u884c\u76ee print y [ 2 ] # 3\u5217\u76ee print y [:, 3 ] # \u7e26\u30d9\u30af\u30c8\u30eb\u3068\u3057\u30661\u884c\u76ee\u3092\u53d6\u308a\u51fa\u3059 print y [:,: 1 ] # \u7e26\u30d9\u30af\u30c8\u30eb\u3068\u3057\u30662\u884c\u76ee\u3092\u53d6\u308a\u51fa\u3059 print y [:, 1 : 2 ] # 2\u884c\u76ee\u3092\u7e26\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u53d6\u308a\u51fa\u3059(\u5f8c\u306e\u6b21\u5143\u6570\u306e\u5897\u52a0\u306e\u8a18\u4e8b\u3092\u53c2\u7167) print ( y [ 2 ][:, np . newaxis ]) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [ 0 1 2 3 4 5 6 7 8 9 ] [ 9 8 7 6 5 4 3 2 1 0 ] [ 4 5 6 7 8 9 ] [ 0 1 2 3 4 5 6 ] [ 4 5 6 ] [ 1 3 5 7 ] [ 8 7 6 5 4 3 2 ] [ 8 6 4 2 ] [ 8 4 ] [[ 0 1 2 3 ] [ 4 5 6 7 ] [ 8 9 10 11 ] [ 12 13 14 15 ]] 11 11 [ 8 9 10 11 ] [ 3 7 11 15 ] [[ 0 ] [ 4 ] [ 8 ] [ 12 ]] [[ 1 ] [ 5 ] [ 9 ] [ 13 ]] [[ 8 ] [ 9 ] [ 10 ] [ 11 ]]","title":"\u30b9\u30e9\u30a4\u30b7\u30f3\u30b0"},{"location":"numpy/numpy008/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy008.ipynb","title":"Notebook"},{"location":"numpy/numpy009/","text":"\u30d6\u30fc\u30eb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # coding:utf-8 import numpy as np # \u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u3088\u308b\u4e71\u6570\u309210\u30b3\u751f\u6210\u3059\u308b x = np . random . randn ( 10 ) print x # 0\u4ee5\u4e0a\u306e\u5834\u5408\u306b\u306fTrue\u3092\u8fd4\u3059 mask = x >= 0.0 print mask # True\u3068\u306a\u3063\u3066\u3044\u308b\u5024\u306e\u307f\u53d6\u5f97\u3059\u308b print x [ mask ] # \u4ee5\u4e0b\u3082\u540c\u69d8 print x [ x >= 0 ] \u51fa\u529b\u7d50\u679c 1 2 3 4 5 [ -0.446617 -0.75362278 -1.75269474 0 .12279827 1 .49845472 -1.02476642 -0.73013694 0 .20197336 0 .9706688 -0.35600103 ] [ False False False True True False False True True False ] [ 0 .12279827 1 .49845472 0 .20197336 0 .9706688 ] [ 0 .12279827 1 .49845472 0 .20197336 0 .9706688 ] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy009.ipynb","title":"\u30d6\u30fc\u30eb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9"},{"location":"numpy/numpy009/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # coding:utf-8 import numpy as np # \u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u3088\u308b\u4e71\u6570\u309210\u30b3\u751f\u6210\u3059\u308b x = np . random . randn ( 10 ) print x # 0\u4ee5\u4e0a\u306e\u5834\u5408\u306b\u306fTrue\u3092\u8fd4\u3059 mask = x >= 0.0 print mask # True\u3068\u306a\u3063\u3066\u3044\u308b\u5024\u306e\u307f\u53d6\u5f97\u3059\u308b print x [ mask ] # \u4ee5\u4e0b\u3082\u540c\u69d8 print x [ x >= 0 ] \u51fa\u529b\u7d50\u679c 1 2 3 4 5 [ -0.446617 -0.75362278 -1.75269474 0 .12279827 1 .49845472 -1.02476642 -0.73013694 0 .20197336 0 .9706688 -0.35600103 ] [ False False False True True False False True True False ] [ 0 .12279827 1 .49845472 0 .20197336 0 .9706688 ] [ 0 .12279827 1 .49845472 0 .20197336 0 .9706688 ]","title":"\u30d6\u30fc\u30eb\u30a4\u30f3\u30c7\u30c3\u30af\u30b9"},{"location":"numpy/numpy009/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy009.ipynb","title":"Notebook"},{"location":"numpy/numpy010/","text":"\u8ee2\u7f6e\u884c\u5217 Sample 1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 x = np . arange ( 9 ) . reshape ( 3 , 3 ) print x # \u8ee2\u7f6e\u884c\u5217 # \u4f8b 1\u884c2\u5217\u76ee\u306e\u8981\u7d20\u306f2\u884c1\u5217\u76ee\u306e\u8981\u7d20\u3068\u5165\u308c\u66ff\u308f\u308b print x . T \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 [[ 0 1 2 ] [ 3 4 5 ] [ 6 7 8 ]] [[ 0 3 6 ] [ 1 4 7 ] [ 2 5 8 ]]","title":"\u8ee2\u7f6e\u884c\u5217"},{"location":"numpy/numpy010/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 x = np . arange ( 9 ) . reshape ( 3 , 3 ) print x # \u8ee2\u7f6e\u884c\u5217 # \u4f8b 1\u884c2\u5217\u76ee\u306e\u8981\u7d20\u306f2\u884c1\u5217\u76ee\u306e\u8981\u7d20\u3068\u5165\u308c\u66ff\u308f\u308b print x . T \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 [[ 0 1 2 ] [ 3 4 5 ] [ 6 7 8 ]] [[ 0 3 6 ] [ 1 4 7 ] [ 2 5 8 ]]","title":"\u8ee2\u7f6e\u884c\u5217"},{"location":"numpy/numpy011/","text":"numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f\u3092\u884c\u3046 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import numpy as np # \u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u3088\u308b\u4e71\u6570\u3092\u5024\u306b\u6301\u30643x3\u884c\u5217 x = np . random . randn ( 3 , 3 ) print x # x\u3092\u4fdd\u5b58\u3059\u308b np . save ( 'random.npy' , x ) # x\u3092\u8aad\u307f\u8fbc\u3080 y = np . load ( 'random.npy' ) print y \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 [[ -0.75842355 -0.44355869 -1.27655413 ] [ 0 .0365756 -0.04802514 -0.21215251 ] [ 0 .60711507 1 .033892 0 .22241454 ]] [[ -0.75842355 -0.44355869 -1.27655413 ] [ 0 .0365756 -0.04802514 -0.21215251 ] [ 0 .60711507 1 .033892 0 .22241454 ]]","title":"numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f"},{"location":"numpy/numpy011/#numpy","text":"numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f\u3092\u884c\u3046 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import numpy as np # \u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u3088\u308b\u4e71\u6570\u3092\u5024\u306b\u6301\u30643x3\u884c\u5217 x = np . random . randn ( 3 , 3 ) print x # x\u3092\u4fdd\u5b58\u3059\u308b np . save ( 'random.npy' , x ) # x\u3092\u8aad\u307f\u8fbc\u3080 y = np . load ( 'random.npy' ) print y \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 [[ -0.75842355 -0.44355869 -1.27655413 ] [ 0 .0365756 -0.04802514 -0.21215251 ] [ 0 .60711507 1 .033892 0 .22241454 ]] [[ -0.75842355 -0.44355869 -1.27655413 ] [ 0 .0365756 -0.04802514 -0.21215251 ] [ 0 .60711507 1 .033892 0 .22241454 ]]","title":"numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f"},{"location":"numpy/numpy012/","text":"CSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u30fb\u4fdd\u5b58 CSV(\u30b3\u30f3\u30de\u533a\u5207\u308a , )\u304a\u3088\u3073TSV(\u30bf\u30d6\u533a\u5207\u308a \\t )\u304b\u3089\u5024\u3092\u8aad\u307f\u8fbc\u3080\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 # coding:utf-8 import numpy as np # CSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f x = np . loadtxt ( 'hoge.csv' , delimiter = ',' ) print x # TSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f y = np . loadtxt ( 'piyo.tsv' , delimiter = ' \\t ' ) print y # CSV\u30d5\u30a1\u30a4\u30eb\u306e\u4fdd\u5b58 np . savetxt ( 'foo.csv' , x , delimiter = ',' ) Datalab\u3067\u306f\u3001\u73fe\u5728\u3001Hello World.ipynb\u3092\u7de8\u96c6\u4e2d\u306a\u3089\u3001~/datalab/datalab/docs\u306e\u4e2d\u306b\u4f5c\u6210\u3059\u308b\u3002 hoge.csv : 1 2 0.2,3.398,592 1.3,0.83,8.8 tsv\u306e\u30b5\u30f3\u30d7\u30eb\u306f\u30b3\u30d4\u30fc\u3060\u3068\u3001Tab\u304c\u7121\u52b9\u306b\u306a\u308b\u306e\u3067\u3001Tab\u3092\u81ea\u5206\u3067\u3046\u3064\u3088\u3046\u306b\u3059\u308b piyo.tsv : 1 2 0.2 3.398 592 1.3 0.83 8.8 \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 [[ 1 .29809056 1 .81463907 ] [ 0 .73759474 -0.5688174 ] [ -1.45300168 0 .2555659 ] [ -0.18510103 -0.18096903 ] [ 0 .37652474 0 .46395745 ] [ -0.45630759 -0.41358571 ] [ -1.55313916 -0.09374661 ] [ 1 .27759706 2 .21715869 ] [ 0 .08562266 0 .67993018 ] [ 0 .14558376 0 .37880808 ]]","title":"CSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u30fb\u4fdd\u5b58"},{"location":"numpy/numpy012/#csv","text":"CSV(\u30b3\u30f3\u30de\u533a\u5207\u308a , )\u304a\u3088\u3073TSV(\u30bf\u30d6\u533a\u5207\u308a \\t )\u304b\u3089\u5024\u3092\u8aad\u307f\u8fbc\u3080\u3002","title":"CSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u30fb\u4fdd\u5b58"},{"location":"numpy/numpy012/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 # coding:utf-8 import numpy as np # CSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f x = np . loadtxt ( 'hoge.csv' , delimiter = ',' ) print x # TSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f y = np . loadtxt ( 'piyo.tsv' , delimiter = ' \\t ' ) print y # CSV\u30d5\u30a1\u30a4\u30eb\u306e\u4fdd\u5b58 np . savetxt ( 'foo.csv' , x , delimiter = ',' ) Datalab\u3067\u306f\u3001\u73fe\u5728\u3001Hello World.ipynb\u3092\u7de8\u96c6\u4e2d\u306a\u3089\u3001~/datalab/datalab/docs\u306e\u4e2d\u306b\u4f5c\u6210\u3059\u308b\u3002 hoge.csv : 1 2 0.2,3.398,592 1.3,0.83,8.8 tsv\u306e\u30b5\u30f3\u30d7\u30eb\u306f\u30b3\u30d4\u30fc\u3060\u3068\u3001Tab\u304c\u7121\u52b9\u306b\u306a\u308b\u306e\u3067\u3001Tab\u3092\u81ea\u5206\u3067\u3046\u3064\u3088\u3046\u306b\u3059\u308b piyo.tsv : 1 2 0.2 3.398 592 1.3 0.83 8.8 \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 [[ 1 .29809056 1 .81463907 ] [ 0 .73759474 -0.5688174 ] [ -1.45300168 0 .2555659 ] [ -0.18510103 -0.18096903 ] [ 0 .37652474 0 .46395745 ] [ -0.45630759 -0.41358571 ] [ -1.55313916 -0.09374661 ] [ 1 .27759706 2 .21715869 ] [ 0 .08562266 0 .67993018 ] [ 0 .14558376 0 .37880808 ]]","title":"Sample"},{"location":"numpy/numpy013/","text":"numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u578b\u30fb\u6b21\u5143\u6570\u30fb\u30b5\u30a4\u30ba Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 x = np . random . randn ( 3 , 3 ) print x # \u578b print x . dtype # \u6b21\u5143\u6570 print x . shape # \u30b5\u30a4\u30ba print x . size # 2x2x2\u591a\u6b21\u5143\u914d\u5217 y = np . arange ( 8 ) . reshape ( 2 , 2 , 2 ) print y # \u578b print y . dtype # \u6b21\u5143\u6570 print y . shape # \u30b5\u30a4\u30ba print y . size \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [[ -1.70284321 1 .3380814 -0.51146346 ] [ 1 .6092976 -0.65849397 -1.46087059 ] [ 0 .9852956 0 .15465021 -0.18471899 ]] float64 ( 3 , 3 ) 9 [[[ 0 1 ] [ 2 3 ]] [[ 4 5 ] [ 6 7 ]]] int64 ( 2 , 2 , 2 ) 8 Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy013.ipynb","title":"numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u578b\u30fb\u6b21\u5143\u6570\u30fb\u30b5\u30a4\u30ba"},{"location":"numpy/numpy013/#numpy","text":"","title":"numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u578b\u30fb\u6b21\u5143\u6570\u30fb\u30b5\u30a4\u30ba"},{"location":"numpy/numpy013/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 x = np . random . randn ( 3 , 3 ) print x # \u578b print x . dtype # \u6b21\u5143\u6570 print x . shape # \u30b5\u30a4\u30ba print x . size # 2x2x2\u591a\u6b21\u5143\u914d\u5217 y = np . arange ( 8 ) . reshape ( 2 , 2 , 2 ) print y # \u578b print y . dtype # \u6b21\u5143\u6570 print y . shape # \u30b5\u30a4\u30ba print y . size \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [[ -1.70284321 1 .3380814 -0.51146346 ] [ 1 .6092976 -0.65849397 -1.46087059 ] [ 0 .9852956 0 .15465021 -0.18471899 ]] float64 ( 3 , 3 ) 9 [[[ 0 1 ] [ 2 3 ]] [[ 4 5 ] [ 6 7 ]]] int64 ( 2 , 2 , 2 ) 8","title":"Sample"},{"location":"numpy/numpy013/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy013.ipynb","title":"Notebook"},{"location":"numpy/numpy014/","text":"\u5e73\u5747\u30fb\u5206\u6563\u30fb\u6a19\u6e96\u504f\u5dee Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 # [[0, 1, 2], # [3, 4, 5], # [6, 7, 8]] x = np . arange ( 9 ) . reshape ( 3 , 3 ) print x # \u5e73\u5747 print x . mean () # \u5206\u6563 print x . var () # \u6a19\u6e96\u504f\u5dee print x . std () # \u5217\u5358\u4f4d\u3067\u306e\u5e73\u5747\u30fb\u5206\u6563\u30fb\u6a19\u6e96\u504f\u5dee print x . mean ( 0 ) print x . var ( 0 ) print x . std ( 0 ) # \u884c\u5358\u4f4d\u3067\u306e\u5e73\u5747\u30fb\u5206\u6563\u30fb\u6a19\u6e96\u504f\u5dee print x . mean ( 1 ) print x . var ( 1 ) print x . std ( 1 ) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 [[ 0 1 2 ] [ 3 4 5 ] [ 6 7 8 ]] 4 .0 6 .66666666667 2 .58198889747 [ 3 . 4 . 5 . ] [ 6 . 6 . 6 . ] [ 2 .44948974 2 .44948974 2 .44948974 ] [ 1 . 4 . 7 . ] [ 0 .66666667 0 .66666667 0 .66666667 ] [ 0 .81649658 0 .81649658 0 .81649658 ] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy014.ipynb","title":"\u5e73\u5747\u30fb\u5206\u6563\u30fb\u6a19\u6e96\u504f\u5dee"},{"location":"numpy/numpy014/#_1","text":"","title":"\u5e73\u5747\u30fb\u5206\u6563\u30fb\u6a19\u6e96\u504f\u5dee"},{"location":"numpy/numpy014/#sample","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 # [[0, 1, 2], # [3, 4, 5], # [6, 7, 8]] x = np . arange ( 9 ) . reshape ( 3 , 3 ) print x # \u5e73\u5747 print x . mean () # \u5206\u6563 print x . var () # \u6a19\u6e96\u504f\u5dee print x . std () # \u5217\u5358\u4f4d\u3067\u306e\u5e73\u5747\u30fb\u5206\u6563\u30fb\u6a19\u6e96\u504f\u5dee print x . mean ( 0 ) print x . var ( 0 ) print x . std ( 0 ) # \u884c\u5358\u4f4d\u3067\u306e\u5e73\u5747\u30fb\u5206\u6563\u30fb\u6a19\u6e96\u504f\u5dee print x . mean ( 1 ) print x . var ( 1 ) print x . std ( 1 ) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 [[ 0 1 2 ] [ 3 4 5 ] [ 6 7 8 ]] 4 .0 6 .66666666667 2 .58198889747 [ 3 . 4 . 5 . ] [ 6 . 6 . 6 . ] [ 2 .44948974 2 .44948974 2 .44948974 ] [ 1 . 4 . 7 . ] [ 0 .66666667 0 .66666667 0 .66666667 ] [ 0 .81649658 0 .81649658 0 .81649658 ]","title":"Sample"},{"location":"numpy/numpy014/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy014.ipynb","title":"Notebook"},{"location":"numpy/numpy015/","text":"\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570 \u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570\u306f\u3001numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u305d\u308c\u305e\u308c\u306e\u8981\u7d20\u306b\u5bfe\u3057\u3066\u8a08\u7b97\u30fb\u64cd\u4f5c\u3092\u8fd4\u3059\u95a2\u6570\u3067\u3042\u308b\u3002 \u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570\u306e\u4f8b: \u95a2\u6570\u540d \u8aac\u660e np.add(x1, x2) \u52a0\u7b97 np.negative(x) -1\u3092\u639b\u3051\u305f\u5024\u3092\u8fd4\u3059 np.power(x1, x2) n\u4e57 np.exp(x) \u6307\u6570\u95a2\u6570 np.log(x) \u5bfe\u6570\u95a2\u6570 np.sqrt(x) \u5e73\u65b9\u6839 \u8a73\u3057\u304f\u306f \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 \u3092\u53c2\u8003\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # coding:utf-8 import numpy as np # 4x4\u884c\u5217 x = np . arange ( 1 , 17 ) . reshape ( 4 , 4 ) print x # \u52a0\u7b97 print np . add ( x , 1 ) # \u4ee5\u4e0b\u3082\u540c\u69d8 print x + 1 # -1\u3092\u639b\u3051\u308b print np . negative ( x ) # n\u4e57 print np . power ( x , 2 ) # \u6307\u6570\u95a2\u6570 print np . exp ( x ) # \u5bfe\u6570\u95a2\u6570 print np . log ( x ) # \u5e73\u65b9\u6839 print np . sqrt ( x ) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [[ 1 2 3 4 ] [ 5 6 7 8 ] [ 9 10 11 12 ] [ 13 14 15 16 ]] [[ 2 3 4 5 ] [ 6 7 8 9 ] [ 10 11 12 13 ] [ 14 15 16 17 ]] [[ 2 3 4 5 ] [ 6 7 8 9 ] [ 10 11 12 13 ] [ 14 15 16 17 ]] [[ -1 -2 -3 -4 ] [ -5 -6 -7 -8 ] [ -9 -10 -11 -12 ] [ -13 -14 -15 -16 ]] [[ 1 4 9 16 ] [ 25 36 49 64 ] [ 81 100 121 144 ] [ 169 196 225 256 ]] [[ 2 .71828183e+00 7 .38905610e+00 2 .00855369e+01 5 .45981500e+01 ] [ 1 .48413159e+02 4 .03428793e+02 1 .09663316e+03 2 .98095799e+03 ] [ 8 .10308393e+03 2 .20264658e+04 5 .98741417e+04 1 .62754791e+05 ] [ 4 .42413392e+05 1 .20260428e+06 3 .26901737e+06 8 .88611052e+06 ]] [[ 0 . 0 .69314718 1 .09861229 1 .38629436 ] [ 1 .60943791 1 .79175947 1 .94591015 2 .07944154 ] [ 2 .19722458 2 .30258509 2 .39789527 2 .48490665 ] [ 2 .56494936 2 .63905733 2 .7080502 2 .77258872 ]] [[ 1 . 1 .41421356 1 .73205081 2 . ] [ 2 .23606798 2 .44948974 2 .64575131 2 .82842712 ] [ 3 . 3 .16227766 3 .31662479 3 .46410162 ] [ 3 .60555128 3 .74165739 3 .87298335 4 . ]] \u53c2\u8003 Numpy\u306e\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570\u4e00\u89a7 https://docs.scipy.org/doc/numpy/reference/ufuncs.html#math-operations","title":"\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570"},{"location":"numpy/numpy015/#_1","text":"\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570\u306f\u3001numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u305d\u308c\u305e\u308c\u306e\u8981\u7d20\u306b\u5bfe\u3057\u3066\u8a08\u7b97\u30fb\u64cd\u4f5c\u3092\u8fd4\u3059\u95a2\u6570\u3067\u3042\u308b\u3002 \u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570\u306e\u4f8b: \u95a2\u6570\u540d \u8aac\u660e np.add(x1, x2) \u52a0\u7b97 np.negative(x) -1\u3092\u639b\u3051\u305f\u5024\u3092\u8fd4\u3059 np.power(x1, x2) n\u4e57 np.exp(x) \u6307\u6570\u95a2\u6570 np.log(x) \u5bfe\u6570\u95a2\u6570 np.sqrt(x) \u5e73\u65b9\u6839 \u8a73\u3057\u304f\u306f \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8 \u3092\u53c2\u8003\u3002 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # coding:utf-8 import numpy as np # 4x4\u884c\u5217 x = np . arange ( 1 , 17 ) . reshape ( 4 , 4 ) print x # \u52a0\u7b97 print np . add ( x , 1 ) # \u4ee5\u4e0b\u3082\u540c\u69d8 print x + 1 # -1\u3092\u639b\u3051\u308b print np . negative ( x ) # n\u4e57 print np . power ( x , 2 ) # \u6307\u6570\u95a2\u6570 print np . exp ( x ) # \u5bfe\u6570\u95a2\u6570 print np . log ( x ) # \u5e73\u65b9\u6839 print np . sqrt ( x ) \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [[ 1 2 3 4 ] [ 5 6 7 8 ] [ 9 10 11 12 ] [ 13 14 15 16 ]] [[ 2 3 4 5 ] [ 6 7 8 9 ] [ 10 11 12 13 ] [ 14 15 16 17 ]] [[ 2 3 4 5 ] [ 6 7 8 9 ] [ 10 11 12 13 ] [ 14 15 16 17 ]] [[ -1 -2 -3 -4 ] [ -5 -6 -7 -8 ] [ -9 -10 -11 -12 ] [ -13 -14 -15 -16 ]] [[ 1 4 9 16 ] [ 25 36 49 64 ] [ 81 100 121 144 ] [ 169 196 225 256 ]] [[ 2 .71828183e+00 7 .38905610e+00 2 .00855369e+01 5 .45981500e+01 ] [ 1 .48413159e+02 4 .03428793e+02 1 .09663316e+03 2 .98095799e+03 ] [ 8 .10308393e+03 2 .20264658e+04 5 .98741417e+04 1 .62754791e+05 ] [ 4 .42413392e+05 1 .20260428e+06 3 .26901737e+06 8 .88611052e+06 ]] [[ 0 . 0 .69314718 1 .09861229 1 .38629436 ] [ 1 .60943791 1 .79175947 1 .94591015 2 .07944154 ] [ 2 .19722458 2 .30258509 2 .39789527 2 .48490665 ] [ 2 .56494936 2 .63905733 2 .7080502 2 .77258872 ]] [[ 1 . 1 .41421356 1 .73205081 2 . ] [ 2 .23606798 2 .44948974 2 .64575131 2 .82842712 ] [ 3 . 3 .16227766 3 .31662479 3 .46410162 ] [ 3 .60555128 3 .74165739 3 .87298335 4 . ]]","title":"\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570"},{"location":"numpy/numpy015/#_2","text":"Numpy\u306e\u30e6\u30cb\u30d0\u30fc\u30b5\u30eb\u95a2\u6570\u4e00\u89a7 https://docs.scipy.org/doc/numpy/reference/ufuncs.html#math-operations","title":"\u53c2\u8003"},{"location":"numpy/numpy_arrmat/","text":"\u914d\u5217\u304b\u3089Tensor(\u884c\u5217)\u3092\u4f5c\u308b Sample 1 2 3 4 5 6 7 import numpy as np arr = np . array ([ 1 , 5 , 10 , 4 , 11 , 22 , 21 , 11 , 10 , 1 ]) print arr mat = arr . reshape ([ 5 , 2 ]) print mat \u7d50\u679c 1 2 3 4 5 [[ 1 5 ] [ 10 4 ] [ 11 22 ] [ 21 11 ] [ 10 1 ]]","title":"\u914d\u5217\u304b\u3089Tensor(\u884c\u5217)\u3092\u4f5c\u308b"},{"location":"numpy/numpy_arrmat/#tensor","text":"","title":"\u914d\u5217\u304b\u3089Tensor(\u884c\u5217)\u3092\u4f5c\u308b"},{"location":"numpy/numpy_arrmat/#sample","text":"1 2 3 4 5 6 7 import numpy as np arr = np . array ([ 1 , 5 , 10 , 4 , 11 , 22 , 21 , 11 , 10 , 1 ]) print arr mat = arr . reshape ([ 5 , 2 ]) print mat \u7d50\u679c 1 2 3 4 5 [[ 1 5 ] [ 10 4 ] [ 11 22 ] [ 21 11 ] [ 10 1 ]]","title":"Sample"},{"location":"numpy/numpy_concat/","text":"\u7d50\u5408\u95a2\u6570 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 x = np . arange ( 0 , 9 ) . reshape ( 3 , 3 ) # 3x3\u884c\u5217 y = np . random . randint ( 0 , 10 , ( 3 , 3 )) print x print y # \u884c\u5217\u306e\u7d50\u5408 # \u884c\u65b9\u5411\u306b\u884c\u5217\u3092\u7d50\u5408\u3059\u308b(\u30c7\u30d5\u30a9\u30eb\u30c8) print np . concatenate (( x , y ), axis = 0 ) # \u4ee5\u4e0b\u3082\u540c\u69d8 # print np.vstack((x,y)) # \u5217\u65b9\u5411\u306b\u884c\u5217\u3092\u7d50\u5408\u3059\u308b print np . concatenate (( x , y ), axis = 1 ) # \u4ee5\u4e0b\u3082\u540c\u69d8 # print np.hstack((x,y)) # \u540c\u3058\u884c\u3068\u5217\u306e\u8981\u7d20\u3092\u4e26\u3079\u308b print np . dstack (( x , y )) # 3x3\u884c\u5217\u3092\u30011\u5217\u76ee\u30012\u5217\u76ee\u3067\u5206\u5272\u3059\u308b x1 , x2 , x3 = np . split ( x ,( 1 , 2 )) print x1 print x2 print x3 \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [[0 1 2] [3 4 5] [6 7 8]] [[9 1 5] [3 3 4] [0 4 1]] [[0 1 2] [3 4 5] [6 7 8] [9 1 5] [3 3 4] [0 4 1]] [[0 1 2 9 1 5] [3 4 5 3 3 4] [6 7 8 0 4 1]] [[[0 9] [1 1] [2 5]] [[3 3] [4 3] [5 4]] [[6 0] [7 4] [8 1]]] [[0 1 2]] [[3 4 5]] [[6 7 8]] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_concat.ipynb","title":"\u7d50\u5408\u95a2\u6570"},{"location":"numpy/numpy_concat/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 x = np . arange ( 0 , 9 ) . reshape ( 3 , 3 ) # 3x3\u884c\u5217 y = np . random . randint ( 0 , 10 , ( 3 , 3 )) print x print y # \u884c\u5217\u306e\u7d50\u5408 # \u884c\u65b9\u5411\u306b\u884c\u5217\u3092\u7d50\u5408\u3059\u308b(\u30c7\u30d5\u30a9\u30eb\u30c8) print np . concatenate (( x , y ), axis = 0 ) # \u4ee5\u4e0b\u3082\u540c\u69d8 # print np.vstack((x,y)) # \u5217\u65b9\u5411\u306b\u884c\u5217\u3092\u7d50\u5408\u3059\u308b print np . concatenate (( x , y ), axis = 1 ) # \u4ee5\u4e0b\u3082\u540c\u69d8 # print np.hstack((x,y)) # \u540c\u3058\u884c\u3068\u5217\u306e\u8981\u7d20\u3092\u4e26\u3079\u308b print np . dstack (( x , y )) # 3x3\u884c\u5217\u3092\u30011\u5217\u76ee\u30012\u5217\u76ee\u3067\u5206\u5272\u3059\u308b x1 , x2 , x3 = np . split ( x ,( 1 , 2 )) print x1 print x2 print x3 \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [[0 1 2] [3 4 5] [6 7 8]] [[9 1 5] [3 3 4] [0 4 1]] [[0 1 2] [3 4 5] [6 7 8] [9 1 5] [3 3 4] [0 4 1]] [[0 1 2 9 1 5] [3 4 5 3 3 4] [6 7 8 0 4 1]] [[[0 9] [1 1] [2 5]] [[3 3] [4 3] [5 4]] [[6 0] [7 4] [8 1]]] [[0 1 2]] [[3 4 5]] [[6 7 8]]","title":"\u7d50\u5408\u95a2\u6570"},{"location":"numpy/numpy_concat/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_concat.ipynb","title":"Notebook"},{"location":"numpy/numpy_copy/","text":"Numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30b3\u30d4\u30fc Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import numpy as np # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b np . random . seed ( 20200724 ) # \u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570\u3092\u8981\u7d20\u306b\u6301\u30644x4\u884c\u5217 x = np . random . randn ( 2 , 2 ) print x # Numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30b3\u30d4\u30fc y = x . copy () print y \u5b9f\u884c\u7d50\u679c 1 2 3 4 [[ 0.061722 -0.57419489] [-0.22935131 0.96591975]] [[ 0.061722 -0.57419489] [-0.22935131 0.96591975]]","title":"Numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30b3\u30d4\u30fc"},{"location":"numpy/numpy_copy/#numpy","text":"Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 # coding:utf-8 import numpy as np # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b np . random . seed ( 20200724 ) # \u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570\u3092\u8981\u7d20\u306b\u6301\u30644x4\u884c\u5217 x = np . random . randn ( 2 , 2 ) print x # Numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30b3\u30d4\u30fc y = x . copy () print y \u5b9f\u884c\u7d50\u679c 1 2 3 4 [[ 0.061722 -0.57419489] [-0.22935131 0.96591975]] [[ 0.061722 -0.57419489] [-0.22935131 0.96591975]]","title":"Numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30b3\u30d4\u30fc"},{"location":"numpy/numpy_flatten/","text":"\u5e73\u5766\u5316 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 # [[0, 1, 2], # [3, 4, 5], # [6, 7, 8]] x = np . arange ( 9 ) . reshape ( 3 , 3 ) print x # \u884c\u5217\u3092\u5e73\u5766\u5316(\u914d\u5217\u5316)\u3059\u308b print x . flatten () # \u4ee5\u4e0b\u3082\u540c\u69d8 # \u305f\u3060\u3057\u3001\u30c7\u30fc\u30bf\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3055\u306a\u3044 print x . ravel () \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 [[0 1 2] [3 4 5] [6 7 8]] [0 1 2 3 4 5 6 7 8] [0 1 2 3 4 5 6 7 8]","title":"\u5e73\u5766\u5316"},{"location":"numpy/numpy_flatten/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # coding:utf-8 import numpy as np # 3x3\u884c\u5217 # [[0, 1, 2], # [3, 4, 5], # [6, 7, 8]] x = np . arange ( 9 ) . reshape ( 3 , 3 ) print x # \u884c\u5217\u3092\u5e73\u5766\u5316(\u914d\u5217\u5316)\u3059\u308b print x . flatten () # \u4ee5\u4e0b\u3082\u540c\u69d8 # \u305f\u3060\u3057\u3001\u30c7\u30fc\u30bf\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3055\u306a\u3044 print x . ravel () \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 [[0 1 2] [3 4 5] [6 7 8]] [0 1 2 3 4 5 6 7 8] [0 1 2 3 4 5 6 7 8]","title":"\u5e73\u5766\u5316"},{"location":"numpy/numpy_matrix_func/","text":"\u884c\u5217\u95a2\u6570 \u884c\u5217\u306e\u8a08\u7b97\u3092\u884c\u3046\u6570\u5b66\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u95a2\u6570\u540d \u8aac\u660e np.linalg.norm(x) \u30ce\u30eb\u30e0 np.diag(x) \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u3059\u308b np.linalg.det(x) \u884c\u5217\u5f0f np.linalg.inv(x) \u9006\u884c\u5217 np.dot(x) \u884c\u5217\u306e\u5185\u7a4d np.trace(x) \u5bfe\u89d2\u548c np.linalg.eig(x) \u56fa\u6709\u884c\u5217 np.eye(x) \u5358\u4f4d\u884c\u5217 np.linalg.solve(x, y) \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding:utf-8 import numpy as np # [ 1. 2. 3.] x = np . arange ( 1.0 , 4.0 ) print x # \u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0(\u8ddd\u96e2) print np . linalg . norm ( x ) # \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u3059\u308b diag_x = np . diag ( x ) # \u884c\u5217\u5f0f print np . linalg . det ( diag_x ) # \u9006\u884c\u5217 print np . linalg . inv ( diag_x ) # \u884c\u5217\u306e\u5185\u7a4d # 3x3\u5358\u4f4d\u884c\u5217 e = np . eye ( 3 ) print np . dot ( diag_x , e ) # \u5bfe\u89d2\u548c print np . trace ( diag_x ) # \u56fa\u6709\u5024\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb print np . linalg . eig ( diag_x ) # \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3 # 2x+y+z = 15 # 4x+6y+3z = 41 # 8x+8y+9z = 83 # \u89e3 : x=5,y=2,z=3 # \u85a9\u6469\u9806\u5409, \u56db\u30c4\u8c37\u6676\u4e8c, \"\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u7dda\u5f62\u4ee3\u6570\" p.2\u3088\u308a a = np . array ([[ 2 , 1 , 1 ],[ 4 , 6 , 3 ],[ 8 , 8 , 9 ]]) b = np . array ([[ 15 ],[ 41 ],[ 83 ]]) print np . linalg . solve ( a , b ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [ 1. 2. 3.] 3.74165738677 6.0 [[ 1. 0. 0. ] [ 0. 0.5 0. ] [ 0. 0. 0.33333333]] [[ 1. 0. 0.] [ 0. 2. 0.] [ 0. 0. 3.]] 6.0 (array([ 1., 2., 3.]), array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]])) [[ 5.] [ 2.] [ 3.]] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_matrix_func.ipynb","title":"\u884c\u5217\u95a2\u6570"},{"location":"numpy/numpy_matrix_func/#_1","text":"\u884c\u5217\u306e\u8a08\u7b97\u3092\u884c\u3046\u6570\u5b66\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb \u95a2\u6570\u540d \u8aac\u660e np.linalg.norm(x) \u30ce\u30eb\u30e0 np.diag(x) \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u3059\u308b np.linalg.det(x) \u884c\u5217\u5f0f np.linalg.inv(x) \u9006\u884c\u5217 np.dot(x) \u884c\u5217\u306e\u5185\u7a4d np.trace(x) \u5bfe\u89d2\u548c np.linalg.eig(x) \u56fa\u6709\u884c\u5217 np.eye(x) \u5358\u4f4d\u884c\u5217 np.linalg.solve(x, y) \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding:utf-8 import numpy as np # [ 1. 2. 3.] x = np . arange ( 1.0 , 4.0 ) print x # \u30d9\u30af\u30c8\u30eb\u306e\u30ce\u30eb\u30e0(\u8ddd\u96e2) print np . linalg . norm ( x ) # \u30d9\u30af\u30c8\u30eb\u3092\u5bfe\u89d2\u884c\u5217\u306b\u3059\u308b diag_x = np . diag ( x ) # \u884c\u5217\u5f0f print np . linalg . det ( diag_x ) # \u9006\u884c\u5217 print np . linalg . inv ( diag_x ) # \u884c\u5217\u306e\u5185\u7a4d # 3x3\u5358\u4f4d\u884c\u5217 e = np . eye ( 3 ) print np . dot ( diag_x , e ) # \u5bfe\u89d2\u548c print np . trace ( diag_x ) # \u56fa\u6709\u5024\u3001\u56fa\u6709\u30d9\u30af\u30c8\u30eb print np . linalg . eig ( diag_x ) # \u9023\u7acb\u65b9\u7a0b\u5f0f\u306e\u89e3 # 2x+y+z = 15 # 4x+6y+3z = 41 # 8x+8y+9z = 83 # \u89e3 : x=5,y=2,z=3 # \u85a9\u6469\u9806\u5409, \u56db\u30c4\u8c37\u6676\u4e8c, \"\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u7dda\u5f62\u4ee3\u6570\" p.2\u3088\u308a a = np . array ([[ 2 , 1 , 1 ],[ 4 , 6 , 3 ],[ 8 , 8 , 9 ]]) b = np . array ([[ 15 ],[ 41 ],[ 83 ]]) print np . linalg . solve ( a , b ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [ 1. 2. 3.] 3.74165738677 6.0 [[ 1. 0. 0. ] [ 0. 0.5 0. ] [ 0. 0. 0.33333333]] [[ 1. 0. 0.] [ 0. 2. 0.] [ 0. 0. 3.]] 6.0 (array([ 1., 2., 3.]), array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]])) [[ 5.] [ 2.] [ 3.]]","title":"\u884c\u5217\u95a2\u6570"},{"location":"numpy/numpy_matrix_func/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_matrix_func.ipynb","title":"Notebook"},{"location":"numpy/numpy_newaxis/","text":"\u6b21\u5143\u306e\u5897\u52a0 Sample 1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np x = np . arange ( 4 ) print x # \u6b21\u5143\u6570\u3092\u5897\u3084\u3059 print x [ np . newaxis ,:] # \u7e26\u65b9\u5411\u306b\u6b21\u5143\u6570\u3092\u5897\u3084\u3059 print x [:, np . newaxis ] \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 [0 1 2 3] [[0 1 2 3]] [[0] [1] [2] [3]]","title":"\u6b21\u5143\u306e\u5897\u52a0"},{"location":"numpy/numpy_newaxis/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 # coding:utf-8 import numpy as np x = np . arange ( 4 ) print x # \u6b21\u5143\u6570\u3092\u5897\u3084\u3059 print x [ np . newaxis ,:] # \u7e26\u65b9\u5411\u306b\u6b21\u5143\u6570\u3092\u5897\u3084\u3059 print x [:, np . newaxis ] \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 [0 1 2 3] [[0 1 2 3]] [[0] [1] [2] [3]]","title":"\u6b21\u5143\u306e\u5897\u52a0"},{"location":"numpy/numpy_randn/","text":"\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306eTensor(\u884c\u5217)\u3092\u4f5c\u308b \u5404\u5024\u306e\u5e73\u5747\u5024\u3092\u53d6\u308b\u30680\u3001\u6a19\u6e96\u504f\u5dee\u3092\u53d6\u308b\u30681\u306b\u53ce\u675f\u3059\u308b\u5206\u5e03\u306b\u306a\u308b\u3002 Sample 1 2 3 4 5 import numpy as np x = np . random . randn ( 10 , 2 ) print x \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 [[ 1 .29809056 1 .81463907 ] [ 0 .73759474 -0.5688174 ] [ -1.45300168 0 .2555659 ] [ -0.18510103 -0.18096903 ] [ 0 .37652474 0 .46395745 ] [ -0.45630759 -0.41358571 ] [ -1.55313916 -0.09374661 ] [ 1 .27759706 2 .21715869 ] [ 0 .08562266 0 .67993018 ] [ 0 .14558376 0 .37880808 ]] \u5b9f\u9a13 \u6b63\u898f\u5206\u5e03\u306e\u8981\u7d20\u30921\u4e07\u500b\u306b\u5897\u3084\u3057\u3001\u5e73\u5747\u5024\u3068\u6a19\u6e96\u504f\u5dee\u3092\u78ba\u8a8d\u3059\u308b\u3002 1 2 3 4 5 6 import numpy as np x = np . random . randn ( 5000 , 2 ) print np . average ( x ) print np . std ( x ) \u51fa\u529b\u7d50\u679c 1 2 0 .00136866896489 1 .00003484542 \u8981\u7d20\u6570\u3092\u5897\u3084\u305b\u3070\u5897\u3084\u3059\u307b\u3069\u3001\u5e73\u5747\u50240\u3001\u6a19\u6e96\u504f\u5dee 1\u306b\u8fd1\u3065\u3044\u3066\u3044\u304f\u3002","title":"\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306eTensor(\u884c\u5217)\u3092\u4f5c\u308b"},{"location":"numpy/numpy_randn/#tensor","text":"\u5404\u5024\u306e\u5e73\u5747\u5024\u3092\u53d6\u308b\u30680\u3001\u6a19\u6e96\u504f\u5dee\u3092\u53d6\u308b\u30681\u306b\u53ce\u675f\u3059\u308b\u5206\u5e03\u306b\u306a\u308b\u3002","title":"\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306eTensor(\u884c\u5217)\u3092\u4f5c\u308b"},{"location":"numpy/numpy_randn/#sample","text":"1 2 3 4 5 import numpy as np x = np . random . randn ( 10 , 2 ) print x \u51fa\u529b\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 [[ 1 .29809056 1 .81463907 ] [ 0 .73759474 -0.5688174 ] [ -1.45300168 0 .2555659 ] [ -0.18510103 -0.18096903 ] [ 0 .37652474 0 .46395745 ] [ -0.45630759 -0.41358571 ] [ -1.55313916 -0.09374661 ] [ 1 .27759706 2 .21715869 ] [ 0 .08562266 0 .67993018 ] [ 0 .14558376 0 .37880808 ]]","title":"Sample"},{"location":"numpy/numpy_randn/#_1","text":"\u6b63\u898f\u5206\u5e03\u306e\u8981\u7d20\u30921\u4e07\u500b\u306b\u5897\u3084\u3057\u3001\u5e73\u5747\u5024\u3068\u6a19\u6e96\u504f\u5dee\u3092\u78ba\u8a8d\u3059\u308b\u3002 1 2 3 4 5 6 import numpy as np x = np . random . randn ( 5000 , 2 ) print np . average ( x ) print np . std ( x ) \u51fa\u529b\u7d50\u679c 1 2 0 .00136866896489 1 .00003484542 \u8981\u7d20\u6570\u3092\u5897\u3084\u305b\u3070\u5897\u3084\u3059\u307b\u3069\u3001\u5e73\u5747\u50240\u3001\u6a19\u6e96\u504f\u5dee 1\u306b\u8fd1\u3065\u3044\u3066\u3044\u304f\u3002","title":"\u5b9f\u9a13"},{"location":"numpy/numpy_random/","text":"\u3055\u307e\u3056\u307e\u306a\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 \u95a2\u6570\u540d \u8aac\u660e np.random.binomial(n,p,size) \u4e8c\u9805\u5206\u5e03 np.random.normal(loc,scale,size) \u6b63\u898f\u5206\u5e03 np.random.beta(a,b,size) \u30d9\u30fc\u30bf\u5206\u5e03 np.random.chisquare(df,size) \u30ab\u30a4\u4e8c\u4e57\u5206\u5e03 np.random.gamma(shape,scale,size) \u30ac\u30f3\u30de\u95a2\u6570 np.random.uniform(low,high,size) \u533a\u95930<=x<1\u306e\u4e00\u69d8\u5206\u5e03 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # coding:utf-8 import numpy as np # \u4e8c\u9805\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 binomial = np . random . binomial ( 10 , 0.5 , ( 2 , 2 )) print binomial # \u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 normal = np . random . normal ( 0 , 1 , ( 2 , 2 )) print normal # \u30d9\u30fc\u30bf\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 beta = np . random . beta ( 1 , 1 , ( 2 , 2 )) print beta # \u30ab\u30a4\u4e8c\u4e57\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 chisquare = np . random . chisquare ( 1 , ( 2 , 2 )) print chisquare # \u30ac\u30f3\u30de\u5206\u5e03 gamma = np . random . gamma ( 2 , 1 , ( 2 , 2 )) print gamma # [0,1)\u306e\u4e00\u69d8\u5206\u5e03 uniform = np . random . uniform ( 0 , 1 , ( 2 , 2 )) print uniform \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 [[5 4] [6 2]] [[ 1.09813924 0.1587407 ] [-0.49877663 0.36559575]] [[ 0.67614291 0.14561996] [ 0.71546136 0.7953883 ]] [[ 0.67752021 4.52508705] [ 0.38940294 0.05195434]] [[ 1.16165311 1.95626104] [ 3.65589006 4.50888572]] [[ 0.43489813 0.25007418] [ 0.42769298 0.19222238]] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_random.ipynb","title":"\u3055\u307e\u3056\u307e\u306a\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570"},{"location":"numpy/numpy_random/#_1","text":"\u95a2\u6570\u540d \u8aac\u660e np.random.binomial(n,p,size) \u4e8c\u9805\u5206\u5e03 np.random.normal(loc,scale,size) \u6b63\u898f\u5206\u5e03 np.random.beta(a,b,size) \u30d9\u30fc\u30bf\u5206\u5e03 np.random.chisquare(df,size) \u30ab\u30a4\u4e8c\u4e57\u5206\u5e03 np.random.gamma(shape,scale,size) \u30ac\u30f3\u30de\u95a2\u6570 np.random.uniform(low,high,size) \u533a\u95930<=x<1\u306e\u4e00\u69d8\u5206\u5e03 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # coding:utf-8 import numpy as np # \u4e8c\u9805\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 binomial = np . random . binomial ( 10 , 0.5 , ( 2 , 2 )) print binomial # \u6b63\u898f\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 normal = np . random . normal ( 0 , 1 , ( 2 , 2 )) print normal # \u30d9\u30fc\u30bf\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 beta = np . random . beta ( 1 , 1 , ( 2 , 2 )) print beta # \u30ab\u30a4\u4e8c\u4e57\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570 chisquare = np . random . chisquare ( 1 , ( 2 , 2 )) print chisquare # \u30ac\u30f3\u30de\u5206\u5e03 gamma = np . random . gamma ( 2 , 1 , ( 2 , 2 )) print gamma # [0,1)\u306e\u4e00\u69d8\u5206\u5e03 uniform = np . random . uniform ( 0 , 1 , ( 2 , 2 )) print uniform \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 [[5 4] [6 2]] [[ 1.09813924 0.1587407 ] [-0.49877663 0.36559575]] [[ 0.67614291 0.14561996] [ 0.71546136 0.7953883 ]] [[ 0.67752021 4.52508705] [ 0.38940294 0.05195434]] [[ 1.16165311 1.95626104] [ 3.65589006 4.50888572]] [[ 0.43489813 0.25007418] [ 0.42769298 0.19222238]]","title":"\u3055\u307e\u3056\u307e\u306a\u5206\u5e03\u306b\u5f93\u3046\u4e71\u6570"},{"location":"numpy/numpy_random/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_random.ipynb","title":"Notebook"},{"location":"numpy/numpy_repeat/","text":"\u914d\u5217\u306e\u7e70\u308a\u8fd4\u3057 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # coding:utf-8 import numpy as np # \u914d\u5217\u306e\u5404\u8981\u7d20\u30924\u3064\u305a\u3064\u4e26\u3079\u308b x = np . array ([ 3 , 6 , 9 ]) . repeat ( 4 ) print x # \u914d\u5217\u306e\u5185\u5bb9\u30923\u3064\u4e26\u3079\u308b y = np . tile ([ 7 , 2 , 4 ], 3 ) print y # \u914d\u5217\u306e\u5185\u5bb9\u30922x2\u3067\u4e26\u3079\u308b z = np . tile ([ 7 , 2 , 4 ], ( 2 , 2 )) print z \u5b9f\u884c\u7d50\u679c 1 2 3 4 [3 3 3 3 6 6 6 6 9 9 9 9] [7 2 4 7 2 4 7 2 4] [[7 2 4 7 2 4] [7 2 4 7 2 4]] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_repeat.ipynb","title":"\u914d\u5217\u306e\u7e70\u308a\u8fd4\u3057"},{"location":"numpy/numpy_repeat/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # coding:utf-8 import numpy as np # \u914d\u5217\u306e\u5404\u8981\u7d20\u30924\u3064\u305a\u3064\u4e26\u3079\u308b x = np . array ([ 3 , 6 , 9 ]) . repeat ( 4 ) print x # \u914d\u5217\u306e\u5185\u5bb9\u30923\u3064\u4e26\u3079\u308b y = np . tile ([ 7 , 2 , 4 ], 3 ) print y # \u914d\u5217\u306e\u5185\u5bb9\u30922x2\u3067\u4e26\u3079\u308b z = np . tile ([ 7 , 2 , 4 ], ( 2 , 2 )) print z \u5b9f\u884c\u7d50\u679c 1 2 3 4 [3 3 3 3 6 6 6 6 9 9 9 9] [7 2 4 7 2 4 7 2 4] [[7 2 4 7 2 4] [7 2 4 7 2 4]]","title":"\u914d\u5217\u306e\u7e70\u308a\u8fd4\u3057"},{"location":"numpy/numpy_repeat/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_repeat.ipynb","title":"Notebook"},{"location":"numpy/numpy_set_func/","text":"\u96c6\u5408\u95a2\u6570 Numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5bfe\u3059\u308b\u96c6\u5408\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb np.unique(x) \u91cd\u8907\u3057\u305f\u5024\u3092\u7701\u304f np.intersect1d(x,y) \u7a4d\u96c6\u5408 np.union1d(x,y) \u548c\u96c6\u5408 np.setdiff1d(x,y) \u5dee\u96c6\u5408 np.setxor1d(x,y) \u6392\u4ed6\u7684\u8ad6\u7406\u548c np.in1d(x,y) \u5305\u542b Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # coding:utf-8 import numpy as np # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b np . random . seed ( 20200724 ) # \u7bc4\u56f20\u301c4\u306e\u4e71\u6570\u309210\u500b\u751f\u6210\u3059\u308b x = np . random . randint ( 0 , 4 , 10 ) print x # \u91cd\u8907\u3057\u305f\u5024\u3092\u53d6\u308a\u9664\u304f print np . unique ( x ) # \u96c6\u5408\u6f14\u7b97 # [0, 1, 2, 3, 4] a = np . arange ( 5 ) # [3, 4, 5, 6, 7] b = np . arange ( 3 , 8 ) print a print b # \u7a4d\u96c6\u5408 a \u2229 b print np . intersect1d ( a , b ) # \u548c\u96c6\u5408 a \u222a b print np . union1d ( a , b ) # \u5dee\u96c6\u5408 a - b print np . setdiff1d ( a , b ) # \u6392\u4ed6\u7684\u8ad6\u7406\u548c a xor b print np . setxor1d ( a , b ) # \u914d\u5217b\u306e\u5404\u8981\u7d20\u304c\u3001\u914d\u5217a\u306e\u4e2d\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u304b\u8abf\u3079\u308b print np . in1d ( a , b ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 [1 0 3 1 3 1 2 0 1 3] [0 1 2 3] [0 1 2 3 4] [3 4 5 6 7] [3 4] [0 1 2 3 4 5 6 7] [0 1 2] [0 1 2 5 6 7] [False False False True True] Notebook https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_repeat.ipynb","title":"\u96c6\u5408\u95a2\u6570"},{"location":"numpy/numpy_set_func/#_1","text":"Numpy\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5bfe\u3059\u308b\u96c6\u5408\u95a2\u6570\u306e\u30b5\u30f3\u30d7\u30eb np.unique(x) \u91cd\u8907\u3057\u305f\u5024\u3092\u7701\u304f np.intersect1d(x,y) \u7a4d\u96c6\u5408 np.union1d(x,y) \u548c\u96c6\u5408 np.setdiff1d(x,y) \u5dee\u96c6\u5408 np.setxor1d(x,y) \u6392\u4ed6\u7684\u8ad6\u7406\u548c np.in1d(x,y) \u5305\u542b Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # coding:utf-8 import numpy as np # \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\u3059\u308b np . random . seed ( 20200724 ) # \u7bc4\u56f20\u301c4\u306e\u4e71\u6570\u309210\u500b\u751f\u6210\u3059\u308b x = np . random . randint ( 0 , 4 , 10 ) print x # \u91cd\u8907\u3057\u305f\u5024\u3092\u53d6\u308a\u9664\u304f print np . unique ( x ) # \u96c6\u5408\u6f14\u7b97 # [0, 1, 2, 3, 4] a = np . arange ( 5 ) # [3, 4, 5, 6, 7] b = np . arange ( 3 , 8 ) print a print b # \u7a4d\u96c6\u5408 a \u2229 b print np . intersect1d ( a , b ) # \u548c\u96c6\u5408 a \u222a b print np . union1d ( a , b ) # \u5dee\u96c6\u5408 a - b print np . setdiff1d ( a , b ) # \u6392\u4ed6\u7684\u8ad6\u7406\u548c a xor b print np . setxor1d ( a , b ) # \u914d\u5217b\u306e\u5404\u8981\u7d20\u304c\u3001\u914d\u5217a\u306e\u4e2d\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u304b\u8abf\u3079\u308b print np . in1d ( a , b ) \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 [1 0 3 1 3 1 2 0 1 3] [0 1 2 3] [0 1 2 3 4] [3 4 5 6 7] [3 4] [0 1 2 3 4 5 6 7] [0 1 2] [0 1 2 5 6 7] [False False False True True]","title":"\u96c6\u5408\u95a2\u6570"},{"location":"numpy/numpy_set_func/#notebook","text":"https://github.com/FaBoPlatform/TensorFlow/blob/master/numpy/numpy_repeat.ipynb","title":"Notebook"},{"location":"numpy/numpy_sort/","text":"\u30bd\u30fc\u30c8 Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # coding:utf-8 import numpy as np # \u7bc4\u56f20\u301c100\u306e\u4e71\u6570\u3092\u6301\u30641\u6b21\u5143\u914d\u5217 x = np . random . randint ( 0 , 100 , 10 ) # \u7bc4\u56f20\u301c100\u306e\u4e71\u6570\u3092\u6301\u30644x4\u884c\u5217 y = np . random . randint ( 0 , 100 , ( 4 , 4 )) print x # \u6607\u9806\u306b\u6574\u5217\u3059\u308b # \u6ce8\u610f: np.sort()\u306f\u7834\u58ca\u7684\u64cd\u4f5c x . sort () print x print y # y\u3092\u30b3\u30d4\u30fc\u3059\u308b y1 = np . array ( y ) y2 = np . array ( y ) # \u5217\u5358\u4f4d\u3067\u6607\u9806\u306b\u6574\u5217\u3059\u308b y1 . sort ( 0 ) print y1 # \u884c\u5358\u4f4d\u3067\u6607\u9806\u306b\u6574\u5217\u3059\u308b # \u5f15\u6570\u304c\u306a\u3044\u5834\u5408\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8 y2 . sort ( 1 ) print y2 \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [54 97 92 27 95 9 15 89 46 69] [ 9 15 27 46 54 69 89 92 95 97] [[22 87 16 9] [14 22 64 7] [77 90 27 39] [87 25 79 0]] [[14 22 16 0] [22 25 27 7] [77 87 64 9] [87 90 79 39]] [[ 9 16 22 87] [ 7 14 22 64] [27 39 77 90] [ 0 25 79 87]]","title":"\u30bd\u30fc\u30c8"},{"location":"numpy/numpy_sort/#_1","text":"Sample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # coding:utf-8 import numpy as np # \u7bc4\u56f20\u301c100\u306e\u4e71\u6570\u3092\u6301\u30641\u6b21\u5143\u914d\u5217 x = np . random . randint ( 0 , 100 , 10 ) # \u7bc4\u56f20\u301c100\u306e\u4e71\u6570\u3092\u6301\u30644x4\u884c\u5217 y = np . random . randint ( 0 , 100 , ( 4 , 4 )) print x # \u6607\u9806\u306b\u6574\u5217\u3059\u308b # \u6ce8\u610f: np.sort()\u306f\u7834\u58ca\u7684\u64cd\u4f5c x . sort () print x print y # y\u3092\u30b3\u30d4\u30fc\u3059\u308b y1 = np . array ( y ) y2 = np . array ( y ) # \u5217\u5358\u4f4d\u3067\u6607\u9806\u306b\u6574\u5217\u3059\u308b y1 . sort ( 0 ) print y1 # \u884c\u5358\u4f4d\u3067\u6607\u9806\u306b\u6574\u5217\u3059\u308b # \u5f15\u6570\u304c\u306a\u3044\u5834\u5408\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8 y2 . sort ( 1 ) print y2 \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [54 97 92 27 95 9 15 89 46 69] [ 9 15 27 46 54 69 89 92 95 97] [[22 87 16 9] [14 22 64 7] [77 90 27 39] [87 25 79 0]] [[14 22 16 0] [22 25 27 7] [77 87 64 9] [87 90 79 39]] [[ 9 16 22 87] [ 7 14 22 64] [27 39 77 90] [ 0 25 79 87]]","title":"\u30bd\u30fc\u30c8"},{"location":"numpy/numpy_zeroarray/","text":"\u30bc\u30ed\u306e\u306f\u3044\u3063\u305f\u914d\u5217\u3092\u4f5c\u308b Sample 1 2 3 4 5 import numpy as np arr = np . array ([ 0 ] * 10 ) print arr \u7d50\u679c 1 [ 0 0 0 0 0 0 0 0 0 0 ]","title":"\u30bc\u30ed\u306e\u306f\u3044\u3063\u305f\u914d\u5217\u3092\u4f5c\u308b"},{"location":"numpy/numpy_zeroarray/#_1","text":"","title":"\u30bc\u30ed\u306e\u306f\u3044\u3063\u305f\u914d\u5217\u3092\u4f5c\u308b"},{"location":"numpy/numpy_zeroarray/#sample","text":"1 2 3 4 5 import numpy as np arr = np . array ([ 0 ] * 10 ) print arr \u7d50\u679c 1 [ 0 0 0 0 0 0 0 0 0 0 ]","title":"Sample"},{"location":"numpy/numpy_zerotensor/","text":"\u30bc\u30ed\u306e\u306f\u3044\u3063\u305fTensor(\u884c\u5217)\u3092\u4f5c\u308b Sample 1 2 3 4 import numpy as np mat = np . zeros ([ 3 , 2 ]) print mat \u7d50\u679c 1 2 3 [[ 0 . 0 . ] [ 0 . 0 . ] [ 0 . 0 . ]]","title":"\u30bc\u30ed\u306e\u306f\u3044\u3063\u305fTensor(\u884c\u5217)\u3092\u4f5c\u308b"},{"location":"numpy/numpy_zerotensor/#tensor","text":"","title":"\u30bc\u30ed\u306e\u306f\u3044\u3063\u305fTensor(\u884c\u5217)\u3092\u4f5c\u308b"},{"location":"numpy/numpy_zerotensor/#sample","text":"1 2 3 4 import numpy as np mat = np . zeros ([ 3 , 2 ]) print mat \u7d50\u679c 1 2 3 [[ 0 . 0 . ] [ 0 . 0 . ] [ 0 . 0 . ]]","title":"Sample"},{"location":"raspberryPI/build/","text":"RaspberryPi\u306bTensorFlow\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b \u624b\u9806 TensorFlow\u306b\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb wheel\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb mock\u306e\u518d\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Tensorflow 1.0.1 \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b \u4ed6\u30d0\u30fc\u30b8\u30e7\u30f3\u306eTensorFlow\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u30da\u30fc\u30b8\u3092\u53c2\u8003\u306b\u3057\u3066\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308bwheel\u30d5\u30a1\u30a4\u30eb\u3092\u5909\u66f4\u3059\u308b https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases Python2.7\u7cfb \uff1a 1 2 3 4 5 6 $ sudo apt-get update $ sudo apt-get install python-pip python-dev $ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.0.1/tensorflow-1.0.1-cp27-none-linux_armv7l.whl $ sudo pip install tensorflow-1.0.1-cp27-none-linux_armv7l.whl $ sudo pip uninstall mock $ sudo pip install mock Python3\u7cfb \uff1a 1 2 3 4 5 6 $ sudo apt-get update $ sudo apt-get install python3-pip python3-dev $ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.0.1/tensorflow-1.0.1-cp34-cp34m-linux_armv7l.whl $ sudo pip3 install tensorflow-1.0.1-cp34-cp34m-linux_armv7l.whl $ sudo pip3 uninstall mock $ sudo pip3 install mock TensorFlow\u306e\u5b9f\u884c TensorFlow\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u8868\u793a\u3055\u305b\u308b\u3002 Python2.7\u7cfb \uff1a 1 2 3 4 5 6 7 $ python Python 2.7 . 9 ( default , Sep 17 2016 , 20 : 26 : 04 ) [ GCC 4.9 . 2 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import tensorflow as tf >>> tf . VERSION '1.0.1' Python3\u7cfb \uff1a 1 2 3 4 5 6 7 $ python3 Python 3.4 . 2 ( default , Oct 19 2014 , 13 : 31 : 11 ) [ GCC 4.9 . 1 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import tensorflow as tf >>> tf . VERSION '1.0.1' \u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb 1 2 3 4 # python2.7+ $ sudo pip uninstall tensorflow # python3+ $ sudo pip uninstall tensorflow \u5b9f\u884c\u74b0\u5883 Raspberry Pi3 Model B Raspbian Jessie Lite 2016-09-23 Python 2.7.9 Python 3.4.2 \u53c2\u8003 samjabrahams/tensorflow-on-raspberry-pi","title":"RaspberryPi\u306bTensorFlow\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b"},{"location":"raspberryPI/build/#raspberrypitensorflow","text":"","title":"RaspberryPi\u306bTensorFlow\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b"},{"location":"raspberryPI/build/#_1","text":"TensorFlow\u306b\u5fc5\u8981\u306a\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb wheel\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066pip\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb mock\u306e\u518d\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","title":"\u624b\u9806"},{"location":"raspberryPI/build/#_2","text":"Tensorflow 1.0.1 \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b \u4ed6\u30d0\u30fc\u30b8\u30e7\u30f3\u306eTensorFlow\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306e\u30da\u30fc\u30b8\u3092\u53c2\u8003\u306b\u3057\u3066\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308bwheel\u30d5\u30a1\u30a4\u30eb\u3092\u5909\u66f4\u3059\u308b https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases Python2.7\u7cfb \uff1a 1 2 3 4 5 6 $ sudo apt-get update $ sudo apt-get install python-pip python-dev $ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.0.1/tensorflow-1.0.1-cp27-none-linux_armv7l.whl $ sudo pip install tensorflow-1.0.1-cp27-none-linux_armv7l.whl $ sudo pip uninstall mock $ sudo pip install mock Python3\u7cfb \uff1a 1 2 3 4 5 6 $ sudo apt-get update $ sudo apt-get install python3-pip python3-dev $ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.0.1/tensorflow-1.0.1-cp34-cp34m-linux_armv7l.whl $ sudo pip3 install tensorflow-1.0.1-cp34-cp34m-linux_armv7l.whl $ sudo pip3 uninstall mock $ sudo pip3 install mock","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"raspberryPI/build/#tensorflow","text":"TensorFlow\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u8868\u793a\u3055\u305b\u308b\u3002 Python2.7\u7cfb \uff1a 1 2 3 4 5 6 7 $ python Python 2.7 . 9 ( default , Sep 17 2016 , 20 : 26 : 04 ) [ GCC 4.9 . 2 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import tensorflow as tf >>> tf . VERSION '1.0.1' Python3\u7cfb \uff1a 1 2 3 4 5 6 7 $ python3 Python 3.4 . 2 ( default , Oct 19 2014 , 13 : 31 : 11 ) [ GCC 4.9 . 1 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> import tensorflow as tf >>> tf . VERSION '1.0.1'","title":"TensorFlow\u306e\u5b9f\u884c"},{"location":"raspberryPI/build/#_3","text":"1 2 3 4 # python2.7+ $ sudo pip uninstall tensorflow # python3+ $ sudo pip uninstall tensorflow","title":"\u30a2\u30f3\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"raspberryPI/build/#_4","text":"Raspberry Pi3 Model B Raspbian Jessie Lite 2016-09-23 Python 2.7.9 Python 3.4.2","title":"\u5b9f\u884c\u74b0\u5883"},{"location":"raspberryPI/build/#_5","text":"samjabrahams/tensorflow-on-raspberry-pi","title":"\u53c2\u8003"},{"location":"raspberryPI/mnist_on_rp3/","text":"RaspberryPi\u3067\u5b66\u7fd2\u6e08\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u52d5\u304b\u3059 RaspberryPi\u4e0a\u3067\u5b66\u7fd2\u6e08\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5229\u7528\u3059\u308b \u5b9f\u884c\u74b0\u5883 Raspberry Pi3 Model B Raspbian Jessie Lite 2016-09-23 Python 2.7.9 Python 3.4.2 TensorFlow 1.0.0 \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # coding:utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) # pb\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\u3092\u6307\u5b9a\u3059\u308b model_fn = \"./mnist_graph.pb\" # pb\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30b0\u30e9\u30d5\u60c5\u5831\u3092\u8aad\u307f\u8fbc\u3080 f = tf . gfile . FastGFile ( model_fn , 'rb' ) graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) tf . import_graph_def ( graph_def , name = \"\" ) f . close () sess = tf . Session () # \u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u304a\u3088\u3073\u30c6\u30f3\u30bd\u30eb\u306e\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b for op in sess . graph . get_operations (): print ( op . name , op . outputs ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30921000\u500b\u5229\u7528 acc = sess . run ( \"accuracy:0\" , feed_dict = { \"X:0\" : mnist . test . images [: 1000 ], \"t:0\" : mnist . test . labels [: 1000 ], \"keep_prob:0\" : 1.0 }) print ( \"accuracy: %f \" % acc ) sess . close () MNIST\u306e\u5b66\u7fd2\u6e08\u307f\u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306epb\u30d5\u30a1\u30a4\u30eb\u306f \u3053\u3061\u3089\u306eURL \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u53ef\u80fd 1 $ curl -O https://www.dropbox.com/s/incxw1qtan68y3a/mnist_graph.pb \u5b9f\u884c\u7d50\u679c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Extracting MNIST_data / train - images - idx3 - ubyte . gz Extracting MNIST_data / train - labels - idx1 - ubyte . gz Extracting MNIST_data / t10k - images - idx3 - ubyte . gz Extracting MNIST_data / t10k - labels - idx1 - ubyte . gz ( u 't' , [ < tf . Tensor 't:0' shape =< unknown > dtype = float32 > ]) ( u 'X' , [ < tf . Tensor 'X:0' shape =< unknown > dtype = float32 > ]) ( u 'Reshape/shape' , [ < tf . Tensor 'Reshape/shape:0' shape = ( 4 ,) dtype = int32 > ]) ... \u7565 ... ( u 'add_2' , [ < tf . Tensor 'add_2:0' shape = ( ? , 1024 ) dtype = float32 > ]) ( u 'Relu_2' , [ < tf . Tensor 'Relu_2:0' shape = ( ? , 1024 ) dtype = float32 > ]) ( u 'keep_prob' , [ < tf . Tensor 'keep_prob:0' shape =< unknown > dtype = float32 > ]) ... \u7565 ... ( u 'Cast_1' , [ < tf . Tensor 'Cast_1:0' shape =< unknown > dtype = float32 > ]) ( u 'Const_5' , [ < tf . Tensor 'Const_5:0' shape = ( 1 ,) dtype = int32 > ]) ( u 'accuracy' , [ < tf . Tensor 'accuracy:0' shape =< unknown > dtype = float32 > ]) accuracy : 0.992000","title":"RaspberryPi\u3067\u5b66\u7fd2\u6e08\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u52d5\u304b\u3059"},{"location":"raspberryPI/mnist_on_rp3/#raspberrypi","text":"RaspberryPi\u4e0a\u3067\u5b66\u7fd2\u6e08\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u5229\u7528\u3059\u308b","title":"RaspberryPi\u3067\u5b66\u7fd2\u6e08\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u52d5\u304b\u3059"},{"location":"raspberryPI/mnist_on_rp3/#_1","text":"Raspberry Pi3 Model B Raspbian Jessie Lite 2016-09-23 Python 2.7.9 Python 3.4.2 TensorFlow 1.0.0","title":"\u5b9f\u884c\u74b0\u5883"},{"location":"raspberryPI/mnist_on_rp3/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # coding:utf-8 from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( 'MNIST_data' , one_hot = True ) # pb\u30d5\u30a1\u30a4\u30eb\u306e\u30d1\u30b9\u3092\u6307\u5b9a\u3059\u308b model_fn = \"./mnist_graph.pb\" # pb\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u30b0\u30e9\u30d5\u60c5\u5831\u3092\u8aad\u307f\u8fbc\u3080 f = tf . gfile . FastGFile ( model_fn , 'rb' ) graph_def = tf . GraphDef () graph_def . ParseFromString ( f . read ()) tf . import_graph_def ( graph_def , name = \"\" ) f . close () sess = tf . Session () # \u30aa\u30da\u30ec\u30fc\u30b7\u30e7\u30f3\u304a\u3088\u3073\u30c6\u30f3\u30bd\u30eb\u306e\u4e00\u89a7\u3092\u8868\u793a\u3059\u308b for op in sess . graph . get_operations (): print ( op . name , op . outputs ) # \u30c6\u30b9\u30c8\u7528\u30c7\u30fc\u30bf\u30921000\u500b\u5229\u7528 acc = sess . run ( \"accuracy:0\" , feed_dict = { \"X:0\" : mnist . test . images [: 1000 ], \"t:0\" : mnist . test . labels [: 1000 ], \"keep_prob:0\" : 1.0 }) print ( \"accuracy: %f \" % acc ) sess . close () MNIST\u306e\u5b66\u7fd2\u6e08\u307f\u7573\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306epb\u30d5\u30a1\u30a4\u30eb\u306f \u3053\u3061\u3089\u306eURL \u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u53ef\u80fd 1 $ curl -O https://www.dropbox.com/s/incxw1qtan68y3a/mnist_graph.pb","title":"\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9"},{"location":"raspberryPI/mnist_on_rp3/#_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Extracting MNIST_data / train - images - idx3 - ubyte . gz Extracting MNIST_data / train - labels - idx1 - ubyte . gz Extracting MNIST_data / t10k - images - idx3 - ubyte . gz Extracting MNIST_data / t10k - labels - idx1 - ubyte . gz ( u 't' , [ < tf . Tensor 't:0' shape =< unknown > dtype = float32 > ]) ( u 'X' , [ < tf . Tensor 'X:0' shape =< unknown > dtype = float32 > ]) ( u 'Reshape/shape' , [ < tf . Tensor 'Reshape/shape:0' shape = ( 4 ,) dtype = int32 > ]) ... \u7565 ... ( u 'add_2' , [ < tf . Tensor 'add_2:0' shape = ( ? , 1024 ) dtype = float32 > ]) ( u 'Relu_2' , [ < tf . Tensor 'Relu_2:0' shape = ( ? , 1024 ) dtype = float32 > ]) ( u 'keep_prob' , [ < tf . Tensor 'keep_prob:0' shape =< unknown > dtype = float32 > ]) ... \u7565 ... ( u 'Cast_1' , [ < tf . Tensor 'Cast_1:0' shape =< unknown > dtype = float32 > ]) ( u 'Const_5' , [ < tf . Tensor 'Const_5:0' shape = ( 1 ,) dtype = int32 > ]) ( u 'accuracy' , [ < tf . Tensor 'accuracy:0' shape =< unknown > dtype = float32 > ]) accuracy : 0.992000","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"raspberryPI/raspberry-network/","text":"RaspberryPi3 (OS:Raspbian) \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u63a5\u7d9a \u30af\u30ed\u30fc\u30ba\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3001RaspberryPi\u4e0a\u3067\u8d77\u52d5\u3057\u305fjupyter\u306bPC\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u6a5f\u5668\u306fRaspberryPi\u306e\u8d77\u52d5\u3067\u96fb\u6e90ON\u3068\u306a\u308bUSB\u7d66\u96fb\u30bf\u30a4\u30d7\u3092\u9078\u629e RaspberryPi3\u306fWiFi\u30e2\u30b8\u30e5\u30fc\u30eb\u5185\u81d3\u306a\u306e\u3067WiFi\u30eb\u30fc\u30bf\u3082\u9078\u629e\u53ef\u80fd \u3061\u3073\u30d5\u30a1\u30a4\u306f\u3059\u3079\u3066\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u4ed5\u69d8\u304c\u7570\u306a\u308b\u306e\u3067\u6ce8\u610f \u30eb\u30fc\u30bf <- WiFi -> RaspberryPi \u30eb\u30fc\u30bf <- WiFi -> PC \u5bfe\u5fdc\uff1a \u3061\u3073\u30d5\u30a1\u30a4 (MZK-RP150NA) \u3061\u3073\u30d5\u30a1\u30a42 (MZK-UE150N) \u3061\u3073\u30d5\u30a1\u30a42ac (MZK-UE450AC) \u3061\u3073\u30d5\u30a1\u30a43 (MZK-DP150N,MZK-DP150N/R) MZK-DP150N/R\u306f\u7bb1\u306e\u30e9\u30d9\u30eb\u3067\u672c\u4f53\u578b\u756a\u306fMZK-DP150N\u306e\u307f \u3061\u3073\u30d5\u30a1\u30a43\u306fUSB\u7d66\u96fb\u51fa\u6765\u306a\u3044\u305f\u3081\u5bfe\u8c61\u5916 RaspberryPi\u5074 \u56fa\u5b9aIP\u8a2d\u5b9a(wlan0) WiFi\u63a5\u7d9a\u5148\u8a2d\u5b9a \u8a2d\u5b9a\u53cd\u6620 PC\u5074 WiFi\u63a5\u7d9a \u56fa\u5b9aIP\u8a2d\u5b9a(wlan0) 1 2 3 4 sudo vi /etc/dhcpcd.conf interface wlan0 static routers=192.168.111.1 static ip_address=192.168.111.100/24 \u30fbinterface\u306fwlan0\u3002(USB\u53d7\u4fe1\u6a5f\u3068\u304b\u3067)\u7121\u7dda\u53d7\u4fe1\u6a5f\u30922\u500b\u30823\u500b\u3082\u7a4d\u3093\u3067\u3044\u308bRaspberyPi\u306a\u3089\u53d7\u4fe1\u6a5f\u306b\u5272\u308a\u632f\u3089\u308c\u3066\u3044\u305d\u3046\u306a\u756a\u53f7\u3002(see iwconfig,iwlist,lsusb,lsmod) \u30fbrouters\u306f\u30eb\u30fc\u30bf\u306eIP\u30a2\u30c9\u30ec\u30b9\u3002\u3061\u3073\u30d5\u30a1\u30a4\u306e\u5de5\u5834\u51fa\u8377\u8a2d\u5b9a\u306f192.168.111.1 \u30fbip_address\u306fRaspberryPi\u3067\u4f7f\u3044\u305f\u3044IP\u30a2\u30c9\u30ec\u30b9192.168.111.100\u3068\u3001/24\u3067IP\u30a2\u30c9\u30ec\u30b9\u306b\u95a2\u9023\u3059\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9192.168.100.0\u3001\u307e\u305f\u306f\u305d\u308c\u306b\u76f8\u5f53\u3059\u308b\u30b5\u30d6\u30cd\u30c3\u30c8\u30de\u30b9\u30af255.255.255.0\u3092\u6307\u5b9a\u3057\u3001\u81ea\u5206(RaspberryPi)\u306eIP\u30a2\u30c9\u30ec\u30b9\u306f192.168.111.100\u3001\u6240\u5c5e\u3057\u3066\u3044\u308b\u30b5\u30d6\u30cd\u30c3\u30c8(\u540c\u4e00\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30a2\u30c9\u30ec\u30b9\u7bc4\u56f2)\u306f192.168.111.0\u304b\u3089192.168.111.255\u3067\u3042\u308b\u3053\u3068\u3092\u6307\u5b9a \u30fb192.168.111.0\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30c9\u30ec\u30b9\u3001192.168.111.255\u306f\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u30a2\u30c9\u30ec\u30b9\u3001192.168.111.1\u306f\u3061\u3073\u30d5\u30a1\u30a4\u306b\u5272\u308a\u632f\u3089\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5229\u7528\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u306f\u305d\u308c\u4ee5\u5916\u306e192.168.111.2\u304b\u3089192.168.111.254\u306e\u7bc4\u56f2\u306b\u306a\u308b \u30fb\u4eca\u56de\u306f\u30af\u30ed\u30fc\u30ba\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306e\u5229\u7528\u306a\u306e\u3067DNS\u8a2d\u5b9a\u306f\u7121\u304f\u3066\u3082\u554f\u984c\u306a\u3044 WiFi\u63a5\u7d9a\u5148\u8a2d\u5b9a ```ruby:qiita.rb sudo vi /etc/wpa_supplicant/wpa_supplicant.conf country=GB ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"planex-xxxxxx\" proto=WPA2 key_mgmt=WPA-PSK psk=\"xxxxxxxxxx\" } 1 2 3 4 \u30fb ssid\u306f\u3061\u3073\u30d5\u30a1\u30a4\u306essid ( game\u3058\u3083\u306a\u3044\u65b9 ) < br /> \u30fb psk\u306f\u9375\u30de\u30fc\u30af\u306e\u82f1\u6570\u5b57 \u3002 \u6697\u53f7\u5316\u3057\u305f\u30d1\u30b9\u30ef\u30fc\u30c9\u3082\u53ef\u80fd ( see [ wpa_passphrase ] ( https : // linux . die . net / man / 8 / wpa_passphrase )) ___ \u8a2d\u5b9a\u53cd\u6620 wifi\u505c\u6b62 sudo ipdown wlan0 wifi\u8d77\u52d5 sudo ipup wlan0 IP\u30a2\u30c9\u30ec\u30b9\u78ba\u8a8d ifconfig -a ``` \u30eb\u30fc\u30bf <- LAN -> RaspberryPi \u30eb\u30fc\u30bf <- WiFi -> PC \u5bfe\u5fdc\uff1a \u3061\u3073\u30d5\u30a1\u30a4 (MZK-RP150NA) RaspberryPi\u5074 \u3061\u3073\u30d5\u30a1\u30a4 LAN\u5074\u30dd\u30fc\u30c8\u3068RaspberryPi LAN\u30dd\u30fc\u30c8\u3092\u63a5\u7d9a \u56fa\u5b9aIP\u8a2d\u5b9a(eth0) \u8a2d\u5b9a\u53cd\u6620 PC\u5074 WiFi\u63a5\u7d9a \u30eb\u30fc\u30bf <- USB -> RaspberryPi \u30eb\u30fc\u30bf <- WiFi -> PC \u5bfe\u5fdc\uff1a \u3061\u3073\u30d5\u30a1\u30a42 (MZK-UE150N) RaspberryPi\u5074 \u3061\u3073\u30d5\u30a1\u30a42\u3092RaspberryPi\u306bUSB\u63a5\u7d9a\u3059\u308b \u56fa\u5b9aIP\u8a2d\u5b9a(eth1) \u8a2d\u5b9a\u53cd\u6620 PC\u5074 WiFi\u63a5\u7d9a","title":"RaspberryPi3 (OS:Raspbian) \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u63a5\u7d9a"},{"location":"raspberryPI/raspberry-network/#raspberrypi3-osraspbian","text":"\u30af\u30ed\u30fc\u30ba\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u3001RaspberryPi\u4e0a\u3067\u8d77\u52d5\u3057\u305fjupyter\u306bPC\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3057\u305f\u3044 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u6a5f\u5668\u306fRaspberryPi\u306e\u8d77\u52d5\u3067\u96fb\u6e90ON\u3068\u306a\u308bUSB\u7d66\u96fb\u30bf\u30a4\u30d7\u3092\u9078\u629e RaspberryPi3\u306fWiFi\u30e2\u30b8\u30e5\u30fc\u30eb\u5185\u81d3\u306a\u306e\u3067WiFi\u30eb\u30fc\u30bf\u3082\u9078\u629e\u53ef\u80fd \u3061\u3073\u30d5\u30a1\u30a4\u306f\u3059\u3079\u3066\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u4ed5\u69d8\u304c\u7570\u306a\u308b\u306e\u3067\u6ce8\u610f","title":"RaspberryPi3 (OS:Raspbian) \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u63a5\u7d9a"},{"location":"raspberryPI/raspberry-network/#-wifi-raspberrypi-wifi-pc","text":"\u5bfe\u5fdc\uff1a \u3061\u3073\u30d5\u30a1\u30a4 (MZK-RP150NA) \u3061\u3073\u30d5\u30a1\u30a42 (MZK-UE150N) \u3061\u3073\u30d5\u30a1\u30a42ac (MZK-UE450AC) \u3061\u3073\u30d5\u30a1\u30a43 (MZK-DP150N,MZK-DP150N/R) MZK-DP150N/R\u306f\u7bb1\u306e\u30e9\u30d9\u30eb\u3067\u672c\u4f53\u578b\u756a\u306fMZK-DP150N\u306e\u307f \u3061\u3073\u30d5\u30a1\u30a43\u306fUSB\u7d66\u96fb\u51fa\u6765\u306a\u3044\u305f\u3081\u5bfe\u8c61\u5916 RaspberryPi\u5074 \u56fa\u5b9aIP\u8a2d\u5b9a(wlan0) WiFi\u63a5\u7d9a\u5148\u8a2d\u5b9a \u8a2d\u5b9a\u53cd\u6620 PC\u5074 WiFi\u63a5\u7d9a \u56fa\u5b9aIP\u8a2d\u5b9a(wlan0) 1 2 3 4 sudo vi /etc/dhcpcd.conf interface wlan0 static routers=192.168.111.1 static ip_address=192.168.111.100/24 \u30fbinterface\u306fwlan0\u3002(USB\u53d7\u4fe1\u6a5f\u3068\u304b\u3067)\u7121\u7dda\u53d7\u4fe1\u6a5f\u30922\u500b\u30823\u500b\u3082\u7a4d\u3093\u3067\u3044\u308bRaspberyPi\u306a\u3089\u53d7\u4fe1\u6a5f\u306b\u5272\u308a\u632f\u3089\u308c\u3066\u3044\u305d\u3046\u306a\u756a\u53f7\u3002(see iwconfig,iwlist,lsusb,lsmod) \u30fbrouters\u306f\u30eb\u30fc\u30bf\u306eIP\u30a2\u30c9\u30ec\u30b9\u3002\u3061\u3073\u30d5\u30a1\u30a4\u306e\u5de5\u5834\u51fa\u8377\u8a2d\u5b9a\u306f192.168.111.1 \u30fbip_address\u306fRaspberryPi\u3067\u4f7f\u3044\u305f\u3044IP\u30a2\u30c9\u30ec\u30b9192.168.111.100\u3068\u3001/24\u3067IP\u30a2\u30c9\u30ec\u30b9\u306b\u95a2\u9023\u3059\u308b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9192.168.100.0\u3001\u307e\u305f\u306f\u305d\u308c\u306b\u76f8\u5f53\u3059\u308b\u30b5\u30d6\u30cd\u30c3\u30c8\u30de\u30b9\u30af255.255.255.0\u3092\u6307\u5b9a\u3057\u3001\u81ea\u5206(RaspberryPi)\u306eIP\u30a2\u30c9\u30ec\u30b9\u306f192.168.111.100\u3001\u6240\u5c5e\u3057\u3066\u3044\u308b\u30b5\u30d6\u30cd\u30c3\u30c8(\u540c\u4e00\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30a2\u30c9\u30ec\u30b9\u7bc4\u56f2)\u306f192.168.111.0\u304b\u3089192.168.111.255\u3067\u3042\u308b\u3053\u3068\u3092\u6307\u5b9a \u30fb192.168.111.0\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30c9\u30ec\u30b9\u3001192.168.111.255\u306f\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u30a2\u30c9\u30ec\u30b9\u3001192.168.111.1\u306f\u3061\u3073\u30d5\u30a1\u30a4\u306b\u5272\u308a\u632f\u3089\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5229\u7528\u53ef\u80fd\u306aIP\u30a2\u30c9\u30ec\u30b9\u306f\u305d\u308c\u4ee5\u5916\u306e192.168.111.2\u304b\u3089192.168.111.254\u306e\u7bc4\u56f2\u306b\u306a\u308b \u30fb\u4eca\u56de\u306f\u30af\u30ed\u30fc\u30ba\u30c9\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306e\u5229\u7528\u306a\u306e\u3067DNS\u8a2d\u5b9a\u306f\u7121\u304f\u3066\u3082\u554f\u984c\u306a\u3044 WiFi\u63a5\u7d9a\u5148\u8a2d\u5b9a ```ruby:qiita.rb sudo vi /etc/wpa_supplicant/wpa_supplicant.conf country=GB ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"planex-xxxxxx\" proto=WPA2 key_mgmt=WPA-PSK psk=\"xxxxxxxxxx\" } 1 2 3 4 \u30fb ssid\u306f\u3061\u3073\u30d5\u30a1\u30a4\u306essid ( game\u3058\u3083\u306a\u3044\u65b9 ) < br /> \u30fb psk\u306f\u9375\u30de\u30fc\u30af\u306e\u82f1\u6570\u5b57 \u3002 \u6697\u53f7\u5316\u3057\u305f\u30d1\u30b9\u30ef\u30fc\u30c9\u3082\u53ef\u80fd ( see [ wpa_passphrase ] ( https : // linux . die . net / man / 8 / wpa_passphrase )) ___ \u8a2d\u5b9a\u53cd\u6620","title":"\u30eb\u30fc\u30bf &lt;- WiFi -&gt; RaspberryPi\u30eb\u30fc\u30bf &lt;- WiFi -&gt; PC"},{"location":"raspberryPI/raspberry-network/#wifi","text":"sudo ipdown wlan0","title":"wifi\u505c\u6b62"},{"location":"raspberryPI/raspberry-network/#wifi_1","text":"sudo ipup wlan0","title":"wifi\u8d77\u52d5"},{"location":"raspberryPI/raspberry-network/#ip","text":"ifconfig -a ```","title":"IP\u30a2\u30c9\u30ec\u30b9\u78ba\u8a8d"},{"location":"raspberryPI/raspberry-network/#-lan-raspberrypi-wifi-pc","text":"\u5bfe\u5fdc\uff1a \u3061\u3073\u30d5\u30a1\u30a4 (MZK-RP150NA) RaspberryPi\u5074 \u3061\u3073\u30d5\u30a1\u30a4 LAN\u5074\u30dd\u30fc\u30c8\u3068RaspberryPi LAN\u30dd\u30fc\u30c8\u3092\u63a5\u7d9a \u56fa\u5b9aIP\u8a2d\u5b9a(eth0) \u8a2d\u5b9a\u53cd\u6620 PC\u5074 WiFi\u63a5\u7d9a","title":"\u30eb\u30fc\u30bf &lt;- LAN -&gt; RaspberryPi\u30eb\u30fc\u30bf &lt;- WiFi -&gt; PC"},{"location":"raspberryPI/raspberry-network/#-usb-raspberrypi-wifi-pc","text":"\u5bfe\u5fdc\uff1a \u3061\u3073\u30d5\u30a1\u30a42 (MZK-UE150N) RaspberryPi\u5074 \u3061\u3073\u30d5\u30a1\u30a42\u3092RaspberryPi\u306bUSB\u63a5\u7d9a\u3059\u308b \u56fa\u5b9aIP\u8a2d\u5b9a(eth1) \u8a2d\u5b9a\u53cd\u6620 PC\u5074 WiFi\u63a5\u7d9a","title":"\u30eb\u30fc\u30bf &lt;- USB -&gt; RaspberryPi\u30eb\u30fc\u30bf &lt;- WiFi -&gt; PC"}]}